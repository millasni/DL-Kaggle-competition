{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting_with_LSTM_CNN_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DsTH-Q4o76_S"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyz7ZdY2gpzy",
        "colab_type": "text"
      },
      "source": [
        "# 1. Summary\n",
        "\n",
        "Please note that no external data was used to improve predictions, i.e. only data available in 5 files on Kaggle was used in training models.\n",
        "\n",
        "This code file contains predictions with **CNN, LSTM, GRU** by using:\n",
        "\n",
        "*   Just article titles (valid.accuracy capped at 75% in colab and at 72.6% in Kaggle) \n",
        "*   Article own titles + titles of incoming/outgoing references concatenated - accuracy about 80.8-81.1% in colab, 81.7-82.1% in kaggle\n",
        "*   Embeddings learnt on the training set vs GloVe and Word2Vec embeddings.\n",
        "\n",
        "Other models were implemented in separate files:\n",
        "\n",
        "*   **Using BERT with ktrain in keras** - accuracy 75.3% in colab, 74% in kaggle using own titles only; own titles+reference titles - 81.5% acc in colab, not used for kaggle uploads due to lower accuracy than for other implementation of BERT.\n",
        "*   **Using BERT in PyTorch** - accuracy using titles only: 76% in colab, 74.5% in kaggle; accuracy using own titles + titles of incoming/outgoing references: 83-85% in colab and 81.7-82.9% in kaggle (trained on whole training data for 3 epochs). **The best model produces 82.86% accuracy on kaggle**.\n",
        "*   **Using text mining techniques** - accuracy using own titles only with logistic regression on TF-IDF matrix - 72% in colab and 69.6% in kaggle. Using own titles + reference titles concatenated + TF-IDF: logistic regression: 82% in colab and kaggle; multi-layer perceptron with (128,256) layers: 82.6% in kaggle (**the second-best!**).\n",
        "\n",
        "It's interesting that using TF-IDF matrix of own titles + reference titles and a logistic regression or multilayer perceptron allows reaching higher accuracy than with most specialized NLP models except BERT and one instance of CNN. It might be due to the fact that our training data is small (12k)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBP76s36DZhs",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvIRP07PgeiF",
        "colab_type": "text"
      },
      "source": [
        "## Reading in data and installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75D3A19MjlY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "3b4198b5-e0d0-436b-bb68-24b8ed15307a"
      },
      "source": [
        "# download the repo to get access to data\n",
        "!rm -rf DL-Kaggle-competition/\n",
        "!git clone https://github.com/millasni/DL-Kaggle-competition/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL-Kaggle-competition'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 25 (delta 4), reused 23 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmaaxB6XmnrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c6ffecc-7605-4252-a80b-e8218f5483f3"
      },
      "source": [
        "!pip install -q torch skorch torchvision torchtext\n",
        "tf\n",
        "!pip install keras\n",
        "!pip install h5py\n",
        "!pip3 install ktrain\n",
        "!pip install NLTK\n",
        "# !pip install bert-tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▉                             | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 40kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 51kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 61kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 71kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 81kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 92kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0rc0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.2)\n",
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/26/b66d9a4e27ca2854ee7f2c66084f90b7bf7e27e46b7611be52f2fe9ae5dc/ktrain-0.13.2.tar.gz (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 183kB/s \n",
            "\u001b[?25hCollecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.2.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.3)\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c5/7e1a0d7b4afd83d6f8de794fce82820ec4c5136c6d52e14000822681a842/cchardet-2.1.6-cp36-cp36m-manylinux2010_x86_64.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.4)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.3)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.0)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Collecting syntok\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/36/5b423791cd877a21c2771a2b070194270f163f2969066923f89aa3099e2d/syntok-1.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.18.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.28.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (3.2.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert>=0.81.0->ktrain) (2.3.1)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/0c/fede535ac576c03863c44bf2e0bf051fe21f5e10103631b6b6236ae446f3/keras-transformer-0.32.0.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2020.4.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (7.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.1)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.21.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (19.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (4.38.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.3.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (2.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->ktrain) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->ktrain) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->ktrain) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->ktrain) (1.12.38)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (46.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0->ktrain) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->ktrain) (1.51.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->ktrain) (7.1.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.7.0->ktrain) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.7.0->ktrain) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.7.0->ktrain) (1.15.38)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.1.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.3.0)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers>=2.7.0->ktrain) (0.15.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.0)\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, seqeval, syntok, gast, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.13.2-cp36-none-any.whl size=25239774 sha256=8cd21c95c633677462b83ceea67cf9d62a6616a561b4cf239b70fbb38a88ccc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/fb/62/cf5424c7a9c267b78db4efacfe8b4c3a0a3f1a755f2d63e428\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=173c1063ebf1de9df4898bd2db8df8f8bf99810f01ccd46ddf9dcb04a6b2b703\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=a9ac987a158ff54bfeb72333846be8a386344fd7219af059f4a1abd6a1a6bc9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=c033f232db7a7c2d47124130d97e888226503160753e0517dc85c833d0c30c5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.2.2-cp36-none-any.whl size=20724 sha256=50a2724be9d469cb874770ab9b5cddca697706d578bf2249e63749e554363ef6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/b0/d2/ffdbbc1a16cb37e580fb7b3a6fbaaf09c7f7c163981db385b3\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=6401c355f40f0860094e4d665c4065e75c9ad777888580d00d464e51ba4d47d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.32.0-cp36-none-any.whl size=13266 sha256=6163b96baa8b6b9ee8a407383d8e1d474c4b1befaef5322a8ad4db0371bf4f6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f0/ce/82fa5d024d5ef8e263f26a50dcee23820efe245680ce9c922a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=d8b013d0ae69daf6cc54b72906e24d020119908a51b3d6c9cdfc20a16b986736\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=8bc8dfd7a43e6beb6652f662f48c846707acfe8f7a7ad3b6a6f09ec5a9befda0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=78b46f8d98919680c0a004939e511dc2e167f503f4e79b4ff6e1a1eac5861edd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=412ddab6e28ab89ae6cf4c820853f4818d7555892f71af4628973167f8ecd315\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=40364f227a01df2b7d1ecac07a87d654371cd6caf506b39653309031584679ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=ec1e43057df46f866b736fcccb543562fcebd19911ba898c66d8d53ca5a28ddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=7cf641a88f949daa7048b9f878339a4cb4563ca51c119d3c1fb475373d3e36e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built ktrain keras-bert langdetect seqeval syntok gast keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, seqeval, sentencepiece, sacremoses, tokenizers, transformers, syntok, ktrain\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed cchardet-2.1.6 gast-0.2.2 keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.32.0 ktrain-0.13.2 langdetect-1.0.8 sacremoses-0.0.41 sentencepiece-0.1.85 seqeval-0.0.12 syntok-1.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 tokenizers-0.5.2 transformers-2.8.0\n",
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from NLTK) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXYdZ9IagpF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "093629e1-33bd-4a12-924c-4f6589e35894"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import skorch\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K # or from keras...?\n",
        "from tensorflow.keras.backend import concatenate\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, Bidirectional,SpatialDropout1D,Concatenate\n",
        "from tensorflow.keras.layers import Conv1D,GlobalMaxPooling1D,MaxPooling1D,Dropout,Flatten,Activation\n",
        "\n",
        "# from tensorflow.python.keras.layers import Dense\n",
        "# from tensorflow.python.keras import Sequential\n",
        "\n",
        "from keras.utils.np_utils import to_categorical \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "from tensorflow.keras import optimizers\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "\n",
        "import ktrain\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXiv3Bcumuc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('DL-Kaggle-competition/train.csv') \n",
        "test = pd.read_csv('DL-Kaggle-competition/test.csv') \n",
        "text = pd.read_csv('DL-Kaggle-competition/text.csv') \n",
        "reference = pd.read_csv('DL-Kaggle-competition/reference.csv') \n",
        "reference_classes = pd.read_csv('DL-Kaggle-competition/ref_class_shares.csv') \n",
        "cross_ref = pd.read_csv('DL-Kaggle-competition/ref_in_out.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TQ6A_0mbPfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "d1d655d2-b32c-486a-a971-da0076130d80"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label\n",
              "0   0      1\n",
              "1   3      1\n",
              "2   6      1\n",
              "3   8      0\n",
              "4   9      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR-laMsksDcb",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPZWNyVJhuvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "b7077856-4a3b-4018-cb58-9f841a153158"
      },
      "source": [
        "# One-hot encode the classes in the train\n",
        "Y = to_categorical(train[['label']])\n",
        "print(Y.shape)\n",
        "print(Y[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12779, 5)\n",
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjzj-tC9cjIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "bb1bfbc8-bc4c-4a72-a0a8-d9e7a4ed7696"
      },
      "source": [
        "print(text.head())\n",
        "text.shape # we have twice as many lines in text than in train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id                                              title\n",
            "0   0  interactive visual exploration of neighbor bas...\n",
            "1   1  autodomainmine a graphical data mining system ...\n",
            "2   2  anipqo almost non intrusive parametric query o...\n",
            "3   3  relational division four algorithms and their ...\n",
            "4   4  selection and ranking of text from highly impe...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25561, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5ZVLLiouozU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1c9542bd-be1a-455c-8dcd-887abab8375f"
      },
      "source": [
        "train = train.join(text, how='left', on='id',  lsuffix='', rsuffix='text')\n",
        "train.head()\n",
        "train.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>idtext</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12774</th>\n",
              "      <td>25547</td>\n",
              "      <td>4</td>\n",
              "      <td>25547</td>\n",
              "      <td>scaling up from dialogue to multilogue some pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12775</th>\n",
              "      <td>25548</td>\n",
              "      <td>3</td>\n",
              "      <td>25548</td>\n",
              "      <td>a laboratory for the development and evaluatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12776</th>\n",
              "      <td>25554</td>\n",
              "      <td>2</td>\n",
              "      <td>25554</td>\n",
              "      <td>an analysis of transformational analogy genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12777</th>\n",
              "      <td>25555</td>\n",
              "      <td>2</td>\n",
              "      <td>25555</td>\n",
              "      <td>exploiting known taxonomies in learning overla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12778</th>\n",
              "      <td>25557</td>\n",
              "      <td>1</td>\n",
              "      <td>25557</td>\n",
              "      <td>maintaining materialized views in distributed ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  label  idtext                                              title\n",
              "12774  25547      4   25547  scaling up from dialogue to multilogue some pr...\n",
              "12775  25548      3   25548  a laboratory for the development and evaluatio...\n",
              "12776  25554      2   25554  an analysis of transformational analogy genera...\n",
              "12777  25555      2   25555  exploiting known taxonomies in learning overla...\n",
              "12778  25557      1   25557  maintaining materialized views in distributed ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6xJXgRQy2fj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "126a3544-8c7b-4ac8-d220-a0b92df1cef4"
      },
      "source": [
        "train = train[['id','label','title']]\n",
        "train.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              title\n",
              "0   0      1  interactive visual exploration of neighbor bas...\n",
              "1   3      1  relational division four algorithms and their ...\n",
              "2   6      1  simplifying xml schema effortless handling of ...\n",
              "3   8      0  funbase a function based information managemen...\n",
              "4   9      0  inverted matrix efficient discovery of frequen..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLRaDZ3ExQv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "33d40bbf-170f-4df5-b2e8-508dd6a7b720"
      },
      "source": [
        "test = test.join(text, how='left', on='id',  lsuffix='', rsuffix='text')\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>idtext</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>autodomainmine a graphical data mining system ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>anipqo almost non intrusive parametric query o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>selection and ranking of text from highly impe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>conditional random fields for multi agent rein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>multi dimensional description logics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  idtext                                              title\n",
              "0   1       1  autodomainmine a graphical data mining system ...\n",
              "1   2       2  anipqo almost non intrusive parametric query o...\n",
              "2   4       4  selection and ranking of text from highly impe...\n",
              "3   5       5  conditional random fields for multi agent rein...\n",
              "4   7       7               multi dimensional description logics"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-MQ_oozDq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "92790f48-55ad-4c9d-9596-f43d3487dcab"
      },
      "source": [
        "test = test[['id','title']]\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>autodomainmine a graphical data mining system ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>anipqo almost non intrusive parametric query o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>selection and ranking of text from highly impe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>conditional random fields for multi agent rein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>multi dimensional description logics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              title\n",
              "0   1  autodomainmine a graphical data mining system ...\n",
              "1   2  anipqo almost non intrusive parametric query o...\n",
              "2   4  selection and ranking of text from highly impe...\n",
              "3   5  conditional random fields for multi agent rein...\n",
              "4   7               multi dimensional description logics"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srmaulRH8VOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize train data on the whole TEXT (train+test) to see the max sent_len - - for exploration only\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text.title)\n",
        "t = tokenizer.texts_to_sequences(text.title)\n",
        "print(t[:10])\n",
        "print(len(max(t, key=len)))\n",
        "sent_len = []\n",
        "for i in t:\n",
        "  sent_len.append(len(i))\n",
        "\n",
        "plt.hist(sent_len,bins=28)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDcaxRln0Wni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize train data (on the whole set of labeled data)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.title)\n",
        "X = tokenizer.texts_to_sequences(train.title)\n",
        "X = pad_sequences(X, maxlen=20)\n",
        "X[:10]\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index)) # Found 9321 unique tokens."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "helbdUizF2un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGj9gsCKmYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAKE A SECOND DATA SPLIT\n",
        "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X, Y, test_size=0.2, random_state=245, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-e50l29Hg3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "7564c8c4-7bd3-41c7-be08-9d490f9ccf17"
      },
      "source": [
        "print(Y_train[:5])\n",
        "print(Y_test[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tVCkJNtr1kUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "ceb72e9a-e279-456c-e397-1ab51a767436"
      },
      "source": [
        "# tokenize test data for Kaggle upload\n",
        "X_new = tokenizer.texts_to_sequences(test.title)\n",
        "X_new = pad_sequences(X_new, maxlen=20)\n",
        "X_new[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    3,  482,    8,   32,   15,    1,  117,   66],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        1591,  130,  758,   20,   66,    1, 1407,  295,  307],\n",
              "       [   0,    0,    0,    0,    0,    0,   95,    4,  120,    2,   35,\n",
              "          21,  693, 2005, 6187,    1,   24,    2,  463,  166],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  378,  229,  793,    1,   47,  278,  399,   14],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   47,  202,  302,  519],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  107,   10,  320,  284,  285,   33,  929,  593],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,   76,  299, 8308],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0, 1923,  287, 5096,   33, 1457, 1855,  189],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,\n",
              "         107,    4, 4608,  148,   53,    1,  305,  216,   25],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,  378,  159,  620,  377, 1261,    4,  206]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKUSQm62IR9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f3cff2ca-5bd6-4796-b899-e4cf4095e918"
      },
      "source": [
        "# trying to find dictionary length (for the whole labelled set tokenized)\n",
        "print(len(X_train))\n",
        "print(np.amax(X)) \n",
        "np.amax(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10223\n",
            "9321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZS9fSpdIg3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "61f08df4-3e43-4936-ceaa-fac9b73d3108"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...    5    8  161]\n",
            " [   0    0    0 ...    4  415   58]\n",
            " [   0    0    0 ... 1866 1444  622]\n",
            " ...\n",
            " [   0    0    0 ...   64    4  206]\n",
            " [   0    0    0 ...   14 2086  314]\n",
            " [   0    0    0 ...    5   38   29]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBsJGIy4xH9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3822a2db-60d0-4d52-d55b-e42c3099342a"
      },
      "source": [
        "# dict_len = max(max(X_train)) # for only X_train tokenized\n",
        "dict_len = np.amax(X)\n",
        "print(dict_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O--othcolipK",
        "colab_type": "text"
      },
      "source": [
        "## Making results reproducible with keras\n",
        "\n",
        "Will repeat this setting seed exercise before each model run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHKjEiuTU19z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "# I replaced tf.set_random_seed with tf.compat.v1.set_random_seed (the former did not work)\n",
        "# the same change in tf.compat.v1.ConfigProto\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(42)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "random.seed(12345)\n",
        "\n",
        "# Force TensorFlow to use single thread.\n",
        "# Multiple threads are a potential source of non-reproducible results.\n",
        "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "# The below tf.set_random_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
        "\n",
        "# tf.set_random_seed(1234) # this is not working\n",
        "tf.compat.v1.set_random_seed(1234)\n",
        "# sess = tf.compat.v1.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIyIe7miw4hM",
        "colab_type": "text"
      },
      "source": [
        "# 3. Using only own article titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYlGfUpMqe7I",
        "colab_type": "text"
      },
      "source": [
        "## LSTM\n",
        "\n",
        "After trying a bunch of LSTM models with Adam and RMSprop optimizers, I keep only RMSprop result as it wins the battle and will be used thereafter. \n",
        "\n",
        "I also played with the learning rate and batch size for RMSprop and default lr with batch=64 gives decent results, so I will stick to them (outputs not shown)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ6TAzmXl2oM",
        "colab_type": "text"
      },
      "source": [
        "#### Trying various basic (one hidden layer) architectures\n",
        "\n",
        "Slight trend to growing valid. accuracy with increased emb and hidden dims."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OG5elm0fHDs_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e55d17c8-d693-412b-eabe-2c092ea7693b"
      },
      "source": [
        "embed = [32,64,128]\n",
        "hidden = [64,128,256]\n",
        "drop = [0, 0.1, 0.2, 0.3]\n",
        "for i in embed:\n",
        "  for j in hidden:\n",
        "    for k in drop:\n",
        "      print('emb_dim:',i,' hidden_dim:',j, ' drop:',k,'----RMSprop')\n",
        "      np.random.seed(42)\n",
        "      random.seed(12345)\n",
        "      session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                    inter_op_parallelism_threads=1)\n",
        "      tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "      model = Sequential() \n",
        "      model.add(Embedding(dict_len + 1, output_dim=i)) \n",
        "      model.add(LSTM(j, dropout=k)) \n",
        "      model.add(Dense(5, activation=\"softmax\"))  \n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=7) \n",
        "      model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_dim: 32  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 1.4046 - accuracy: 0.4062 - val_loss: 1.0870 - val_accuracy: 0.5646\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.9025 - accuracy: 0.6645 - val_loss: 0.8483 - val_accuracy: 0.6714\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.6947 - accuracy: 0.7523 - val_loss: 0.7841 - val_accuracy: 0.7066\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5924 - accuracy: 0.7889 - val_loss: 0.7771 - val_accuracy: 0.7148\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5224 - accuracy: 0.8178 - val_loss: 0.7396 - val_accuracy: 0.7340\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.4708 - accuracy: 0.8353 - val_loss: 0.7413 - val_accuracy: 0.7238\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4269 - accuracy: 0.8534 - val_loss: 0.8439 - val_accuracy: 0.7254\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8438 - accuracy: 0.7254\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 1.4154 - accuracy: 0.4041 - val_loss: 1.1124 - val_accuracy: 0.5630\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.9264 - accuracy: 0.6512 - val_loss: 0.8616 - val_accuracy: 0.6628\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.7200 - accuracy: 0.7429 - val_loss: 0.7980 - val_accuracy: 0.6987\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.6173 - accuracy: 0.7819 - val_loss: 0.7669 - val_accuracy: 0.7171\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5479 - accuracy: 0.8100 - val_loss: 0.7417 - val_accuracy: 0.7308\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4990 - accuracy: 0.8258 - val_loss: 0.7373 - val_accuracy: 0.7261\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4524 - accuracy: 0.8426 - val_loss: 0.8228 - val_accuracy: 0.7312\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8227 - accuracy: 0.7312\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 1.4302 - accuracy: 0.3948 - val_loss: 1.1282 - val_accuracy: 0.5634\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.9572 - accuracy: 0.6368 - val_loss: 0.8824 - val_accuracy: 0.6534\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.7443 - accuracy: 0.7299 - val_loss: 0.8039 - val_accuracy: 0.6972\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.6409 - accuracy: 0.7704 - val_loss: 0.7561 - val_accuracy: 0.7164\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5734 - accuracy: 0.7985 - val_loss: 0.7404 - val_accuracy: 0.7297\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5256 - accuracy: 0.8155 - val_loss: 0.7315 - val_accuracy: 0.7308\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4788 - accuracy: 0.8335 - val_loss: 0.7876 - val_accuracy: 0.7351\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7875 - accuracy: 0.7351\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 1.4445 - accuracy: 0.3913 - val_loss: 1.1533 - val_accuracy: 0.5618\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.9875 - accuracy: 0.6211 - val_loss: 0.8993 - val_accuracy: 0.6573\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.7762 - accuracy: 0.7175 - val_loss: 0.8156 - val_accuracy: 0.6917\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.6740 - accuracy: 0.7566 - val_loss: 0.7865 - val_accuracy: 0.7034\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.6048 - accuracy: 0.7883 - val_loss: 0.7451 - val_accuracy: 0.7273\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5578 - accuracy: 0.8049 - val_loss: 0.7280 - val_accuracy: 0.7320\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5150 - accuracy: 0.8189 - val_loss: 0.7850 - val_accuracy: 0.7289\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7850 - accuracy: 0.7289\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 44ms/step - loss: 1.3526 - accuracy: 0.4420 - val_loss: 1.0136 - val_accuracy: 0.5908\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.8659 - accuracy: 0.6854 - val_loss: 0.8256 - val_accuracy: 0.6917\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.6853 - accuracy: 0.7549 - val_loss: 0.7742 - val_accuracy: 0.7121\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.5885 - accuracy: 0.7924 - val_loss: 0.7714 - val_accuracy: 0.7203\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.5178 - accuracy: 0.8167 - val_loss: 0.7498 - val_accuracy: 0.7234\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.4656 - accuracy: 0.8393 - val_loss: 0.7485 - val_accuracy: 0.7261\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.4207 - accuracy: 0.8564 - val_loss: 0.7914 - val_accuracy: 0.7312\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7915 - accuracy: 0.7312\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 45ms/step - loss: 1.3888 - accuracy: 0.4377 - val_loss: 1.0658 - val_accuracy: 0.5861\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.9107 - accuracy: 0.6652 - val_loss: 0.8438 - val_accuracy: 0.6878\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.7222 - accuracy: 0.7390 - val_loss: 0.7777 - val_accuracy: 0.7109\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 44ms/step - loss: 0.6222 - accuracy: 0.7822 - val_loss: 0.7717 - val_accuracy: 0.7160\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.5515 - accuracy: 0.8084 - val_loss: 0.7408 - val_accuracy: 0.7312\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 7s 44ms/step - loss: 0.5019 - accuracy: 0.8250 - val_loss: 0.7339 - val_accuracy: 0.7312\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.4543 - accuracy: 0.8425 - val_loss: 0.7754 - val_accuracy: 0.7371\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7755 - accuracy: 0.7371\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 45ms/step - loss: 1.3785 - accuracy: 0.4265 - val_loss: 1.0589 - val_accuracy: 0.6052\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.9183 - accuracy: 0.6555 - val_loss: 0.8590 - val_accuracy: 0.6776\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.7447 - accuracy: 0.7309 - val_loss: 0.7855 - val_accuracy: 0.7019\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.6448 - accuracy: 0.7728 - val_loss: 0.7816 - val_accuracy: 0.7171\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.5772 - accuracy: 0.7960 - val_loss: 0.7437 - val_accuracy: 0.7332\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.5275 - accuracy: 0.8167 - val_loss: 0.7339 - val_accuracy: 0.7312\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.4844 - accuracy: 0.8324 - val_loss: 0.7558 - val_accuracy: 0.7379\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7558 - accuracy: 0.7379\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 1.4082 - accuracy: 0.4178 - val_loss: 1.0898 - val_accuracy: 0.5861\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.9478 - accuracy: 0.6463 - val_loss: 0.8681 - val_accuracy: 0.6674\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.7738 - accuracy: 0.7220 - val_loss: 0.7916 - val_accuracy: 0.6991\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.6772 - accuracy: 0.7589 - val_loss: 0.7813 - val_accuracy: 0.7093\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 44ms/step - loss: 0.6127 - accuracy: 0.7840 - val_loss: 0.7463 - val_accuracy: 0.7273\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 7s 44ms/step - loss: 0.5649 - accuracy: 0.8029 - val_loss: 0.7344 - val_accuracy: 0.7316\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.5260 - accuracy: 0.8142 - val_loss: 0.7498 - val_accuracy: 0.7347\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7498 - accuracy: 0.7347\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 108ms/step - loss: 1.3428 - accuracy: 0.4540 - val_loss: 1.0193 - val_accuracy: 0.6440\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.8470 - accuracy: 0.6981 - val_loss: 0.8155 - val_accuracy: 0.7015\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.6700 - accuracy: 0.7646 - val_loss: 0.7654 - val_accuracy: 0.7191\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.5732 - accuracy: 0.7983 - val_loss: 0.7454 - val_accuracy: 0.7336\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 105ms/step - loss: 0.5069 - accuracy: 0.8213 - val_loss: 0.7469 - val_accuracy: 0.7261\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.4556 - accuracy: 0.8427 - val_loss: 0.7703 - val_accuracy: 0.7191\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.4094 - accuracy: 0.8586 - val_loss: 0.7964 - val_accuracy: 0.7300\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.7963 - accuracy: 0.7300\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 1.3633 - accuracy: 0.4401 - val_loss: 1.0395 - val_accuracy: 0.6260\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.8651 - accuracy: 0.6899 - val_loss: 0.8279 - val_accuracy: 0.6972\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.6919 - accuracy: 0.7583 - val_loss: 0.7627 - val_accuracy: 0.7250\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 0.5939 - accuracy: 0.7912 - val_loss: 0.7636 - val_accuracy: 0.7312\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.5296 - accuracy: 0.8151 - val_loss: 0.7466 - val_accuracy: 0.7254\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.4815 - accuracy: 0.8315 - val_loss: 0.7669 - val_accuracy: 0.7265\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.4358 - accuracy: 0.8498 - val_loss: 0.7785 - val_accuracy: 0.7320\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.7784 - accuracy: 0.7320\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 18s 109ms/step - loss: 1.3766 - accuracy: 0.4263 - val_loss: 1.0562 - val_accuracy: 0.6420\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.8922 - accuracy: 0.6766 - val_loss: 0.8298 - val_accuracy: 0.6964\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 17s 108ms/step - loss: 0.7209 - accuracy: 0.7427 - val_loss: 0.7652 - val_accuracy: 0.7179\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.6243 - accuracy: 0.7793 - val_loss: 0.7584 - val_accuracy: 0.7273\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.5624 - accuracy: 0.8008 - val_loss: 0.7374 - val_accuracy: 0.7281\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.5118 - accuracy: 0.8187 - val_loss: 0.7389 - val_accuracy: 0.7383\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 105ms/step - loss: 0.4702 - accuracy: 0.8371 - val_loss: 0.7647 - val_accuracy: 0.7351\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.7646 - accuracy: 0.7351\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 1.4010 - accuracy: 0.4147 - val_loss: 1.1083 - val_accuracy: 0.6154\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.9348 - accuracy: 0.6601 - val_loss: 0.8473 - val_accuracy: 0.6757\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.7543 - accuracy: 0.7324 - val_loss: 0.7796 - val_accuracy: 0.7101\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.6593 - accuracy: 0.7649 - val_loss: 0.7557 - val_accuracy: 0.7242\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.5992 - accuracy: 0.7856 - val_loss: 0.7437 - val_accuracy: 0.7277\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 17s 106ms/step - loss: 0.5527 - accuracy: 0.8065 - val_loss: 0.7327 - val_accuracy: 0.7336\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.5128 - accuracy: 0.8207 - val_loss: 0.7494 - val_accuracy: 0.7347\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.7493 - accuracy: 0.7347\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 1.3140 - accuracy: 0.4698 - val_loss: 0.9584 - val_accuracy: 0.6412\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.7901 - accuracy: 0.7173 - val_loss: 0.7754 - val_accuracy: 0.7144\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.6150 - accuracy: 0.7819 - val_loss: 0.7621 - val_accuracy: 0.7210\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5228 - accuracy: 0.8162 - val_loss: 0.7646 - val_accuracy: 0.7281\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.4576 - accuracy: 0.8427 - val_loss: 0.7381 - val_accuracy: 0.7308\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.4068 - accuracy: 0.8600 - val_loss: 0.7678 - val_accuracy: 0.7254\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.3645 - accuracy: 0.8730 - val_loss: 0.8440 - val_accuracy: 0.7242\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8441 - accuracy: 0.7242\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 1.3275 - accuracy: 0.4645 - val_loss: 0.9757 - val_accuracy: 0.6346\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.8106 - accuracy: 0.7088 - val_loss: 0.7799 - val_accuracy: 0.7089\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.6314 - accuracy: 0.7772 - val_loss: 0.7701 - val_accuracy: 0.7175\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5408 - accuracy: 0.8090 - val_loss: 0.7549 - val_accuracy: 0.7285\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.4785 - accuracy: 0.8363 - val_loss: 0.7271 - val_accuracy: 0.7344\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.4275 - accuracy: 0.8533 - val_loss: 0.7554 - val_accuracy: 0.7289\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.3880 - accuracy: 0.8648 - val_loss: 0.8131 - val_accuracy: 0.7261\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8131 - accuracy: 0.7261\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 1.3424 - accuracy: 0.4533 - val_loss: 1.0031 - val_accuracy: 0.6056\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.8370 - accuracy: 0.6985 - val_loss: 0.7861 - val_accuracy: 0.7046\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.6573 - accuracy: 0.7675 - val_loss: 0.7658 - val_accuracy: 0.7128\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5670 - accuracy: 0.8003 - val_loss: 0.7518 - val_accuracy: 0.7300\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5076 - accuracy: 0.8232 - val_loss: 0.7280 - val_accuracy: 0.7383\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.4565 - accuracy: 0.8400 - val_loss: 0.7488 - val_accuracy: 0.7312\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.4194 - accuracy: 0.8548 - val_loss: 0.7953 - val_accuracy: 0.7312\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7952 - accuracy: 0.7312\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 1.3555 - accuracy: 0.4463 - val_loss: 1.0157 - val_accuracy: 0.6068\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.8575 - accuracy: 0.6871 - val_loss: 0.7971 - val_accuracy: 0.7023\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.6801 - accuracy: 0.7578 - val_loss: 0.7609 - val_accuracy: 0.7160\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.5923 - accuracy: 0.7906 - val_loss: 0.7541 - val_accuracy: 0.7257\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5308 - accuracy: 0.8167 - val_loss: 0.7249 - val_accuracy: 0.7320\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.4816 - accuracy: 0.8303 - val_loss: 0.7436 - val_accuracy: 0.7324\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.4491 - accuracy: 0.8445 - val_loss: 0.7944 - val_accuracy: 0.7297\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7945 - accuracy: 0.7297\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 1.2559 - accuracy: 0.4971 - val_loss: 0.9268 - val_accuracy: 0.6444\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.7755 - accuracy: 0.7227 - val_loss: 0.7812 - val_accuracy: 0.7152\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.6162 - accuracy: 0.7832 - val_loss: 0.7565 - val_accuracy: 0.7308\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.5262 - accuracy: 0.8143 - val_loss: 0.7613 - val_accuracy: 0.7375\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.4614 - accuracy: 0.8398 - val_loss: 0.7575 - val_accuracy: 0.7347\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.4104 - accuracy: 0.8590 - val_loss: 0.7790 - val_accuracy: 0.7246\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.3664 - accuracy: 0.8758 - val_loss: 0.8289 - val_accuracy: 0.7293\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8288 - accuracy: 0.7293\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.2728 - accuracy: 0.4866 - val_loss: 0.9817 - val_accuracy: 0.6639\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.7962 - accuracy: 0.7163 - val_loss: 0.7917 - val_accuracy: 0.7023\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.6367 - accuracy: 0.7757 - val_loss: 0.7553 - val_accuracy: 0.7289\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.5471 - accuracy: 0.8069 - val_loss: 0.7606 - val_accuracy: 0.7347\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.4841 - accuracy: 0.8316 - val_loss: 0.7538 - val_accuracy: 0.7320\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.4349 - accuracy: 0.8495 - val_loss: 0.7624 - val_accuracy: 0.7308\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.3921 - accuracy: 0.8654 - val_loss: 0.8101 - val_accuracy: 0.7289\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8100 - accuracy: 0.7289\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.2910 - accuracy: 0.4770 - val_loss: 1.0100 - val_accuracy: 0.6592\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.8217 - accuracy: 0.7069 - val_loss: 0.7995 - val_accuracy: 0.6991\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.6604 - accuracy: 0.7699 - val_loss: 0.7602 - val_accuracy: 0.7242\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.5702 - accuracy: 0.7995 - val_loss: 0.7522 - val_accuracy: 0.7351\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.5099 - accuracy: 0.8225 - val_loss: 0.7494 - val_accuracy: 0.7379\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.4631 - accuracy: 0.8408 - val_loss: 0.7542 - val_accuracy: 0.7359\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.4201 - accuracy: 0.8554 - val_loss: 0.8040 - val_accuracy: 0.7300\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8040 - accuracy: 0.7300\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 1.3109 - accuracy: 0.4728 - val_loss: 1.0070 - val_accuracy: 0.6412\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.8473 - accuracy: 0.6958 - val_loss: 0.8121 - val_accuracy: 0.6948\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.6836 - accuracy: 0.7608 - val_loss: 0.7627 - val_accuracy: 0.7234\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.5959 - accuracy: 0.7922 - val_loss: 0.7509 - val_accuracy: 0.7375\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.5372 - accuracy: 0.8130 - val_loss: 0.7462 - val_accuracy: 0.7351\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.4906 - accuracy: 0.8308 - val_loss: 0.7463 - val_accuracy: 0.7363\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.4500 - accuracy: 0.8439 - val_loss: 0.7777 - val_accuracy: 0.7406\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7776 - accuracy: 0.7406\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 19s 116ms/step - loss: 1.2814 - accuracy: 0.5020 - val_loss: 0.9757 - val_accuracy: 0.6581\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 19s 116ms/step - loss: 0.7813 - accuracy: 0.7261 - val_loss: 0.7842 - val_accuracy: 0.7101\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 19s 119ms/step - loss: 0.6163 - accuracy: 0.7833 - val_loss: 0.7570 - val_accuracy: 0.7281\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 18s 112ms/step - loss: 0.5263 - accuracy: 0.8149 - val_loss: 0.7473 - val_accuracy: 0.7359\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 18s 115ms/step - loss: 0.4618 - accuracy: 0.8388 - val_loss: 0.7600 - val_accuracy: 0.7285\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 18s 114ms/step - loss: 0.4118 - accuracy: 0.8555 - val_loss: 0.7930 - val_accuracy: 0.7254\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.3671 - accuracy: 0.8696 - val_loss: 0.8189 - val_accuracy: 0.7297\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.8187 - accuracy: 0.7297\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 19s 116ms/step - loss: 1.2572 - accuracy: 0.4972 - val_loss: 0.9430 - val_accuracy: 0.6647\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.7842 - accuracy: 0.7273 - val_loss: 0.7839 - val_accuracy: 0.7132\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.6271 - accuracy: 0.7838 - val_loss: 0.7518 - val_accuracy: 0.7332\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.5412 - accuracy: 0.8092 - val_loss: 0.7346 - val_accuracy: 0.7383\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 18s 112ms/step - loss: 0.4785 - accuracy: 0.8318 - val_loss: 0.7467 - val_accuracy: 0.7347\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.4288 - accuracy: 0.8504 - val_loss: 0.7831 - val_accuracy: 0.7265\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 18s 114ms/step - loss: 0.3868 - accuracy: 0.8643 - val_loss: 0.8015 - val_accuracy: 0.7332\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.8013 - accuracy: 0.7332\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 18s 116ms/step - loss: 1.2638 - accuracy: 0.4910 - val_loss: 0.9245 - val_accuracy: 0.6514\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.8036 - accuracy: 0.7188 - val_loss: 0.7934 - val_accuracy: 0.7089\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 18s 114ms/step - loss: 0.6483 - accuracy: 0.7752 - val_loss: 0.7572 - val_accuracy: 0.7289\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.5629 - accuracy: 0.8025 - val_loss: 0.7367 - val_accuracy: 0.7387\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 18s 114ms/step - loss: 0.5041 - accuracy: 0.8229 - val_loss: 0.7377 - val_accuracy: 0.7363\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.4559 - accuracy: 0.8415 - val_loss: 0.7730 - val_accuracy: 0.7265\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 18s 115ms/step - loss: 0.4153 - accuracy: 0.8528 - val_loss: 0.7810 - val_accuracy: 0.7387\n",
            "80/80 [==============================] - 2s 22ms/step - loss: 0.7808 - accuracy: 0.7387\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 18s 115ms/step - loss: 1.2822 - accuracy: 0.4847 - val_loss: 0.9465 - val_accuracy: 0.6506\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.8206 - accuracy: 0.7102 - val_loss: 0.7979 - val_accuracy: 0.7089\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.6647 - accuracy: 0.7672 - val_loss: 0.7645 - val_accuracy: 0.7297\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.5836 - accuracy: 0.7955 - val_loss: 0.7437 - val_accuracy: 0.7344\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 18s 114ms/step - loss: 0.5262 - accuracy: 0.8167 - val_loss: 0.7386 - val_accuracy: 0.7332\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.4796 - accuracy: 0.8310 - val_loss: 0.7659 - val_accuracy: 0.7297\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 18s 113ms/step - loss: 0.4418 - accuracy: 0.8447 - val_loss: 0.7669 - val_accuracy: 0.7398\n",
            "80/80 [==============================] - 2s 21ms/step - loss: 0.7668 - accuracy: 0.7398\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 1.2042 - accuracy: 0.5260 - val_loss: 0.8735 - val_accuracy: 0.6733\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.7145 - accuracy: 0.7471 - val_loss: 0.7469 - val_accuracy: 0.7261\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.5641 - accuracy: 0.8033 - val_loss: 0.7525 - val_accuracy: 0.7281\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.4784 - accuracy: 0.8319 - val_loss: 0.7593 - val_accuracy: 0.7347\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.4142 - accuracy: 0.8553 - val_loss: 0.7770 - val_accuracy: 0.7340\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.3622 - accuracy: 0.8739 - val_loss: 0.8016 - val_accuracy: 0.7265\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3197 - accuracy: 0.8904 - val_loss: 0.8934 - val_accuracy: 0.7128\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8934 - accuracy: 0.7128\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 33ms/step - loss: 1.2161 - accuracy: 0.5217 - val_loss: 0.8822 - val_accuracy: 0.6667\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.7271 - accuracy: 0.7397 - val_loss: 0.7535 - val_accuracy: 0.7210\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.5793 - accuracy: 0.7954 - val_loss: 0.7555 - val_accuracy: 0.7269\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.4961 - accuracy: 0.8254 - val_loss: 0.7538 - val_accuracy: 0.7371\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.4330 - accuracy: 0.8480 - val_loss: 0.7752 - val_accuracy: 0.7355\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.3825 - accuracy: 0.8670 - val_loss: 0.8018 - val_accuracy: 0.7281\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.3421 - accuracy: 0.8809 - val_loss: 0.8706 - val_accuracy: 0.7144\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8706 - accuracy: 0.7144\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 1.2335 - accuracy: 0.5101 - val_loss: 0.8968 - val_accuracy: 0.6510\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.7484 - accuracy: 0.7322 - val_loss: 0.7611 - val_accuracy: 0.7171\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.5992 - accuracy: 0.7897 - val_loss: 0.7547 - val_accuracy: 0.7246\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.5174 - accuracy: 0.8158 - val_loss: 0.7538 - val_accuracy: 0.7359\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.4560 - accuracy: 0.8408 - val_loss: 0.7606 - val_accuracy: 0.7371\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.4078 - accuracy: 0.8562 - val_loss: 0.7812 - val_accuracy: 0.7332\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.3679 - accuracy: 0.8721 - val_loss: 0.8391 - val_accuracy: 0.7195\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8391 - accuracy: 0.7195\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 33ms/step - loss: 1.2552 - accuracy: 0.4991 - val_loss: 0.9184 - val_accuracy: 0.6408\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.7725 - accuracy: 0.7232 - val_loss: 0.7748 - val_accuracy: 0.7132\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.6230 - accuracy: 0.7793 - val_loss: 0.7574 - val_accuracy: 0.7257\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.5434 - accuracy: 0.8073 - val_loss: 0.7565 - val_accuracy: 0.7344\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.4822 - accuracy: 0.8311 - val_loss: 0.7511 - val_accuracy: 0.7379\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.4369 - accuracy: 0.8446 - val_loss: 0.7635 - val_accuracy: 0.7351\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 0.3975 - accuracy: 0.8615 - val_loss: 0.8273 - val_accuracy: 0.7246\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.8273 - accuracy: 0.7246\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 1.1936 - accuracy: 0.5399 - val_loss: 0.9003 - val_accuracy: 0.6960\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 10s 62ms/step - loss: 0.7128 - accuracy: 0.7463 - val_loss: 0.7494 - val_accuracy: 0.7308\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.5690 - accuracy: 0.8007 - val_loss: 0.7471 - val_accuracy: 0.7387\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 10s 61ms/step - loss: 0.4808 - accuracy: 0.8306 - val_loss: 0.7474 - val_accuracy: 0.7390\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 10s 61ms/step - loss: 0.4168 - accuracy: 0.8550 - val_loss: 0.7807 - val_accuracy: 0.7336\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 10s 61ms/step - loss: 0.3640 - accuracy: 0.8726 - val_loss: 0.8111 - val_accuracy: 0.7257\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 10s 62ms/step - loss: 0.3192 - accuracy: 0.8893 - val_loss: 0.8629 - val_accuracy: 0.7214\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.8630 - accuracy: 0.7214\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 1.2007 - accuracy: 0.5324 - val_loss: 0.8979 - val_accuracy: 0.6897\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.7271 - accuracy: 0.7404 - val_loss: 0.7546 - val_accuracy: 0.7257\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.5841 - accuracy: 0.7948 - val_loss: 0.7458 - val_accuracy: 0.7398\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.4958 - accuracy: 0.8241 - val_loss: 0.7447 - val_accuracy: 0.7379\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.4351 - accuracy: 0.8482 - val_loss: 0.7608 - val_accuracy: 0.7359\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.3833 - accuracy: 0.8651 - val_loss: 0.7972 - val_accuracy: 0.7324\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 10s 62ms/step - loss: 0.3400 - accuracy: 0.8801 - val_loss: 0.8371 - val_accuracy: 0.7222\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.8371 - accuracy: 0.7222\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 1.2273 - accuracy: 0.5234 - val_loss: 0.9097 - val_accuracy: 0.6815\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.7507 - accuracy: 0.7302 - val_loss: 0.7609 - val_accuracy: 0.7265\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.6068 - accuracy: 0.7877 - val_loss: 0.7455 - val_accuracy: 0.7371\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.5215 - accuracy: 0.8162 - val_loss: 0.7366 - val_accuracy: 0.7441\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.4624 - accuracy: 0.8383 - val_loss: 0.7482 - val_accuracy: 0.7375\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.4126 - accuracy: 0.8558 - val_loss: 0.7802 - val_accuracy: 0.7320\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.3694 - accuracy: 0.8714 - val_loss: 0.8092 - val_accuracy: 0.7320\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.8092 - accuracy: 0.7320\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 1.2556 - accuracy: 0.5070 - val_loss: 0.9190 - val_accuracy: 0.6463\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.7794 - accuracy: 0.7208 - val_loss: 0.7744 - val_accuracy: 0.7156\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.6332 - accuracy: 0.7781 - val_loss: 0.7488 - val_accuracy: 0.7351\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.5470 - accuracy: 0.8069 - val_loss: 0.7421 - val_accuracy: 0.7414\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.4892 - accuracy: 0.8294 - val_loss: 0.7344 - val_accuracy: 0.7422\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.4447 - accuracy: 0.8436 - val_loss: 0.7563 - val_accuracy: 0.7347\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 0.4019 - accuracy: 0.8607 - val_loss: 0.7841 - val_accuracy: 0.7387\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.7841 - accuracy: 0.7387\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 1.1622 - accuracy: 0.5543 - val_loss: 0.8862 - val_accuracy: 0.6882\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.7126 - accuracy: 0.7523 - val_loss: 0.7481 - val_accuracy: 0.7289\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.5665 - accuracy: 0.8014 - val_loss: 0.7498 - val_accuracy: 0.7332\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.4784 - accuracy: 0.8325 - val_loss: 0.7467 - val_accuracy: 0.7437\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.4162 - accuracy: 0.8550 - val_loss: 0.7769 - val_accuracy: 0.7328\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.3620 - accuracy: 0.8737 - val_loss: 0.8254 - val_accuracy: 0.7246\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.3160 - accuracy: 0.8918 - val_loss: 0.8582 - val_accuracy: 0.7222\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.8582 - accuracy: 0.7222\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 1.1768 - accuracy: 0.5470 - val_loss: 0.8677 - val_accuracy: 0.6835\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 21s 130ms/step - loss: 0.7202 - accuracy: 0.7454 - val_loss: 0.7512 - val_accuracy: 0.7297\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.5773 - accuracy: 0.7977 - val_loss: 0.7492 - val_accuracy: 0.7304\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 21s 130ms/step - loss: 0.4918 - accuracy: 0.8273 - val_loss: 0.7413 - val_accuracy: 0.7418\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 21s 130ms/step - loss: 0.4332 - accuracy: 0.8494 - val_loss: 0.7777 - val_accuracy: 0.7293\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.3802 - accuracy: 0.8660 - val_loss: 0.8074 - val_accuracy: 0.7265\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.3351 - accuracy: 0.8838 - val_loss: 0.8434 - val_accuracy: 0.7285\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.8434 - accuracy: 0.7285\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 22s 135ms/step - loss: 1.2097 - accuracy: 0.5353 - val_loss: 0.8895 - val_accuracy: 0.6851\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.7470 - accuracy: 0.7377 - val_loss: 0.7592 - val_accuracy: 0.7281\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 22s 135ms/step - loss: 0.6024 - accuracy: 0.7888 - val_loss: 0.7529 - val_accuracy: 0.7277\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.5190 - accuracy: 0.8178 - val_loss: 0.7392 - val_accuracy: 0.7422\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.4587 - accuracy: 0.8380 - val_loss: 0.7660 - val_accuracy: 0.7308\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.4095 - accuracy: 0.8567 - val_loss: 0.7877 - val_accuracy: 0.7293\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.3635 - accuracy: 0.8712 - val_loss: 0.8136 - val_accuracy: 0.7308\n",
            "80/80 [==============================] - 2s 24ms/step - loss: 0.8135 - accuracy: 0.7308\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 1.2088 - accuracy: 0.5269 - val_loss: 0.8884 - val_accuracy: 0.6835\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.7642 - accuracy: 0.7318 - val_loss: 0.7686 - val_accuracy: 0.7226\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.6212 - accuracy: 0.7822 - val_loss: 0.7602 - val_accuracy: 0.7277\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.5394 - accuracy: 0.8102 - val_loss: 0.7420 - val_accuracy: 0.7433\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.4806 - accuracy: 0.8338 - val_loss: 0.7472 - val_accuracy: 0.7324\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 21s 130ms/step - loss: 0.4339 - accuracy: 0.8486 - val_loss: 0.7782 - val_accuracy: 0.7297\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.3905 - accuracy: 0.8632 - val_loss: 0.8209 - val_accuracy: 0.7269\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.8208 - accuracy: 0.7269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xejQKFqZrsw0",
        "colab_type": "text"
      },
      "source": [
        "#### Predict on the test set for kaggle\n",
        "\n",
        "72.085% on Kaggle vs 74.41% here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4CiSia6HwW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Running the best model on the test set\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model2 = Sequential() \n",
        "model2.add(Embedding(dict_len + 1, 128)) # adding trainable=True does not change the result, likely trainable by default\n",
        "model2.add(LSTM(128, dropout=0.2)) \n",
        "# number of classes = 5\n",
        "model2.add(Dense(5, activation=\"softmax\"))  \n",
        "model2.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H58q2cmFIBi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "f485b60e-d5e6-4566-f7b1-4f872b53ff87"
      },
      "source": [
        "# Train the model\n",
        "model2.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=4) \n",
        "model2.evaluate(X_test, Y_test) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 1.2273 - accuracy: 0.5234 - val_loss: 0.9097 - val_accuracy: 0.6815\n",
            "Epoch 2/4\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.7507 - accuracy: 0.7302 - val_loss: 0.7609 - val_accuracy: 0.7265\n",
            "Epoch 3/4\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.6068 - accuracy: 0.7877 - val_loss: 0.7455 - val_accuracy: 0.7371\n",
            "Epoch 4/4\n",
            "160/160 [==============================] - 10s 63ms/step - loss: 0.5215 - accuracy: 0.8162 - val_loss: 0.7366 - val_accuracy: 0.7441\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.7365 - accuracy: 0.7441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7365121841430664, 0.7441314458847046]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyC6D0mVyk5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c7ad16c3-a478-4fcf-f6f9-35444c7c3a39"
      },
      "source": [
        "Y_test_pred = model2.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[374, 104,  67,  18,  25],\n",
              "       [ 70, 466,  20,  27,   5],\n",
              "       [ 47,  25, 405,  42,  41],\n",
              "       [  9,  28,  40, 410,  15],\n",
              "       [ 25,   1,  34,  11, 247]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcHWsV6hy7tZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "0df29d48-8ccd-47a9-b1a6-a1084e5949a4"
      },
      "source": [
        "print(Y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw-kaJjfy9e8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "92544141-b780-4284-f859-1f3febba01f4"
      },
      "source": [
        "print(Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 3 2 ... 2 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cw0NFLAIBye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "45fc0cf1-7316-4863-cd08-ee2809e1f8c2"
      },
      "source": [
        "Y_new2 = model2.predict_classes(X_new)\n",
        "# show the inputs and predicted outputs\n",
        "print(X_new[:5])\n",
        "for i in range(5):\n",
        "\tprint(\"X=%s, Predicted=%s\" % (X_new[i], Y_new2[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    3  482\n",
            "     8   32   15    1  117   66]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 1591  130  758\n",
            "    20   66    1 1407  295  307]\n",
            " [   0    0    0    0    0    0   95    4  120    2   35   21  693 2005\n",
            "  6187    1   24    2  463  166]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0  378  229\n",
            "   793    1   47  278  399   14]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0   47  202  302  519]]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   3 482   8  32  15   1\n",
            " 117  66], Predicted=1\n",
            "X=[   0    0    0    0    0    0    0    0    0    0    0 1591  130  758\n",
            "   20   66    1 1407  295  307], Predicted=1\n",
            "X=[   0    0    0    0    0    0   95    4  120    2   35   21  693 2005\n",
            " 6187    1   24    2  463  166], Predicted=0\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0 378 229 793   1  47 278\n",
            " 399  14], Predicted=2\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  47 202\n",
            " 302 519], Predicted=2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1UH2QUvICEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = pd.DataFrame(list(zip(test['id'], Y_new2)), \n",
        "               columns = ['id', 'label'])\n",
        "test2\n",
        "test2.to_csv('test2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEJl50Vx5gtL",
        "colab_type": "text"
      },
      "source": [
        "###LSTM w 2 layers, doubling hidden_units in the second one\n",
        "\n",
        "No gain vs 1-layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjEIueYlIB9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b595830-17e6-4089-f3e5-a039f385a7fb"
      },
      "source": [
        "embed = [128]\n",
        "hidden = [64,128,256]\n",
        "drop = [0, 0.1, 0.2]\n",
        "# optim = ['adam','RMSprop']\n",
        "for i in embed:\n",
        "  for j in hidden:\n",
        "    for k in drop:\n",
        "      # for l in optim:\n",
        "      # print('emb_dim: ',i,' hidden_dim: ',j, ' drop: ',k, ' optim: ',l)\n",
        "      print('emb_dim:',i,' hidden_dim:',j, ' drop:',k,'----RMSprop')\n",
        "      np.random.seed(42)\n",
        "      random.seed(12345)\n",
        "      session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                    inter_op_parallelism_threads=1)\n",
        "      tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "      model = Sequential() \n",
        "      model.add(Embedding(dict_len + 1, output_dim=i)) \n",
        "      model.add(LSTM(j, dropout=k, recurrent_dropout=0.1, return_sequences=True))\n",
        "      model.add(LSTM(j*2, dropout=k, recurrent_dropout=0.1)) \n",
        "      model.add(Dense(5, activation=\"softmax\"))  \n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # Train the model\n",
        "      model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "      model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_dim: 128  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 1.2005 - accuracy: 0.5063 - val_loss: 0.8927 - val_accuracy: 0.6737\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.7358 - accuracy: 0.7362 - val_loss: 0.7685 - val_accuracy: 0.7179\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.5843 - accuracy: 0.7909 - val_loss: 0.7500 - val_accuracy: 0.7277\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.4949 - accuracy: 0.8240 - val_loss: 0.7609 - val_accuracy: 0.7351\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.4259 - accuracy: 0.8536 - val_loss: 0.7902 - val_accuracy: 0.7304\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.7902 - accuracy: 0.7304\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 20s 123ms/step - loss: 1.2122 - accuracy: 0.5003 - val_loss: 0.9012 - val_accuracy: 0.6631\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 19s 120ms/step - loss: 0.7551 - accuracy: 0.7268 - val_loss: 0.7771 - val_accuracy: 0.7113\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 19s 120ms/step - loss: 0.5980 - accuracy: 0.7860 - val_loss: 0.7527 - val_accuracy: 0.7254\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 19s 121ms/step - loss: 0.5101 - accuracy: 0.8184 - val_loss: 0.7613 - val_accuracy: 0.7285\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 19s 121ms/step - loss: 0.4421 - accuracy: 0.8440 - val_loss: 0.7787 - val_accuracy: 0.7293\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.7787 - accuracy: 0.7293\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 19s 121ms/step - loss: 1.2331 - accuracy: 0.4900 - val_loss: 0.9100 - val_accuracy: 0.6600\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 19s 118ms/step - loss: 0.7721 - accuracy: 0.7186 - val_loss: 0.7809 - val_accuracy: 0.7132\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 19s 118ms/step - loss: 0.6119 - accuracy: 0.7838 - val_loss: 0.7496 - val_accuracy: 0.7269\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 19s 120ms/step - loss: 0.5269 - accuracy: 0.8113 - val_loss: 0.7591 - val_accuracy: 0.7328\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 19s 120ms/step - loss: 0.4618 - accuracy: 0.8367 - val_loss: 0.7758 - val_accuracy: 0.7344\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 0.7758 - accuracy: 0.7344\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 45s 279ms/step - loss: 1.1658 - accuracy: 0.5290 - val_loss: 0.8633 - val_accuracy: 0.6811\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 44s 277ms/step - loss: 0.7062 - accuracy: 0.7481 - val_loss: 0.7453 - val_accuracy: 0.7324\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 44s 277ms/step - loss: 0.5587 - accuracy: 0.8037 - val_loss: 0.7397 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 44s 278ms/step - loss: 0.4683 - accuracy: 0.8362 - val_loss: 0.7595 - val_accuracy: 0.7359\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 44s 278ms/step - loss: 0.3979 - accuracy: 0.8588 - val_loss: 0.8033 - val_accuracy: 0.7234\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.8032 - accuracy: 0.7234\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 48s 301ms/step - loss: 1.1804 - accuracy: 0.5172 - val_loss: 0.8711 - val_accuracy: 0.6729\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 48s 300ms/step - loss: 0.7306 - accuracy: 0.7389 - val_loss: 0.7544 - val_accuracy: 0.7242\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 48s 300ms/step - loss: 0.5778 - accuracy: 0.7967 - val_loss: 0.7404 - val_accuracy: 0.7320\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 48s 298ms/step - loss: 0.4882 - accuracy: 0.8283 - val_loss: 0.7481 - val_accuracy: 0.7332\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 48s 297ms/step - loss: 0.4177 - accuracy: 0.8498 - val_loss: 0.7868 - val_accuracy: 0.7269\n",
            "80/80 [==============================] - 2s 26ms/step - loss: 0.7867 - accuracy: 0.7269\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 48s 298ms/step - loss: 1.1889 - accuracy: 0.5113 - val_loss: 0.8805 - val_accuracy: 0.6796\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 47s 296ms/step - loss: 0.7381 - accuracy: 0.7364 - val_loss: 0.7549 - val_accuracy: 0.7250\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 48s 297ms/step - loss: 0.5904 - accuracy: 0.7940 - val_loss: 0.7443 - val_accuracy: 0.7297\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 47s 295ms/step - loss: 0.5034 - accuracy: 0.8204 - val_loss: 0.7449 - val_accuracy: 0.7320\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 47s 295ms/step - loss: 0.4338 - accuracy: 0.8460 - val_loss: 0.7722 - val_accuracy: 0.7320\n",
            "80/80 [==============================] - 2s 28ms/step - loss: 0.7721 - accuracy: 0.7320\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 142s 887ms/step - loss: 1.1787 - accuracy: 0.5195 - val_loss: 0.8549 - val_accuracy: 0.6909\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 142s 891ms/step - loss: 0.7001 - accuracy: 0.7489 - val_loss: 0.7359 - val_accuracy: 0.7316\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 143s 891ms/step - loss: 0.5481 - accuracy: 0.8086 - val_loss: 0.7623 - val_accuracy: 0.7320\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 143s 891ms/step - loss: 0.4566 - accuracy: 0.8389 - val_loss: 0.7798 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 143s 891ms/step - loss: 0.3850 - accuracy: 0.8649 - val_loss: 0.8254 - val_accuracy: 0.7261\n",
            "80/80 [==============================] - 10s 126ms/step - loss: 0.8254 - accuracy: 0.7261\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 148s 926ms/step - loss: 1.1902 - accuracy: 0.5144 - val_loss: 0.8588 - val_accuracy: 0.6882\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 148s 923ms/step - loss: 0.7191 - accuracy: 0.7425 - val_loss: 0.7466 - val_accuracy: 0.7261\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 149s 931ms/step - loss: 0.5659 - accuracy: 0.8021 - val_loss: 0.7540 - val_accuracy: 0.7273\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 149s 933ms/step - loss: 0.4765 - accuracy: 0.8320 - val_loss: 0.7751 - val_accuracy: 0.7379\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 149s 928ms/step - loss: 0.4064 - accuracy: 0.8548 - val_loss: 0.7998 - val_accuracy: 0.7336\n",
            "80/80 [==============================] - 10s 128ms/step - loss: 0.7999 - accuracy: 0.7336\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 148s 924ms/step - loss: 1.2304 - accuracy: 0.4934 - val_loss: 0.8732 - val_accuracy: 0.6768\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 147s 921ms/step - loss: 0.7462 - accuracy: 0.7339 - val_loss: 0.7594 - val_accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 147s 921ms/step - loss: 0.5947 - accuracy: 0.7920 - val_loss: 0.7520 - val_accuracy: 0.7238\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 147s 920ms/step - loss: 0.5029 - accuracy: 0.8226 - val_loss: 0.7583 - val_accuracy: 0.7398\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 147s 919ms/step - loss: 0.4330 - accuracy: 0.8465 - val_loss: 0.7794 - val_accuracy: 0.7308\n",
            "80/80 [==============================] - 10s 122ms/step - loss: 0.7793 - accuracy: 0.7308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1B9mXbO5y10"
      },
      "source": [
        "###Bi-LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuOgCcbRalRA",
        "colab_type": "text"
      },
      "source": [
        "#### With 2 layers, doubling hidden_units in the second one\n",
        "\n",
        "No gain vs one-layer LSTM\n",
        "\n",
        "Tried also bi-LSTM with 5 layers, different emb_dim, hid_dim and dropout - no gain (even worse accuracy) - results not shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "padoFM3LaBR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75d3c22b-6b88-4fbf-c3ad-ce88bcf620f3"
      },
      "source": [
        "embed = [128]\n",
        "hidden = [64,128,256]\n",
        "drop = [0, 0.1, 0.2]\n",
        "# optim = ['adam','RMSprop']\n",
        "for i in embed:\n",
        "  for j in hidden:\n",
        "    for k in drop:\n",
        "      # for l in optim:\n",
        "      # print('emb_dim: ',i,' hidden_dim: ',j, ' drop: ',k, ' optim: ',l)\n",
        "      print('emb_dim:',i,' hidden_dim:',j, ' drop:',k,'----RMSprop')\n",
        "      np.random.seed(42)\n",
        "      random.seed(12345)\n",
        "      session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                    inter_op_parallelism_threads=1)\n",
        "      tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "      model = Sequential() \n",
        "      model.add(Embedding(dict_len + 1, output_dim=i)) \n",
        "      model.add(Bidirectional(LSTM(j, dropout=k, recurrent_dropout=0.1, return_sequences=True)))\n",
        "      model.add(Bidirectional(LSTM(j*2, dropout=k, recurrent_dropout=0.1))) \n",
        "      model.add(Dense(5, activation=\"softmax\"))  \n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # Train the model\n",
        "      model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "      model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_dim: 128  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 36s 228ms/step - loss: 1.0897 - accuracy: 0.5615 - val_loss: 0.8242 - val_accuracy: 0.7027\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 36s 224ms/step - loss: 0.6882 - accuracy: 0.7567 - val_loss: 0.7650 - val_accuracy: 0.7269\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 36s 224ms/step - loss: 0.5561 - accuracy: 0.8067 - val_loss: 0.7366 - val_accuracy: 0.7375\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 35s 221ms/step - loss: 0.4653 - accuracy: 0.8377 - val_loss: 0.7847 - val_accuracy: 0.7402\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 35s 221ms/step - loss: 0.3940 - accuracy: 0.8632 - val_loss: 0.8228 - val_accuracy: 0.7277\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.8228 - accuracy: 0.7277\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 42s 262ms/step - loss: 1.1065 - accuracy: 0.5522 - val_loss: 0.8085 - val_accuracy: 0.7054\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 0.6953 - accuracy: 0.7519 - val_loss: 0.7627 - val_accuracy: 0.7257\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.5608 - accuracy: 0.8035 - val_loss: 0.7376 - val_accuracy: 0.7351\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.4715 - accuracy: 0.8375 - val_loss: 0.7880 - val_accuracy: 0.7324\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.4006 - accuracy: 0.8614 - val_loss: 0.8100 - val_accuracy: 0.7273\n",
            "80/80 [==============================] - 2s 22ms/step - loss: 0.8100 - accuracy: 0.7273\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 1.1067 - accuracy: 0.5549 - val_loss: 0.8088 - val_accuracy: 0.7038\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.7098 - accuracy: 0.7471 - val_loss: 0.7754 - val_accuracy: 0.7144\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 0.5743 - accuracy: 0.8011 - val_loss: 0.7430 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.4873 - accuracy: 0.8302 - val_loss: 0.7751 - val_accuracy: 0.7383\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 0.4176 - accuracy: 0.8542 - val_loss: 0.8059 - val_accuracy: 0.7242\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 0.8059 - accuracy: 0.7242\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 107s 669ms/step - loss: 1.1326 - accuracy: 0.5448 - val_loss: 0.8260 - val_accuracy: 0.7042\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 107s 667ms/step - loss: 0.6964 - accuracy: 0.7535 - val_loss: 0.7785 - val_accuracy: 0.7148\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 107s 667ms/step - loss: 0.5566 - accuracy: 0.8057 - val_loss: 0.7553 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 106s 660ms/step - loss: 0.4616 - accuracy: 0.8396 - val_loss: 0.7900 - val_accuracy: 0.7363\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 106s 662ms/step - loss: 0.3914 - accuracy: 0.8625 - val_loss: 0.8321 - val_accuracy: 0.7195\n",
            "80/80 [==============================] - 5s 67ms/step - loss: 0.8321 - accuracy: 0.7195\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 117s 730ms/step - loss: 1.1048 - accuracy: 0.5494 - val_loss: 0.8240 - val_accuracy: 0.6980\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 117s 733ms/step - loss: 0.6962 - accuracy: 0.7544 - val_loss: 0.7603 - val_accuracy: 0.7218\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 117s 730ms/step - loss: 0.5571 - accuracy: 0.8050 - val_loss: 0.7623 - val_accuracy: 0.7238\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 118s 735ms/step - loss: 0.4676 - accuracy: 0.8377 - val_loss: 0.8043 - val_accuracy: 0.7300\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 118s 736ms/step - loss: 0.4011 - accuracy: 0.8603 - val_loss: 0.8355 - val_accuracy: 0.7207\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.8356 - accuracy: 0.7207\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 117s 733ms/step - loss: 1.1571 - accuracy: 0.5248 - val_loss: 0.8524 - val_accuracy: 0.6905\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 116s 726ms/step - loss: 0.7201 - accuracy: 0.7475 - val_loss: 0.7720 - val_accuracy: 0.7167\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 117s 730ms/step - loss: 0.5757 - accuracy: 0.7997 - val_loss: 0.7547 - val_accuracy: 0.7210\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 117s 730ms/step - loss: 0.4860 - accuracy: 0.8315 - val_loss: 0.7792 - val_accuracy: 0.7351\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 117s 731ms/step - loss: 0.4172 - accuracy: 0.8525 - val_loss: 0.8225 - val_accuracy: 0.7121\n",
            "80/80 [==============================] - 5s 68ms/step - loss: 0.8226 - accuracy: 0.7121\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 345s 2s/step - loss: 1.2195 - accuracy: 0.5067 - val_loss: 0.8496 - val_accuracy: 0.6874\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 337s 2s/step - loss: 0.7235 - accuracy: 0.7420 - val_loss: 0.7783 - val_accuracy: 0.7230\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 326s 2s/step - loss: 0.5676 - accuracy: 0.8036 - val_loss: 0.7529 - val_accuracy: 0.7293\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 330s 2s/step - loss: 0.4653 - accuracy: 0.8385 - val_loss: 0.7954 - val_accuracy: 0.7281\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 339s 2s/step - loss: 0.3882 - accuracy: 0.8634 - val_loss: 0.8155 - val_accuracy: 0.7210\n",
            "80/80 [==============================] - 23s 285ms/step - loss: 0.8156 - accuracy: 0.7210\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 342s 2s/step - loss: 1.1982 - accuracy: 0.5063 - val_loss: 0.8592 - val_accuracy: 0.6831\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 337s 2s/step - loss: 0.7314 - accuracy: 0.7342 - val_loss: 0.7870 - val_accuracy: 0.7128\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 338s 2s/step - loss: 0.5758 - accuracy: 0.8009 - val_loss: 0.7658 - val_accuracy: 0.7167\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 340s 2s/step - loss: 0.4761 - accuracy: 0.8358 - val_loss: 0.8019 - val_accuracy: 0.7230\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 339s 2s/step - loss: 0.4000 - accuracy: 0.8607 - val_loss: 0.8261 - val_accuracy: 0.7140\n",
            "80/80 [==============================] - 22s 278ms/step - loss: 0.8261 - accuracy: 0.7140\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 338s 2s/step - loss: 1.2124 - accuracy: 0.5033 - val_loss: 0.8421 - val_accuracy: 0.6882\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 336s 2s/step - loss: 0.7324 - accuracy: 0.7379 - val_loss: 0.7718 - val_accuracy: 0.7187\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 336s 2s/step - loss: 0.5804 - accuracy: 0.7977 - val_loss: 0.7425 - val_accuracy: 0.7277\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 335s 2s/step - loss: 0.4823 - accuracy: 0.8303 - val_loss: 0.7928 - val_accuracy: 0.7293\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 335s 2s/step - loss: 0.4070 - accuracy: 0.8583 - val_loss: 0.8052 - val_accuracy: 0.7250\n",
            "80/80 [==============================] - 22s 271ms/step - loss: 0.8051 - accuracy: 0.7250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W_z0Vm1LoM9",
        "colab_type": "text"
      },
      "source": [
        "## GRU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c2iyZAEICRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba09e994-ffb3-4510-e879-6e259113fd63"
      },
      "source": [
        "embed = [32,64,128]\n",
        "hidden = [64,128,256]\n",
        "drop = [0, 0.1, 0.2, 0.3]\n",
        "for i in embed:\n",
        "  for j in hidden:\n",
        "    for k in drop:\n",
        "      print('emb_dim:',i,' hidden_dim:',j, ' drop:',k,'----RMSprop')\n",
        "      np.random.seed(42)\n",
        "      random.seed(12345)\n",
        "      session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                    inter_op_parallelism_threads=1)\n",
        "      tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "      model = Sequential() \n",
        "      model.add(Embedding(dict_len + 1, output_dim=i)) \n",
        "      model.add(GRU(j, dropout=k)) \n",
        "      model.add(Dense(5, activation=\"softmax\"))  \n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # Train the model\n",
        "      model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=7) \n",
        "      model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb_dim: 32  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 1.3809 - accuracy: 0.4405 - val_loss: 1.0540 - val_accuracy: 0.6146\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.8531 - accuracy: 0.6901 - val_loss: 0.8065 - val_accuracy: 0.6937\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.6603 - accuracy: 0.7632 - val_loss: 0.7495 - val_accuracy: 0.7207\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5663 - accuracy: 0.8021 - val_loss: 0.7385 - val_accuracy: 0.7320\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5013 - accuracy: 0.8250 - val_loss: 0.7604 - val_accuracy: 0.7277\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.4506 - accuracy: 0.8453 - val_loss: 0.7725 - val_accuracy: 0.7312\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.4077 - accuracy: 0.8606 - val_loss: 0.8103 - val_accuracy: 0.7246\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8104 - accuracy: 0.7246\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 1.3977 - accuracy: 0.4256 - val_loss: 1.1032 - val_accuracy: 0.5978\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.8892 - accuracy: 0.6799 - val_loss: 0.8192 - val_accuracy: 0.6921\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.6840 - accuracy: 0.7534 - val_loss: 0.7520 - val_accuracy: 0.7191\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5899 - accuracy: 0.7916 - val_loss: 0.7357 - val_accuracy: 0.7328\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5246 - accuracy: 0.8169 - val_loss: 0.7456 - val_accuracy: 0.7304\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.4780 - accuracy: 0.8354 - val_loss: 0.7638 - val_accuracy: 0.7328\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.4355 - accuracy: 0.8473 - val_loss: 0.8048 - val_accuracy: 0.7222\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8049 - accuracy: 0.7222\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 1.4061 - accuracy: 0.4164 - val_loss: 1.1155 - val_accuracy: 0.5814\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.9015 - accuracy: 0.6667 - val_loss: 0.8250 - val_accuracy: 0.6890\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.7078 - accuracy: 0.7449 - val_loss: 0.7579 - val_accuracy: 0.7179\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.6175 - accuracy: 0.7796 - val_loss: 0.7422 - val_accuracy: 0.7293\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5542 - accuracy: 0.8047 - val_loss: 0.7409 - val_accuracy: 0.7336\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5095 - accuracy: 0.8231 - val_loss: 0.7642 - val_accuracy: 0.7324\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.4673 - accuracy: 0.8382 - val_loss: 0.7933 - val_accuracy: 0.7269\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7934 - accuracy: 0.7269\n",
            "emb_dim: 32  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 1.4338 - accuracy: 0.4021 - val_loss: 1.1686 - val_accuracy: 0.5458\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.9452 - accuracy: 0.6548 - val_loss: 0.8467 - val_accuracy: 0.6858\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.7407 - accuracy: 0.7340 - val_loss: 0.7715 - val_accuracy: 0.7152\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.6519 - accuracy: 0.7689 - val_loss: 0.7420 - val_accuracy: 0.7273\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5882 - accuracy: 0.7919 - val_loss: 0.7389 - val_accuracy: 0.7312\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5451 - accuracy: 0.8089 - val_loss: 0.7663 - val_accuracy: 0.7269\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5049 - accuracy: 0.8238 - val_loss: 0.7828 - val_accuracy: 0.7300\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7829 - accuracy: 0.7300\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.3033 - accuracy: 0.4770 - val_loss: 0.9533 - val_accuracy: 0.6416\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.7858 - accuracy: 0.7204 - val_loss: 0.7860 - val_accuracy: 0.7132\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.6302 - accuracy: 0.7765 - val_loss: 0.7518 - val_accuracy: 0.7304\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.5407 - accuracy: 0.8107 - val_loss: 0.7549 - val_accuracy: 0.7410\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.4799 - accuracy: 0.8331 - val_loss: 0.7676 - val_accuracy: 0.7402\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.4305 - accuracy: 0.8490 - val_loss: 0.7790 - val_accuracy: 0.7316\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.3872 - accuracy: 0.8669 - val_loss: 0.8305 - val_accuracy: 0.7285\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.8306 - accuracy: 0.7285\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.3180 - accuracy: 0.4651 - val_loss: 0.9679 - val_accuracy: 0.6307\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.8160 - accuracy: 0.7089 - val_loss: 0.7929 - val_accuracy: 0.7144\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.6557 - accuracy: 0.7687 - val_loss: 0.7566 - val_accuracy: 0.7254\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.5660 - accuracy: 0.8017 - val_loss: 0.7539 - val_accuracy: 0.7406\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.5054 - accuracy: 0.8249 - val_loss: 0.7573 - val_accuracy: 0.7402\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.4598 - accuracy: 0.8414 - val_loss: 0.7753 - val_accuracy: 0.7340\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.4201 - accuracy: 0.8541 - val_loss: 0.7988 - val_accuracy: 0.7344\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.7988 - accuracy: 0.7344\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 1.3310 - accuracy: 0.4576 - val_loss: 0.9866 - val_accuracy: 0.6283\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.8258 - accuracy: 0.7045 - val_loss: 0.7987 - val_accuracy: 0.7066\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.6722 - accuracy: 0.7610 - val_loss: 0.7640 - val_accuracy: 0.7238\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.5863 - accuracy: 0.7943 - val_loss: 0.7507 - val_accuracy: 0.7394\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.5286 - accuracy: 0.8154 - val_loss: 0.7501 - val_accuracy: 0.7394\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.4840 - accuracy: 0.8339 - val_loss: 0.7662 - val_accuracy: 0.7347\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.4466 - accuracy: 0.8452 - val_loss: 0.7823 - val_accuracy: 0.7316\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.7823 - accuracy: 0.7316\n",
            "emb_dim: 32  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.3597 - accuracy: 0.4427 - val_loss: 1.0305 - val_accuracy: 0.6099\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 35ms/step - loss: 0.8631 - accuracy: 0.6844 - val_loss: 0.8080 - val_accuracy: 0.7019\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.7054 - accuracy: 0.7461 - val_loss: 0.7864 - val_accuracy: 0.7183\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.6210 - accuracy: 0.7839 - val_loss: 0.7551 - val_accuracy: 0.7367\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.5647 - accuracy: 0.8028 - val_loss: 0.7436 - val_accuracy: 0.7355\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.5211 - accuracy: 0.8170 - val_loss: 0.7694 - val_accuracy: 0.7289\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.4858 - accuracy: 0.8301 - val_loss: 0.7655 - val_accuracy: 0.7363\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.7654 - accuracy: 0.7363\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 14s 85ms/step - loss: 1.2659 - accuracy: 0.4850 - val_loss: 0.9156 - val_accuracy: 0.6608\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.7858 - accuracy: 0.7198 - val_loss: 0.7948 - val_accuracy: 0.7031\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.6375 - accuracy: 0.7777 - val_loss: 0.7714 - val_accuracy: 0.7269\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.5487 - accuracy: 0.8069 - val_loss: 0.7519 - val_accuracy: 0.7379\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.4821 - accuracy: 0.8306 - val_loss: 0.7735 - val_accuracy: 0.7332\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 13s 84ms/step - loss: 0.4314 - accuracy: 0.8514 - val_loss: 0.7925 - val_accuracy: 0.7308\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.3845 - accuracy: 0.8690 - val_loss: 0.8393 - val_accuracy: 0.7308\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.8391 - accuracy: 0.7308\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 14s 85ms/step - loss: 1.2830 - accuracy: 0.4728 - val_loss: 0.9288 - val_accuracy: 0.6534\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.8039 - accuracy: 0.7104 - val_loss: 0.7989 - val_accuracy: 0.7015\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 85ms/step - loss: 0.6573 - accuracy: 0.7678 - val_loss: 0.7618 - val_accuracy: 0.7273\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 13s 84ms/step - loss: 0.5708 - accuracy: 0.7993 - val_loss: 0.7451 - val_accuracy: 0.7383\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.5029 - accuracy: 0.8222 - val_loss: 0.7688 - val_accuracy: 0.7297\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.4556 - accuracy: 0.8414 - val_loss: 0.7830 - val_accuracy: 0.7293\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 13s 84ms/step - loss: 0.4173 - accuracy: 0.8549 - val_loss: 0.8163 - val_accuracy: 0.7273\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.8162 - accuracy: 0.7273\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 14s 86ms/step - loss: 1.2950 - accuracy: 0.4631 - val_loss: 0.9758 - val_accuracy: 0.6428\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 13s 84ms/step - loss: 0.8287 - accuracy: 0.6987 - val_loss: 0.8002 - val_accuracy: 0.7019\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.6894 - accuracy: 0.7548 - val_loss: 0.7683 - val_accuracy: 0.7179\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.6038 - accuracy: 0.7869 - val_loss: 0.7571 - val_accuracy: 0.7285\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.5369 - accuracy: 0.8121 - val_loss: 0.7657 - val_accuracy: 0.7238\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.4919 - accuracy: 0.8303 - val_loss: 0.7673 - val_accuracy: 0.7316\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.4560 - accuracy: 0.8419 - val_loss: 0.7889 - val_accuracy: 0.7324\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.7888 - accuracy: 0.7324\n",
            "emb_dim: 32  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 13s 84ms/step - loss: 1.3193 - accuracy: 0.4522 - val_loss: 0.9710 - val_accuracy: 0.6346\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.8585 - accuracy: 0.6854 - val_loss: 0.8090 - val_accuracy: 0.6921\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.7208 - accuracy: 0.7425 - val_loss: 0.7710 - val_accuracy: 0.7195\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.6353 - accuracy: 0.7735 - val_loss: 0.7481 - val_accuracy: 0.7316\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.5737 - accuracy: 0.8004 - val_loss: 0.7632 - val_accuracy: 0.7238\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 0.5271 - accuracy: 0.8142 - val_loss: 0.7868 - val_accuracy: 0.7250\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 13s 83ms/step - loss: 0.4965 - accuracy: 0.8264 - val_loss: 0.7610 - val_accuracy: 0.7324\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.7610 - accuracy: 0.7324\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 1.2590 - accuracy: 0.5032 - val_loss: 0.9033 - val_accuracy: 0.6596\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.7491 - accuracy: 0.7294 - val_loss: 0.7727 - val_accuracy: 0.7132\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.6032 - accuracy: 0.7870 - val_loss: 0.7561 - val_accuracy: 0.7242\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5203 - accuracy: 0.8188 - val_loss: 0.7522 - val_accuracy: 0.7367\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4590 - accuracy: 0.8402 - val_loss: 0.7747 - val_accuracy: 0.7277\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.4102 - accuracy: 0.8613 - val_loss: 0.8105 - val_accuracy: 0.7148\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.3692 - accuracy: 0.8733 - val_loss: 0.8451 - val_accuracy: 0.7199\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8452 - accuracy: 0.7199\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 1.2810 - accuracy: 0.4926 - val_loss: 0.9223 - val_accuracy: 0.6545\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.7661 - accuracy: 0.7205 - val_loss: 0.7751 - val_accuracy: 0.7101\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.6192 - accuracy: 0.7823 - val_loss: 0.7592 - val_accuracy: 0.7222\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5381 - accuracy: 0.8116 - val_loss: 0.7508 - val_accuracy: 0.7336\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.4808 - accuracy: 0.8348 - val_loss: 0.7586 - val_accuracy: 0.7351\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.4322 - accuracy: 0.8516 - val_loss: 0.7885 - val_accuracy: 0.7230\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.3899 - accuracy: 0.8672 - val_loss: 0.8174 - val_accuracy: 0.7265\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8175 - accuracy: 0.7265\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 1.3009 - accuracy: 0.4781 - val_loss: 0.9425 - val_accuracy: 0.6432\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.7891 - accuracy: 0.7118 - val_loss: 0.7830 - val_accuracy: 0.7074\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.6407 - accuracy: 0.7735 - val_loss: 0.7699 - val_accuracy: 0.7167\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5605 - accuracy: 0.8043 - val_loss: 0.7502 - val_accuracy: 0.7359\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5051 - accuracy: 0.8229 - val_loss: 0.7530 - val_accuracy: 0.7355\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.4568 - accuracy: 0.8440 - val_loss: 0.7772 - val_accuracy: 0.7261\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.4190 - accuracy: 0.8551 - val_loss: 0.8055 - val_accuracy: 0.7281\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8055 - accuracy: 0.7281\n",
            "emb_dim: 64  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 1.3223 - accuracy: 0.4690 - val_loss: 0.9660 - val_accuracy: 0.6389\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.8171 - accuracy: 0.7002 - val_loss: 0.7951 - val_accuracy: 0.6991\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.6649 - accuracy: 0.7652 - val_loss: 0.7761 - val_accuracy: 0.7136\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5841 - accuracy: 0.7982 - val_loss: 0.7496 - val_accuracy: 0.7328\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5328 - accuracy: 0.8147 - val_loss: 0.7487 - val_accuracy: 0.7351\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.4895 - accuracy: 0.8303 - val_loss: 0.7640 - val_accuracy: 0.7289\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.4514 - accuracy: 0.8466 - val_loss: 0.7854 - val_accuracy: 0.7332\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7854 - accuracy: 0.7332\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 1.1940 - accuracy: 0.5276 - val_loss: 0.8624 - val_accuracy: 0.6831\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.7290 - accuracy: 0.7396 - val_loss: 0.7678 - val_accuracy: 0.7218\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.5980 - accuracy: 0.7896 - val_loss: 0.7560 - val_accuracy: 0.7257\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.5164 - accuracy: 0.8205 - val_loss: 0.7495 - val_accuracy: 0.7351\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.4521 - accuracy: 0.8423 - val_loss: 0.7761 - val_accuracy: 0.7273\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.4016 - accuracy: 0.8629 - val_loss: 0.8059 - val_accuracy: 0.7187\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.3569 - accuracy: 0.8776 - val_loss: 0.8377 - val_accuracy: 0.7183\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.8377 - accuracy: 0.7183\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 1.2116 - accuracy: 0.5197 - val_loss: 0.8712 - val_accuracy: 0.6811\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.7479 - accuracy: 0.7348 - val_loss: 0.7730 - val_accuracy: 0.7242\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.6150 - accuracy: 0.7817 - val_loss: 0.7526 - val_accuracy: 0.7261\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.5322 - accuracy: 0.8145 - val_loss: 0.7427 - val_accuracy: 0.7367\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.4713 - accuracy: 0.8345 - val_loss: 0.7652 - val_accuracy: 0.7304\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.4214 - accuracy: 0.8587 - val_loss: 0.7871 - val_accuracy: 0.7250\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.3796 - accuracy: 0.8696 - val_loss: 0.8110 - val_accuracy: 0.7234\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.8110 - accuracy: 0.7234\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 1.2287 - accuracy: 0.5116 - val_loss: 0.8823 - val_accuracy: 0.6753\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.7650 - accuracy: 0.7257 - val_loss: 0.7782 - val_accuracy: 0.7230\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.6329 - accuracy: 0.7753 - val_loss: 0.7503 - val_accuracy: 0.7293\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.5524 - accuracy: 0.8064 - val_loss: 0.7362 - val_accuracy: 0.7355\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.4940 - accuracy: 0.8252 - val_loss: 0.7548 - val_accuracy: 0.7281\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.4454 - accuracy: 0.8474 - val_loss: 0.7676 - val_accuracy: 0.7308\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.4074 - accuracy: 0.8579 - val_loss: 0.7904 - val_accuracy: 0.7340\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.7904 - accuracy: 0.7340\n",
            "emb_dim: 64  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 1.2489 - accuracy: 0.5037 - val_loss: 0.8974 - val_accuracy: 0.6667\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.7881 - accuracy: 0.7152 - val_loss: 0.7893 - val_accuracy: 0.7171\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.6573 - accuracy: 0.7664 - val_loss: 0.7586 - val_accuracy: 0.7254\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.5768 - accuracy: 0.7946 - val_loss: 0.7343 - val_accuracy: 0.7336\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 6s 41ms/step - loss: 0.5168 - accuracy: 0.8170 - val_loss: 0.7523 - val_accuracy: 0.7304\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.4757 - accuracy: 0.8366 - val_loss: 0.7539 - val_accuracy: 0.7390\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.4406 - accuracy: 0.8475 - val_loss: 0.7711 - val_accuracy: 0.7359\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.7711 - accuracy: 0.7359\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 1.1686 - accuracy: 0.5393 - val_loss: 0.8547 - val_accuracy: 0.6819\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.7267 - accuracy: 0.7420 - val_loss: 0.7639 - val_accuracy: 0.7222\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.5901 - accuracy: 0.7953 - val_loss: 0.7536 - val_accuracy: 0.7300\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.5078 - accuracy: 0.8210 - val_loss: 0.7514 - val_accuracy: 0.7371\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4425 - accuracy: 0.8436 - val_loss: 0.7795 - val_accuracy: 0.7328\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.3914 - accuracy: 0.8639 - val_loss: 0.8179 - val_accuracy: 0.7297\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.3471 - accuracy: 0.8802 - val_loss: 0.8574 - val_accuracy: 0.7304\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.8573 - accuracy: 0.7304\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 1.1819 - accuracy: 0.5305 - val_loss: 0.8626 - val_accuracy: 0.6776\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.7441 - accuracy: 0.7334 - val_loss: 0.7680 - val_accuracy: 0.7218\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.6076 - accuracy: 0.7875 - val_loss: 0.7563 - val_accuracy: 0.7257\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.5247 - accuracy: 0.8149 - val_loss: 0.7457 - val_accuracy: 0.7418\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.4623 - accuracy: 0.8389 - val_loss: 0.7738 - val_accuracy: 0.7308\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.4152 - accuracy: 0.8552 - val_loss: 0.8051 - val_accuracy: 0.7336\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.3731 - accuracy: 0.8688 - val_loss: 0.8300 - val_accuracy: 0.7320\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.8300 - accuracy: 0.7320\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 1.2043 - accuracy: 0.5201 - val_loss: 0.8704 - val_accuracy: 0.6768\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.7619 - accuracy: 0.7248 - val_loss: 0.7794 - val_accuracy: 0.7148\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.6269 - accuracy: 0.7807 - val_loss: 0.7542 - val_accuracy: 0.7238\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.5477 - accuracy: 0.8065 - val_loss: 0.7387 - val_accuracy: 0.7402\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.4866 - accuracy: 0.8317 - val_loss: 0.7615 - val_accuracy: 0.7304\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.4406 - accuracy: 0.8437 - val_loss: 0.7877 - val_accuracy: 0.7277\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4023 - accuracy: 0.8584 - val_loss: 0.8044 - val_accuracy: 0.7371\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.8044 - accuracy: 0.7371\n",
            "emb_dim: 64  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 1.2254 - accuracy: 0.5074 - val_loss: 0.8836 - val_accuracy: 0.6792\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.7800 - accuracy: 0.7201 - val_loss: 0.7859 - val_accuracy: 0.7121\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 14s 91ms/step - loss: 0.6465 - accuracy: 0.7734 - val_loss: 0.7551 - val_accuracy: 0.7246\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 14s 91ms/step - loss: 0.5676 - accuracy: 0.7974 - val_loss: 0.7367 - val_accuracy: 0.7390\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.5080 - accuracy: 0.8225 - val_loss: 0.7536 - val_accuracy: 0.7398\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.4684 - accuracy: 0.8362 - val_loss: 0.7601 - val_accuracy: 0.7359\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4316 - accuracy: 0.8499 - val_loss: 0.7820 - val_accuracy: 0.7398\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.7820 - accuracy: 0.7398\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 1.1642 - accuracy: 0.5550 - val_loss: 0.8388 - val_accuracy: 0.6800\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 0.6829 - accuracy: 0.7561 - val_loss: 0.7422 - val_accuracy: 0.7285\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 0.5480 - accuracy: 0.8067 - val_loss: 0.7570 - val_accuracy: 0.7308\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 0.4641 - accuracy: 0.8373 - val_loss: 0.7463 - val_accuracy: 0.7367\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 0.4023 - accuracy: 0.8612 - val_loss: 0.7928 - val_accuracy: 0.7269\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 0.3507 - accuracy: 0.8807 - val_loss: 0.8393 - val_accuracy: 0.7132\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 0.3088 - accuracy: 0.8944 - val_loss: 0.8653 - val_accuracy: 0.7136\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8652 - accuracy: 0.7136\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 1.1860 - accuracy: 0.5465 - val_loss: 0.8514 - val_accuracy: 0.6757\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 33ms/step - loss: 0.7000 - accuracy: 0.7488 - val_loss: 0.7438 - val_accuracy: 0.7261\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.5640 - accuracy: 0.8014 - val_loss: 0.7576 - val_accuracy: 0.7304\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.4818 - accuracy: 0.8305 - val_loss: 0.7430 - val_accuracy: 0.7387\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.4221 - accuracy: 0.8541 - val_loss: 0.7798 - val_accuracy: 0.7300\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3719 - accuracy: 0.8739 - val_loss: 0.8203 - val_accuracy: 0.7195\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3300 - accuracy: 0.8866 - val_loss: 0.8463 - val_accuracy: 0.7183\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8461 - accuracy: 0.7183\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 31ms/step - loss: 1.2091 - accuracy: 0.5377 - val_loss: 0.8672 - val_accuracy: 0.6706\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.7179 - accuracy: 0.7433 - val_loss: 0.7512 - val_accuracy: 0.7214\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 0.5819 - accuracy: 0.7948 - val_loss: 0.7629 - val_accuracy: 0.7308\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.5033 - accuracy: 0.8263 - val_loss: 0.7411 - val_accuracy: 0.7390\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.4456 - accuracy: 0.8443 - val_loss: 0.7710 - val_accuracy: 0.7297\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3954 - accuracy: 0.8639 - val_loss: 0.8035 - val_accuracy: 0.7210\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3578 - accuracy: 0.8753 - val_loss: 0.8237 - val_accuracy: 0.7195\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.8236 - accuracy: 0.7195\n",
            "emb_dim: 128  hidden_dim: 64  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 1.2315 - accuracy: 0.5252 - val_loss: 0.8827 - val_accuracy: 0.6608\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.7406 - accuracy: 0.7338 - val_loss: 0.7576 - val_accuracy: 0.7199\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 4s 27ms/step - loss: 0.6046 - accuracy: 0.7871 - val_loss: 0.7658 - val_accuracy: 0.7265\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.5261 - accuracy: 0.8184 - val_loss: 0.7389 - val_accuracy: 0.7375\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.4698 - accuracy: 0.8367 - val_loss: 0.7616 - val_accuracy: 0.7344\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.4223 - accuracy: 0.8519 - val_loss: 0.7899 - val_accuracy: 0.7257\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 0.3860 - accuracy: 0.8647 - val_loss: 0.7998 - val_accuracy: 0.7242\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7997 - accuracy: 0.7242\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 1.1122 - accuracy: 0.5725 - val_loss: 0.8132 - val_accuracy: 0.7046\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.6734 - accuracy: 0.7630 - val_loss: 0.7353 - val_accuracy: 0.7328\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.5415 - accuracy: 0.8111 - val_loss: 0.7446 - val_accuracy: 0.7367\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.4570 - accuracy: 0.8400 - val_loss: 0.7492 - val_accuracy: 0.7387\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3960 - accuracy: 0.8625 - val_loss: 0.8058 - val_accuracy: 0.7269\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3457 - accuracy: 0.8784 - val_loss: 0.8340 - val_accuracy: 0.7238\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3033 - accuracy: 0.8946 - val_loss: 0.8765 - val_accuracy: 0.7152\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.8765 - accuracy: 0.7152\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 1.1289 - accuracy: 0.5634 - val_loss: 0.8287 - val_accuracy: 0.7038\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.6899 - accuracy: 0.7544 - val_loss: 0.7389 - val_accuracy: 0.7328\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.5596 - accuracy: 0.8048 - val_loss: 0.7420 - val_accuracy: 0.7398\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4765 - accuracy: 0.8340 - val_loss: 0.7421 - val_accuracy: 0.7422\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4151 - accuracy: 0.8548 - val_loss: 0.7907 - val_accuracy: 0.7328\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.3675 - accuracy: 0.8714 - val_loss: 0.8130 - val_accuracy: 0.7238\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3255 - accuracy: 0.8856 - val_loss: 0.8435 - val_accuracy: 0.7257\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8434 - accuracy: 0.7257\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 1.1452 - accuracy: 0.5563 - val_loss: 0.8419 - val_accuracy: 0.7027\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.7031 - accuracy: 0.7501 - val_loss: 0.7439 - val_accuracy: 0.7285\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.5744 - accuracy: 0.7990 - val_loss: 0.7440 - val_accuracy: 0.7398\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4943 - accuracy: 0.8265 - val_loss: 0.7394 - val_accuracy: 0.7410\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4371 - accuracy: 0.8456 - val_loss: 0.7801 - val_accuracy: 0.7308\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.3899 - accuracy: 0.8643 - val_loss: 0.7956 - val_accuracy: 0.7277\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3524 - accuracy: 0.8753 - val_loss: 0.8275 - val_accuracy: 0.7308\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.8274 - accuracy: 0.7308\n",
            "emb_dim: 128  hidden_dim: 128  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 1.1648 - accuracy: 0.5460 - val_loss: 0.8545 - val_accuracy: 0.6937\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.7220 - accuracy: 0.7427 - val_loss: 0.7487 - val_accuracy: 0.7285\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.5971 - accuracy: 0.7905 - val_loss: 0.7539 - val_accuracy: 0.7316\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.5213 - accuracy: 0.8175 - val_loss: 0.7371 - val_accuracy: 0.7410\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4641 - accuracy: 0.8352 - val_loss: 0.7671 - val_accuracy: 0.7371\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.4192 - accuracy: 0.8523 - val_loss: 0.7771 - val_accuracy: 0.7355\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3820 - accuracy: 0.8665 - val_loss: 0.7961 - val_accuracy: 0.7340\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.7960 - accuracy: 0.7340\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 1.1422 - accuracy: 0.5721 - val_loss: 0.8421 - val_accuracy: 0.6768\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.6991 - accuracy: 0.7538 - val_loss: 0.7472 - val_accuracy: 0.7336\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.5643 - accuracy: 0.8027 - val_loss: 0.7473 - val_accuracy: 0.7375\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 16s 100ms/step - loss: 0.4765 - accuracy: 0.8343 - val_loss: 0.7452 - val_accuracy: 0.7449\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 16s 100ms/step - loss: 0.4099 - accuracy: 0.8559 - val_loss: 0.7948 - val_accuracy: 0.7293\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 16s 101ms/step - loss: 0.3541 - accuracy: 0.8767 - val_loss: 0.8241 - val_accuracy: 0.7261\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.3031 - accuracy: 0.8947 - val_loss: 0.8871 - val_accuracy: 0.7203\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.8870 - accuracy: 0.7203\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.1 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 1.1242 - accuracy: 0.5647 - val_loss: 0.8242 - val_accuracy: 0.6874\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 16s 100ms/step - loss: 0.7049 - accuracy: 0.7478 - val_loss: 0.7443 - val_accuracy: 0.7336\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.5731 - accuracy: 0.8015 - val_loss: 0.7462 - val_accuracy: 0.7379\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 16s 101ms/step - loss: 0.4885 - accuracy: 0.8298 - val_loss: 0.7436 - val_accuracy: 0.7441\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 0.4239 - accuracy: 0.8501 - val_loss: 0.7860 - val_accuracy: 0.7312\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.3702 - accuracy: 0.8719 - val_loss: 0.8176 - val_accuracy: 0.7304\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 16s 101ms/step - loss: 0.3229 - accuracy: 0.8882 - val_loss: 0.8610 - val_accuracy: 0.7257\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.8609 - accuracy: 0.7257\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.2 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 105ms/step - loss: 1.1281 - accuracy: 0.5644 - val_loss: 0.8403 - val_accuracy: 0.6862\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.7170 - accuracy: 0.7456 - val_loss: 0.7498 - val_accuracy: 0.7281\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.5866 - accuracy: 0.7955 - val_loss: 0.7528 - val_accuracy: 0.7355\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.5048 - accuracy: 0.8248 - val_loss: 0.7415 - val_accuracy: 0.7445\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.4442 - accuracy: 0.8411 - val_loss: 0.7750 - val_accuracy: 0.7340\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 17s 103ms/step - loss: 0.3911 - accuracy: 0.8658 - val_loss: 0.7984 - val_accuracy: 0.7300\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.3477 - accuracy: 0.8781 - val_loss: 0.8341 - val_accuracy: 0.7324\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.8339 - accuracy: 0.7324\n",
            "emb_dim: 128  hidden_dim: 256  drop: 0.3 ----RMSprop\n",
            "Epoch 1/7\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 1.1402 - accuracy: 0.5562 - val_loss: 0.8320 - val_accuracy: 0.6815\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 17s 103ms/step - loss: 0.7347 - accuracy: 0.7366 - val_loss: 0.7519 - val_accuracy: 0.7210\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.6058 - accuracy: 0.7881 - val_loss: 0.7487 - val_accuracy: 0.7355\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 0.5297 - accuracy: 0.8127 - val_loss: 0.7372 - val_accuracy: 0.7449\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 17s 103ms/step - loss: 0.4680 - accuracy: 0.8318 - val_loss: 0.7600 - val_accuracy: 0.7367\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.4218 - accuracy: 0.8524 - val_loss: 0.7837 - val_accuracy: 0.7367\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 17s 103ms/step - loss: 0.3782 - accuracy: 0.8683 - val_loss: 0.8126 - val_accuracy: 0.7308\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 0.8125 - accuracy: 0.7308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG1BIwzVnBTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "6a25f5a2-70e1-4401-b97a-8202136f10ef"
      },
      "source": [
        "# best simple GRU, learnt embeddings\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=128, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"GRU\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=4) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "160/160 [==============================] - 17s 104ms/step - loss: 1.1402 - accuracy: 0.5562 - val_loss: 0.8320 - val_accuracy: 0.6815\n",
            "Epoch 2/4\n",
            "160/160 [==============================] - 17s 103ms/step - loss: 0.7347 - accuracy: 0.7366 - val_loss: 0.7519 - val_accuracy: 0.7210\n",
            "Epoch 3/4\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.6058 - accuracy: 0.7881 - val_loss: 0.7487 - val_accuracy: 0.7355\n",
            "Epoch 4/4\n",
            "160/160 [==============================] - 16s 103ms/step - loss: 0.5297 - accuracy: 0.8127 - val_loss: 0.7372 - val_accuracy: 0.7449\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.7370 - accuracy: 0.7449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7370414137840271, 0.7449139356613159]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AI9EGOix0-wp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "2dbe48cb-f64f-4da9-a54a-4c338713e956"
      },
      "source": [
        "# GRU(256)\n",
        "Y_test_pred = model.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[403,  91,  58,  14,  22],\n",
              "       [ 77, 459,  25,  22,   5],\n",
              "       [ 56,  26, 400,  38,  40],\n",
              "       [ 13,  29,  44, 402,  14],\n",
              "       [ 33,   2,  32,  11, 240]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YOXbsb_q1QVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c7ad16c3-a478-4fcf-f6f9-35444c7c3a39"
      },
      "source": [
        "# LSTM-128 RMSprop\n",
        "Y_test_pred = model2.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[374, 104,  67,  18,  25],\n",
              "       [ 70, 466,  20,  27,   5],\n",
              "       [ 47,  25, 405,  42,  41],\n",
              "       [  9,  28,  40, 410,  15],\n",
              "       [ 25,   1,  34,  11, 247]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsJAeGR2b_T2",
        "colab_type": "text"
      },
      "source": [
        "## Models with and without GloVe\n",
        "\n",
        "Overall, accuracy with GloVe is smaller than without (keeping everything else constant and allowing GloVe embeddings to train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNtcy-2OgI1",
        "colab_type": "text"
      },
      "source": [
        "#### Getting GloVe data and matching it to our text.\n",
        "\n",
        "Due to technical difficulties, the file was uploaded manually from the local drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJzWG2tIg71k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bad05d10-d0ce-4f8b-c0e4-ab6e3b37c2b6"
      },
      "source": [
        "# From https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html \n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 65\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgM5bakyd4Mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "5f6cd9d0-b372-4a36-a769-94999466b47c"
      },
      "source": [
        "# Filter GloVE vectors to specific task \n",
        "def filter_glove(vocabulary_dict, glove_dict, wordvec_dim=100): \n",
        "  # Create a matrix to store the vectors \n",
        "  # 0 in vocabulary is reserved for padding    \n",
        "  embedding_matrix = np.zeros((len(vocabulary_dict) + 1, wordvec_dim)) \n",
        "  for word, i in vocabulary_dict.items():\n",
        "    embedding_vector = glove_dict.get(word) \n",
        "    if embedding_vector is not None:             \n",
        "      # words not found in the glove_dict will be all-zeros.             \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return embedding_matrix\n",
        "\n",
        "emb_mat = filter_glove(vocabulary_dict=word_index, glove_dict=embeddings_index)\n",
        "print(emb_mat[:10])\n",
        "plt.plot(emb_mat.sum(axis=1))\n",
        "plt.show()\n",
        "plt.hist(emb_mat.sum(axis=1))\n",
        "plt.show()\n",
        "(emb_mat.sum(axis=1)==0).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiAIPkaEvCAIgo8RIS8IguBj/h9p0QyB72KIUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQSS1DmDQJI69/8B/PYmSJyiL5AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRYW3dnPglW",
        "colab_type": "text"
      },
      "source": [
        "#### Trying multi-layer models with different components\n",
        "\n",
        "With GloVe, somewhat better accuracy, but does not beat simple models above.\n",
        "\n",
        "Smaller model (64 hidden units everywhere, 4 epochs) - 0.7269\n",
        "\n",
        "Larger model (128 hidden units everywhere, 4 epochs) - 0.7300\n",
        "\n",
        "With Glove, smaller (64 hu everywhere, 9 epochs) - 0.7363\n",
        "\n",
        "With Glove, larger (128 hu everywhere, 11 epochs) - 0.7320 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihNpZP_nY2S0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "59f1c5c0-ef6f-400a-aecd-9f8b503dd1e1"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=128)) \n",
        "model.add(Dense(128, activation='relu', name=\"Dense1\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.15, name=\"LSTM\")) \n",
        "model.add(GRU(64, return_sequences=False, dropout=0.15, name=\"GRU\")) \n",
        "model.add(Dense(64, name=\"Dense2\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(Dense(32, name=\"Dense3\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 1.2399 - accuracy: 0.4771 - val_loss: 0.9397 - val_accuracy: 0.6416\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 9s 53ms/step - loss: 0.7857 - accuracy: 0.7154 - val_loss: 0.7758 - val_accuracy: 0.7250\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.6039 - accuracy: 0.7896 - val_loss: 0.7769 - val_accuracy: 0.7218\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.5088 - accuracy: 0.8217 - val_loss: 0.7927 - val_accuracy: 0.7269\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.4314 - accuracy: 0.8519 - val_loss: 0.8006 - val_accuracy: 0.7230\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3755 - accuracy: 0.8725 - val_loss: 0.8606 - val_accuracy: 0.7136\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.3348 - accuracy: 0.8850 - val_loss: 0.9206 - val_accuracy: 0.7097\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.2981 - accuracy: 0.8965 - val_loss: 1.0497 - val_accuracy: 0.6984\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.2664 - accuracy: 0.9084 - val_loss: 1.0032 - val_accuracy: 0.7062\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.2520 - accuracy: 0.9172 - val_loss: 1.0635 - val_accuracy: 0.6933\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 1.0634 - accuracy: 0.6933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0634212493896484, 0.693270742893219]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_anCEnB_bmMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "4b095e31-f9bf-4f25-cbdb-33f8541ae7d2"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=128)) \n",
        "model.add(Dense(128, activation='relu', name=\"Dense1\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(LSTM(128, return_sequences=True, dropout=0.15, name=\"LSTM\")) \n",
        "model.add(GRU(128, return_sequences=False, dropout=0.15, name=\"GRU\")) \n",
        "model.add(Dense(64, name=\"Dense2\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(Dense(32, name=\"Dense3\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 18s 112ms/step - loss: 1.1814 - accuracy: 0.5187 - val_loss: 0.8764 - val_accuracy: 0.6909\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 0.7437 - accuracy: 0.7366 - val_loss: 0.7643 - val_accuracy: 0.7187\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 0.5937 - accuracy: 0.7962 - val_loss: 0.7642 - val_accuracy: 0.7210\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 0.5015 - accuracy: 0.8265 - val_loss: 0.8001 - val_accuracy: 0.7300\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 18s 110ms/step - loss: 0.4300 - accuracy: 0.8500 - val_loss: 0.8042 - val_accuracy: 0.7261\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 17s 108ms/step - loss: 0.3736 - accuracy: 0.8682 - val_loss: 0.8351 - val_accuracy: 0.7210\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 18s 110ms/step - loss: 0.3293 - accuracy: 0.8858 - val_loss: 0.9160 - val_accuracy: 0.7164\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 18s 110ms/step - loss: 0.3004 - accuracy: 0.8991 - val_loss: 1.0306 - val_accuracy: 0.6980\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 17s 109ms/step - loss: 0.2675 - accuracy: 0.9112 - val_loss: 1.0087 - val_accuracy: 0.7031\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 18s 109ms/step - loss: 0.2465 - accuracy: 0.9186 - val_loss: 1.0573 - val_accuracy: 0.6894\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 1.0571 - accuracy: 0.6894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.057132601737976, 0.6893583536148071]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89NxM_chbSTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "c7729f0d-07f1-4539-8a2d-126a5479f693"
      },
      "source": [
        "# from DataCamp example, using pre-trained gloVe embeddings\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(emb_mat),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(Dense(128, activation='relu', name=\"Dense1\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.15, name=\"LSTM\")) \n",
        "model.add(GRU(64, return_sequences=False, dropout=0.15, name=\"GRU\")) \n",
        "model.add(Dense(64, name=\"Dense2\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(Dense(32, name=\"Dense3\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=20) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 1.2301 - accuracy: 0.4977 - val_loss: 0.9351 - val_accuracy: 0.6408\n",
            "Epoch 2/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.9486 - accuracy: 0.6433 - val_loss: 0.8585 - val_accuracy: 0.6729\n",
            "Epoch 3/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.8423 - accuracy: 0.6876 - val_loss: 0.7844 - val_accuracy: 0.7074\n",
            "Epoch 4/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.7644 - accuracy: 0.7189 - val_loss: 0.7624 - val_accuracy: 0.7160\n",
            "Epoch 5/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.7044 - accuracy: 0.7452 - val_loss: 0.7536 - val_accuracy: 0.7113\n",
            "Epoch 6/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.6542 - accuracy: 0.7622 - val_loss: 0.7394 - val_accuracy: 0.7289\n",
            "Epoch 7/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.6081 - accuracy: 0.7829 - val_loss: 0.7714 - val_accuracy: 0.7214\n",
            "Epoch 8/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.5773 - accuracy: 0.7936 - val_loss: 0.7494 - val_accuracy: 0.7300\n",
            "Epoch 9/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.5433 - accuracy: 0.8074 - val_loss: 0.7338 - val_accuracy: 0.7363\n",
            "Epoch 10/20\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.5093 - accuracy: 0.8193 - val_loss: 0.7571 - val_accuracy: 0.7289\n",
            "Epoch 11/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.4769 - accuracy: 0.8278 - val_loss: 0.7562 - val_accuracy: 0.7355\n",
            "Epoch 12/20\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.4415 - accuracy: 0.8436 - val_loss: 0.8507 - val_accuracy: 0.7175\n",
            "Epoch 13/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.4249 - accuracy: 0.8496 - val_loss: 0.8198 - val_accuracy: 0.7285\n",
            "Epoch 14/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3906 - accuracy: 0.8616 - val_loss: 0.8705 - val_accuracy: 0.7269\n",
            "Epoch 15/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3677 - accuracy: 0.8716 - val_loss: 0.8710 - val_accuracy: 0.7242\n",
            "Epoch 16/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3494 - accuracy: 0.8784 - val_loss: 0.8978 - val_accuracy: 0.7164\n",
            "Epoch 17/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3313 - accuracy: 0.8853 - val_loss: 0.8746 - val_accuracy: 0.7234\n",
            "Epoch 18/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.3114 - accuracy: 0.8903 - val_loss: 0.9238 - val_accuracy: 0.7191\n",
            "Epoch 19/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.2891 - accuracy: 0.9004 - val_loss: 0.9649 - val_accuracy: 0.7148\n",
            "Epoch 20/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.2784 - accuracy: 0.9016 - val_loss: 0.9917 - val_accuracy: 0.7140\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.9913 - accuracy: 0.7140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9912737607955933, 0.714006245136261]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM2xBwVr41tG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "b1c61b76-6553-420a-bffb-6144aeb7f4d0"
      },
      "source": [
        "# from DataCamp example, using pre-trained gloVe embeddings\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(emb_mat),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(Dense(128, activation='relu', name=\"Dense1\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(LSTM(128, return_sequences=True, dropout=0.15, name=\"LSTM\")) \n",
        "model.add(GRU(128, return_sequences=False, dropout=0.15, name=\"GRU\")) \n",
        "model.add(Dense(128, name=\"Dense2\")) \n",
        "model.add(Dropout(rate=0.25)) \n",
        "model.add(Dense(32, name=\"Dense3\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=20) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "160/160 [==============================] - 17s 107ms/step - loss: 1.2195 - accuracy: 0.5045 - val_loss: 0.9517 - val_accuracy: 0.6405\n",
            "Epoch 2/20\n",
            "160/160 [==============================] - 16s 99ms/step - loss: 0.9421 - accuracy: 0.6448 - val_loss: 0.8833 - val_accuracy: 0.6604\n",
            "Epoch 3/20\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.8357 - accuracy: 0.6953 - val_loss: 0.7923 - val_accuracy: 0.7031\n",
            "Epoch 4/20\n",
            "160/160 [==============================] - 16s 97ms/step - loss: 0.7544 - accuracy: 0.7211 - val_loss: 0.7680 - val_accuracy: 0.7113\n",
            "Epoch 5/20\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.6988 - accuracy: 0.7422 - val_loss: 0.7669 - val_accuracy: 0.7128\n",
            "Epoch 6/20\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.6478 - accuracy: 0.7670 - val_loss: 0.7463 - val_accuracy: 0.7293\n",
            "Epoch 7/20\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.6013 - accuracy: 0.7810 - val_loss: 0.8149 - val_accuracy: 0.7007\n",
            "Epoch 8/20\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.5657 - accuracy: 0.7956 - val_loss: 0.7873 - val_accuracy: 0.7230\n",
            "Epoch 9/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.5320 - accuracy: 0.8108 - val_loss: 0.7638 - val_accuracy: 0.7265\n",
            "Epoch 10/20\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.4943 - accuracy: 0.8231 - val_loss: 0.7639 - val_accuracy: 0.7293\n",
            "Epoch 11/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4632 - accuracy: 0.8362 - val_loss: 0.7986 - val_accuracy: 0.7320\n",
            "Epoch 12/20\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.4321 - accuracy: 0.8428 - val_loss: 0.8935 - val_accuracy: 0.7054\n",
            "Epoch 13/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.3983 - accuracy: 0.8585 - val_loss: 0.8716 - val_accuracy: 0.7164\n",
            "Epoch 14/20\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.3791 - accuracy: 0.8647 - val_loss: 0.8692 - val_accuracy: 0.7254\n",
            "Epoch 15/20\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.3486 - accuracy: 0.8793 - val_loss: 0.9290 - val_accuracy: 0.7199\n",
            "Epoch 16/20\n",
            "160/160 [==============================] - 16s 102ms/step - loss: 0.3273 - accuracy: 0.8859 - val_loss: 0.9422 - val_accuracy: 0.7148\n",
            "Epoch 17/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.3039 - accuracy: 0.8927 - val_loss: 0.9343 - val_accuracy: 0.7085\n",
            "Epoch 18/20\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.2855 - accuracy: 0.8958 - val_loss: 0.9967 - val_accuracy: 0.7089\n",
            "Epoch 19/20\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.2649 - accuracy: 0.9045 - val_loss: 1.0660 - val_accuracy: 0.7019\n",
            "Epoch 20/20\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.2522 - accuracy: 0.9098 - val_loss: 1.0540 - val_accuracy: 0.7171\n",
            "80/80 [==============================] - 1s 11ms/step - loss: 1.0538 - accuracy: 0.7171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0538235902786255, 0.7171361446380615]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6b9mzn2Rf8u",
        "colab_type": "text"
      },
      "source": [
        "#### Best simple LSTM (emb.dim changed from 128 to 100)\n",
        "\n",
        "with GloVe, results are worse and the model converges longer.\n",
        "\n",
        "No GloVe, RMSprop = 0.7437, 4 epoch\n",
        "\n",
        "No GloVe, Adam = 0.7387, 2 epoch\n",
        "\n",
        "GloVe, RMSprop = 0.7371, 11 epoch\n",
        "\n",
        "GloVe, Adam = 0.7359, 8 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usFo40fsNZ8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "4349e3b2-feeb-40e5-ae13-b3b4670b3e6d"
      },
      "source": [
        "# best simple LSTM - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, name=\"Embedding\"))  \n",
        "model.add(LSTM(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 10s 60ms/step - loss: 1.2520 - accuracy: 0.5018 - val_loss: 0.9268 - val_accuracy: 0.6628\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.7785 - accuracy: 0.7230 - val_loss: 0.7708 - val_accuracy: 0.7140\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 9s 58ms/step - loss: 0.6255 - accuracy: 0.7802 - val_loss: 0.7451 - val_accuracy: 0.7383\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 9s 58ms/step - loss: 0.5406 - accuracy: 0.8105 - val_loss: 0.7363 - val_accuracy: 0.7437\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.4800 - accuracy: 0.8313 - val_loss: 0.7340 - val_accuracy: 0.7430\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.4314 - accuracy: 0.8508 - val_loss: 0.7621 - val_accuracy: 0.7336\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.3910 - accuracy: 0.8641 - val_loss: 0.7969 - val_accuracy: 0.7312\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.3556 - accuracy: 0.8751 - val_loss: 0.8115 - val_accuracy: 0.7324\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.3226 - accuracy: 0.8868 - val_loss: 0.8127 - val_accuracy: 0.7203\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.2974 - accuracy: 0.8969 - val_loss: 0.8795 - val_accuracy: 0.7140\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.8795 - accuracy: 0.7140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8794649839401245, 0.714006245136261]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kusg_yciFIcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "8dacfeba-974e-4866-8850-311b52c60ae7"
      },
      "source": [
        "# best simple LSTM, using pre-trained gloVe embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(emb_mat),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(LSTM(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=20) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 1.1676 - accuracy: 0.5338 - val_loss: 0.9576 - val_accuracy: 0.6412\n",
            "Epoch 2/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.9285 - accuracy: 0.6498 - val_loss: 0.8785 - val_accuracy: 0.6729\n",
            "Epoch 3/20\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.8227 - accuracy: 0.6894 - val_loss: 0.8080 - val_accuracy: 0.6952\n",
            "Epoch 4/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.7499 - accuracy: 0.7180 - val_loss: 0.7703 - val_accuracy: 0.7121\n",
            "Epoch 5/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.6889 - accuracy: 0.7418 - val_loss: 0.7500 - val_accuracy: 0.7199\n",
            "Epoch 6/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.6421 - accuracy: 0.7611 - val_loss: 0.7392 - val_accuracy: 0.7218\n",
            "Epoch 7/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.5968 - accuracy: 0.7786 - val_loss: 0.7649 - val_accuracy: 0.7242\n",
            "Epoch 8/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.5550 - accuracy: 0.8016 - val_loss: 0.7528 - val_accuracy: 0.7324\n",
            "Epoch 9/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.5162 - accuracy: 0.8117 - val_loss: 0.7345 - val_accuracy: 0.7336\n",
            "Epoch 10/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.4820 - accuracy: 0.8250 - val_loss: 0.7535 - val_accuracy: 0.7336\n",
            "Epoch 11/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.4392 - accuracy: 0.8453 - val_loss: 0.7723 - val_accuracy: 0.7371\n",
            "Epoch 12/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.4072 - accuracy: 0.8519 - val_loss: 0.7881 - val_accuracy: 0.7359\n",
            "Epoch 13/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.3703 - accuracy: 0.8658 - val_loss: 0.8131 - val_accuracy: 0.7316\n",
            "Epoch 14/20\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3491 - accuracy: 0.8740 - val_loss: 0.8229 - val_accuracy: 0.7293\n",
            "Epoch 15/20\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.3122 - accuracy: 0.8885 - val_loss: 0.8439 - val_accuracy: 0.7355\n",
            "Epoch 16/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.2895 - accuracy: 0.8968 - val_loss: 0.8677 - val_accuracy: 0.7312\n",
            "Epoch 17/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.2624 - accuracy: 0.9073 - val_loss: 0.8841 - val_accuracy: 0.7222\n",
            "Epoch 18/20\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.2433 - accuracy: 0.9142 - val_loss: 0.9354 - val_accuracy: 0.7203\n",
            "Epoch 19/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.2143 - accuracy: 0.9272 - val_loss: 0.9321 - val_accuracy: 0.7230\n",
            "Epoch 20/20\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.1938 - accuracy: 0.9332 - val_loss: 0.9933 - val_accuracy: 0.7230\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 0.9929 - accuracy: 0.7230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9929080009460449, 0.7230046987533569]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrrhQHUSNdrs",
        "colab_type": "text"
      },
      "source": [
        "#### Best simple GRU (128 emb.dim changed to 100, GRU 256 + drop 0.3)\n",
        "\n",
        "No GloVe = 0.7394, 4 epoch\n",
        "\n",
        "No GloVe, with emb=128 (above) = 0.7449, 4 epoch\n",
        "\n",
        "GloVe = 0.7324, 9 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-l_VpsyTnna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "e8be5225-65db-476d-f8a3-2b567f94727a"
      },
      "source": [
        "# best simple GRU, no GloVe\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"GRU\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 16s 99ms/step - loss: 1.1915 - accuracy: 0.5300 - val_loss: 0.8594 - val_accuracy: 0.6858\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 97ms/step - loss: 0.7509 - accuracy: 0.7357 - val_loss: 0.7548 - val_accuracy: 0.7238\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 16s 97ms/step - loss: 0.6149 - accuracy: 0.7855 - val_loss: 0.7388 - val_accuracy: 0.7371\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.5363 - accuracy: 0.8123 - val_loss: 0.7342 - val_accuracy: 0.7394\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 16s 98ms/step - loss: 0.4830 - accuracy: 0.8277 - val_loss: 0.7527 - val_accuracy: 0.7344\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 16s 97ms/step - loss: 0.4335 - accuracy: 0.8462 - val_loss: 0.7761 - val_accuracy: 0.7312\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 16s 97ms/step - loss: 0.3904 - accuracy: 0.8654 - val_loss: 0.8043 - val_accuracy: 0.7367\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 16s 98ms/step - loss: 0.3519 - accuracy: 0.8750 - val_loss: 0.9046 - val_accuracy: 0.7191\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.3175 - accuracy: 0.8880 - val_loss: 0.8650 - val_accuracy: 0.7281\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 16s 98ms/step - loss: 0.2864 - accuracy: 0.9012 - val_loss: 0.9318 - val_accuracy: 0.7191\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.9317 - accuracy: 0.7191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9317275285720825, 0.7190923094749451]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaV1nluyDPKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "157dc9ca-775a-4f9d-f502-21b477659896"
      },
      "source": [
        "# best simple GRU using pre-trained gloVe embeddings\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(emb_mat),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"GRU\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=20) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 1.1713 - accuracy: 0.5296 - val_loss: 0.9622 - val_accuracy: 0.6381\n",
            "Epoch 2/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.9504 - accuracy: 0.6393 - val_loss: 0.8777 - val_accuracy: 0.6694\n",
            "Epoch 3/20\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.8484 - accuracy: 0.6835 - val_loss: 0.7997 - val_accuracy: 0.6972\n",
            "Epoch 4/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.7782 - accuracy: 0.7095 - val_loss: 0.7715 - val_accuracy: 0.7124\n",
            "Epoch 5/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.7236 - accuracy: 0.7327 - val_loss: 0.7525 - val_accuracy: 0.7167\n",
            "Epoch 6/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.6719 - accuracy: 0.7536 - val_loss: 0.7382 - val_accuracy: 0.7199\n",
            "Epoch 7/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.6301 - accuracy: 0.7680 - val_loss: 0.7714 - val_accuracy: 0.7128\n",
            "Epoch 8/20\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.5854 - accuracy: 0.7842 - val_loss: 0.7495 - val_accuracy: 0.7250\n",
            "Epoch 9/20\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.5428 - accuracy: 0.7979 - val_loss: 0.7361 - val_accuracy: 0.7324\n",
            "Epoch 10/20\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.5061 - accuracy: 0.8115 - val_loss: 0.7406 - val_accuracy: 0.7304\n",
            "Epoch 11/20\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.4653 - accuracy: 0.8300 - val_loss: 0.7690 - val_accuracy: 0.7234\n",
            "Epoch 12/20\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.4258 - accuracy: 0.8432 - val_loss: 0.7943 - val_accuracy: 0.7234\n",
            "Epoch 13/20\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.3891 - accuracy: 0.8626 - val_loss: 0.8255 - val_accuracy: 0.7257\n",
            "Epoch 14/20\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.3497 - accuracy: 0.8679 - val_loss: 0.8049 - val_accuracy: 0.7281\n",
            "Epoch 15/20\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.3132 - accuracy: 0.8877 - val_loss: 0.8440 - val_accuracy: 0.7285\n",
            "Epoch 16/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.2782 - accuracy: 0.8999 - val_loss: 0.8828 - val_accuracy: 0.7250\n",
            "Epoch 17/20\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.2522 - accuracy: 0.9107 - val_loss: 0.9007 - val_accuracy: 0.7203\n",
            "Epoch 18/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.2158 - accuracy: 0.9245 - val_loss: 0.9617 - val_accuracy: 0.7234\n",
            "Epoch 19/20\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.1925 - accuracy: 0.9305 - val_loss: 0.9958 - val_accuracy: 0.7230\n",
            "Epoch 20/20\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.1663 - accuracy: 0.9396 - val_loss: 1.0771 - val_accuracy: 0.7214\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 1.0766 - accuracy: 0.7214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0766452550888062, 0.721439778804779]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA2BslOxPJpj",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QnbA3JRVwAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "ab292393-9f20-4b2c-8e61-034578face4b"
      },
      "source": [
        "# model from keras site, adapted, 250 hu, emb=128\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=128,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='valid', # 0.4 points worse with same \n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 5s 30ms/step - loss: 1.1967 - accuracy: 0.5187 - val_loss: 0.8391 - val_accuracy: 0.6941\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.7109 - accuracy: 0.7476 - val_loss: 0.7417 - val_accuracy: 0.7328\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.5511 - accuracy: 0.8059 - val_loss: 0.7363 - val_accuracy: 0.7430\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.4425 - accuracy: 0.8440 - val_loss: 0.7624 - val_accuracy: 0.7367\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 5s 29ms/step - loss: 0.3418 - accuracy: 0.8839 - val_loss: 0.7912 - val_accuracy: 0.7316\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7911 - accuracy: 0.7316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7911454439163208, 0.7316119074821472]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4gNUfodbip2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "78577f09-9d28-421a-ecaf-38f848c789fc"
      },
      "source": [
        "# model from keras site, adapted, 250 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='valid', # 0.3p lowers with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "model.add(Dropout(0.2)) # 0.27 lower w/o dropout here\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 1.2176 - accuracy: 0.5091 - val_loss: 0.8525 - val_accuracy: 0.6905\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.7304 - accuracy: 0.7412 - val_loss: 0.7384 - val_accuracy: 0.7371\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.5752 - accuracy: 0.7997 - val_loss: 0.7333 - val_accuracy: 0.7453\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.4668 - accuracy: 0.8391 - val_loss: 0.7450 - val_accuracy: 0.7457\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.3756 - accuracy: 0.8695 - val_loss: 0.7806 - val_accuracy: 0.7359\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7805 - accuracy: 0.7359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7805317640304565, 0.73591548204422]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF7jPzICcQpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3d84f95b-d723-41df-e8a8-df11a7a3a515"
      },
      "source": [
        "# model from keras site, adapted, 250 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 4s 26ms/step - loss: 1.1768 - accuracy: 0.5307 - val_loss: 0.8257 - val_accuracy: 0.6972\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 0.7040 - accuracy: 0.7529 - val_loss: 0.7279 - val_accuracy: 0.7453\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 0.5464 - accuracy: 0.8091 - val_loss: 0.7269 - val_accuracy: 0.7508\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 0.4457 - accuracy: 0.8450 - val_loss: 0.7442 - val_accuracy: 0.7418\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 4s 25ms/step - loss: 0.3551 - accuracy: 0.8751 - val_loss: 0.7814 - val_accuracy: 0.7449\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.7814 - accuracy: 0.7449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7813548445701599, 0.7449139356613159]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9prxt1FvFdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3d98c662-6514-458e-9675-7a20ba57efea"
      },
      "source": [
        "# model from keras site, adapted, 250 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 5s 28ms/step - loss: 1.2028 - accuracy: 0.5157 - val_loss: 0.8483 - val_accuracy: 0.6851\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.7261 - accuracy: 0.7469 - val_loss: 0.7382 - val_accuracy: 0.7390\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.5726 - accuracy: 0.8004 - val_loss: 0.7364 - val_accuracy: 0.7426\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.4690 - accuracy: 0.8383 - val_loss: 0.7500 - val_accuracy: 0.7414\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 4s 28ms/step - loss: 0.3778 - accuracy: 0.8681 - val_loss: 0.7862 - val_accuracy: 0.7402\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 0.7862 - accuracy: 0.7402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7861568331718445, 0.7402191162109375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6df8H6uEAB0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "6ab277c9-43d1-4862-e88a-14dc4eefce7d"
      },
      "source": [
        "# with pretrained GloVe embeddings, worse.\n",
        "# with trainable = False, even worse: max valid.acc = 0.682   \n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100, trainable=True,                    \n",
        "                    embeddings_initializer=Constant(emb_mat),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 1.2436 - accuracy: 0.4873 - val_loss: 0.9376 - val_accuracy: 0.6534\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.9269 - accuracy: 0.6476 - val_loss: 0.8627 - val_accuracy: 0.6729\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.8033 - accuracy: 0.7015 - val_loss: 0.7889 - val_accuracy: 0.7015\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.7027 - accuracy: 0.7374 - val_loss: 0.7651 - val_accuracy: 0.7152\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.6251 - accuracy: 0.7672 - val_loss: 0.7628 - val_accuracy: 0.7203\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5453 - accuracy: 0.8001 - val_loss: 0.7796 - val_accuracy: 0.7171\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.4765 - accuracy: 0.8244 - val_loss: 0.8984 - val_accuracy: 0.6843\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.4182 - accuracy: 0.8455 - val_loss: 0.8848 - val_accuracy: 0.6897\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.3790 - accuracy: 0.8633 - val_loss: 0.8611 - val_accuracy: 0.7074\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.3271 - accuracy: 0.8797 - val_loss: 0.8884 - val_accuracy: 0.7058\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.8881 - accuracy: 0.7058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8881068229675293, 0.7057902812957764]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfdBXyWXDQmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "e6d2d1e0-6543-48dd-b0f4-f5fa7b0d762b"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=128,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(50, 3,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # 0.4 points better with same\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(Conv1D(100, 4,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # 0.4 points better with same\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(Conv1D(150, 5,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # 0.4 points better with same\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 9s 59ms/step - loss: 1.3419 - accuracy: 0.3714 - val_loss: 1.1514 - val_accuracy: 0.4734\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.9957 - accuracy: 0.6026 - val_loss: 0.9340 - val_accuracy: 0.6448\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 0.7298 - accuracy: 0.7375 - val_loss: 0.8080 - val_accuracy: 0.7062\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.5823 - accuracy: 0.7983 - val_loss: 0.8014 - val_accuracy: 0.7250\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.4777 - accuracy: 0.8352 - val_loss: 0.7892 - val_accuracy: 0.7226\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.4000 - accuracy: 0.8618 - val_loss: 0.8077 - val_accuracy: 0.7261\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.3383 - accuracy: 0.8856 - val_loss: 0.8721 - val_accuracy: 0.7152\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.2819 - accuracy: 0.9050 - val_loss: 0.9466 - val_accuracy: 0.7222\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.2301 - accuracy: 0.9217 - val_loss: 0.9947 - val_accuracy: 0.7152\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.1858 - accuracy: 0.9378 - val_loss: 1.0959 - val_accuracy: 0.6937\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 1.0959 - accuracy: 0.6937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0959151983261108, 0.6936619877815247]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY975i3gAagP",
        "colab_type": "text"
      },
      "source": [
        "### Best CNN model so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCF2fjhunaoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "2b4ddeb8-029e-4ef7-ea35-3c31f1690df3"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_57 (Embedding)     (None, 20, 100)           932200    \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_56 (Conv1D)           (None, 20, 500)           150500    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_56 (Glo (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 5)                 1255      \n",
            "=================================================================\n",
            "Total params: 1,209,205\n",
            "Trainable params: 1,209,205\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 1.1335 - accuracy: 0.5475 - val_loss: 0.7956 - val_accuracy: 0.7132\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.6920 - accuracy: 0.7550 - val_loss: 0.7236 - val_accuracy: 0.7484\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.5359 - accuracy: 0.8139 - val_loss: 0.7199 - val_accuracy: 0.7508\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.4377 - accuracy: 0.8472 - val_loss: 0.7378 - val_accuracy: 0.7551\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.3446 - accuracy: 0.8817 - val_loss: 0.7765 - val_accuracy: 0.7445\n",
            "80/80 [==============================] - 0s 5ms/step - loss: 0.7764 - accuracy: 0.7445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7764409780502319, 0.7445226907730103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2xOoKPegMn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e9d421b7-13c1-49b4-f60f-70cd7a40eca5"
      },
      "source": [
        "max(history.history[\"val_accuracy\"]) # extract mav val_accuracy from the trained model."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7550860643386841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaqndbsUAnLv",
        "colab_type": "text"
      },
      "source": [
        "#### Predict on test set for kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bhNjEPQgHx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "842ed5d6-3c0a-4417-9e31-9369a16303a7"
      },
      "source": [
        "# the same, but trained for 4 epochs only to achieve max accuracy\n",
        "# model from keras site, adapted, 500 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(250))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dense(5, activation=\"softmax\"))  \n",
        "model3.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model3.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=4) \n",
        "model3.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 1.1335 - accuracy: 0.5475 - val_loss: 0.7956 - val_accuracy: 0.7132\n",
            "Epoch 2/4\n",
            "160/160 [==============================] - 10s 64ms/step - loss: 0.6920 - accuracy: 0.7550 - val_loss: 0.7236 - val_accuracy: 0.7484\n",
            "Epoch 3/4\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 0.5359 - accuracy: 0.8139 - val_loss: 0.7199 - val_accuracy: 0.7508\n",
            "Epoch 4/4\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 0.4377 - accuracy: 0.8472 - val_loss: 0.7378 - val_accuracy: 0.7551\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.7377 - accuracy: 0.7551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7377182841300964, 0.7550860643386841]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hhjpySOu3E9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "e46b06f3-568c-40d1-ff92-589fe681de8a"
      },
      "source": [
        "# Best CNN \n",
        "Y_test_pred = model3.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[413,  90,  40,  13,  32],\n",
              "       [ 90, 454,  18,  21,   5],\n",
              "       [ 65,  18, 398,  38,  41],\n",
              "       [ 12,  27,  31, 415,  17],\n",
              "       [ 30,   3,  24,  11, 250]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktjjsV_D28JI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "2dbe48cb-f64f-4da9-a54a-4c338713e956"
      },
      "source": [
        "# GRU(256)\n",
        "Y_test_pred = model.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[403,  91,  58,  14,  22],\n",
              "       [ 77, 459,  25,  22,   5],\n",
              "       [ 56,  26, 400,  38,  40],\n",
              "       [ 13,  29,  44, 402,  14],\n",
              "       [ 33,   2,  32,  11, 240]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5VKHFj24zCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c7ad16c3-a478-4fcf-f6f9-35444c7c3a39"
      },
      "source": [
        "# LSTM-128 RMSprop\n",
        "Y_test_pred = model2.predict_classes(X_test)\n",
        "Y_test_df = pd.DataFrame(Y_test)\n",
        "Y_test_categ = Y_test_df.idxmax(axis=1)\n",
        "confusion_matrix(Y_test_categ,Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[374, 104,  67,  18,  25],\n",
              "       [ 70, 466,  20,  27,   5],\n",
              "       [ 47,  25, 405,  42,  41],\n",
              "       [  9,  28,  40, 410,  15],\n",
              "       [ 25,   1,  34,  11, 247]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JVJQCSWgRFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "86373bc0-6619-4dcf-880b-d07f894c67af"
      },
      "source": [
        "Y_new3 = model3.predict_classes(X_new)\n",
        "# show the inputs and predicted outputs\n",
        "print(X_new[:5])\n",
        "for i in range(5):\n",
        "\tprint(\"X=%s, Predicted=%s\" % (X_new[i], Y_new3[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    3  482\n",
            "     8   32   15    1  117   66]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 1591  130  758\n",
            "    20   66    1 1407  295  307]\n",
            " [   0    0    0    0    0    0   95    4  120    2   35   21  693 2005\n",
            "  6187    1   24    2  463  166]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0  378  229\n",
            "   793    1   47  278  399   14]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0   47  202  302  519]]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   3 482   8  32  15   1\n",
            " 117  66], Predicted=1\n",
            "X=[   0    0    0    0    0    0    0    0    0    0    0 1591  130  758\n",
            "   20   66    1 1407  295  307], Predicted=1\n",
            "X=[   0    0    0    0    0    0   95    4  120    2   35   21  693 2005\n",
            " 6187    1   24    2  463  166], Predicted=0\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0 378 229 793   1  47 278\n",
            " 399  14], Predicted=2\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  47 202\n",
            " 302 519], Predicted=2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTmNkFUlgwyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test3 = pd.DataFrame(list(zip(test['id'], Y_new3)), \n",
        "               columns = ['id', 'label'])\n",
        "test3\n",
        "test3.to_csv('test3.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJaAMJUIzOL3"
      },
      "source": [
        "### Custom models with different filter sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng_swncW77nL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "8d4f67a1-e5d6-4997-a50c-e08b8062c193"
      },
      "source": [
        "# try the best simple model for a benchmark with a new seed\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_131 (Embedding)    (None, 20, 100)           932200    \n",
            "_________________________________________________________________\n",
            "dropout_121 (Dropout)        (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_120 (Conv1D)          (None, 20, 500)           150500    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_114 (Gl (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 5)                 1255      \n",
            "=================================================================\n",
            "Total params: 1,209,205\n",
            "Trainable params: 1,209,205\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 1.1528 - accuracy: 0.5422 - val_loss: 0.8278 - val_accuracy: 0.7023\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 11s 67ms/step - loss: 0.6961 - accuracy: 0.7471 - val_loss: 0.7481 - val_accuracy: 0.7332\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 11s 66ms/step - loss: 0.5460 - accuracy: 0.8094 - val_loss: 0.7455 - val_accuracy: 0.7351\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 0.4398 - accuracy: 0.8500 - val_loss: 0.7658 - val_accuracy: 0.7402\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 10s 65ms/step - loss: 0.3451 - accuracy: 0.8828 - val_loss: 0.8262 - val_accuracy: 0.7246\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 0.8260 - accuracy: 0.7246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8260191679000854, 0.7245696187019348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d5xMc67GzOL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca67b7bb-5692-431a-9ea3-ec4ea885ea09"
      },
      "source": [
        "def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=500, kernel_size=2, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  # pool1 = GlobalMaxPooling1D()(drop1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=500, kernel_size=3, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  # pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=500, kernel_size=4, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  # pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "# define model\n",
        "model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_36\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_97 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_98 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_99 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_62 (Embedding)        (None, 20, 100)      932200      input_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_63 (Embedding)        (None, 20, 100)      932200      input_98[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_64 (Embedding)        (None, 20, 100)      932200      input_99[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_51 (Conv1D)              (None, 20, 500)      100500      embedding_62[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_52 (Conv1D)              (None, 20, 500)      150500      embedding_63[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_53 (Conv1D)              (None, 20, 500)      200500      embedding_64[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 20, 500)      0           conv1d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 20, 500)      0           conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 20, 500)      0           conv1d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 10, 500)      0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 10, 500)      0           dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 10, 500)      0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_49 (Flatten)            (None, 5000)         0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_50 (Flatten)            (None, 5000)         0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_51 (Flatten)            (None, 5000)         0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_38 (TensorFl [(None, 15000)]      0           flatten_49[0][0]                 \n",
            "                                                                 flatten_50[0][0]                 \n",
            "                                                                 flatten_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_96 (Dense)                (None, 500)          7500500     tf_op_layer_concat_38[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_97 (Dense)                (None, 5)            2505        dense_96[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 10,751,105\n",
            "Trainable params: 10,751,105\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 43s 272ms/step - loss: 1.1038 - accuracy: 0.5597 - val_loss: 0.8078 - val_accuracy: 0.7105\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 44s 272ms/step - loss: 0.6131 - accuracy: 0.7849 - val_loss: 0.7723 - val_accuracy: 0.7152\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 43s 268ms/step - loss: 0.4092 - accuracy: 0.8606 - val_loss: 0.7858 - val_accuracy: 0.7179\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.2416 - accuracy: 0.9200 - val_loss: 0.9304 - val_accuracy: 0.6995\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.1284 - accuracy: 0.9590 - val_loss: 1.1386 - val_accuracy: 0.6819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f818f256f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjTFWWcY162B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63abb535-f4e4-4051-dd1a-f80ca47e933d"
      },
      "source": [
        "def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=500, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.3)(conv1)\n",
        "  # pool1 = GlobalMaxPooling1D()(drop1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=500, kernel_size=6, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.3)(conv2)\n",
        "  # pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=500, kernel_size=8, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.3)(conv3)\n",
        "  # pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "# define model\n",
        "model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_37\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_100 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_101 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_102 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_65 (Embedding)        (None, 20, 100)      932200      input_100[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_66 (Embedding)        (None, 20, 100)      932200      input_101[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_67 (Embedding)        (None, 20, 100)      932200      input_102[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_54 (Conv1D)              (None, 20, 500)      200500      embedding_65[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_55 (Conv1D)              (None, 20, 500)      300500      embedding_66[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_56 (Conv1D)              (None, 20, 500)      400500      embedding_67[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 20, 500)      0           conv1d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 20, 500)      0           conv1d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 20, 500)      0           conv1d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 10, 500)      0           dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 10, 500)      0           dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 10, 500)      0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_52 (Flatten)            (None, 5000)         0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 5000)         0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 5000)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_39 (TensorFl [(None, 15000)]      0           flatten_52[0][0]                 \n",
            "                                                                 flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_98 (Dense)                (None, 500)          7500500     tf_op_layer_concat_39[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_99 (Dense)                (None, 5)            2505        dense_98[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 11,201,105\n",
            "Trainable params: 11,201,105\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 63s 392ms/step - loss: 1.0949 - accuracy: 0.5608 - val_loss: 0.8033 - val_accuracy: 0.7097\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 63s 394ms/step - loss: 0.5963 - accuracy: 0.7924 - val_loss: 0.7791 - val_accuracy: 0.7167\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 63s 397ms/step - loss: 0.3688 - accuracy: 0.8748 - val_loss: 0.8073 - val_accuracy: 0.7128\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 63s 394ms/step - loss: 0.1820 - accuracy: 0.9409 - val_loss: 1.0523 - val_accuracy: 0.6909\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 63s 394ms/step - loss: 0.0780 - accuracy: 0.9752 - val_loss: 1.4483 - val_accuracy: 0.6764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f818ef07668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SatApdMc3LYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a23911ef-3ec4-4da6-fe4c-a66cefec1416"
      },
      "source": [
        "# Imitating article CNN for sentense classification:\n",
        "# filters: 100\n",
        "# kernel size: 3,4,5\n",
        "# dropout = 0.5\n",
        "# MaxOverTime pooling\n",
        "\n",
        "def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "# define model\n",
        "model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_39\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_106 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_107 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_108 (InputLayer)          [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_71 (Embedding)        (None, 20, 100)      932200      input_106[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_72 (Embedding)        (None, 20, 100)      932200      input_107[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_73 (Embedding)        (None, 20, 100)      932200      input_108[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_60 (Conv1D)              (None, 20, 100)      30100       embedding_71[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_61 (Conv1D)              (None, 20, 100)      40100       embedding_72[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_62 (Conv1D)              (None, 20, 100)      50100       embedding_73[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 20, 100)      0           conv1d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 20, 100)      0           conv1d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 20, 100)      0           conv1d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_54 (Global (None, 100)          0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_55 (Global (None, 100)          0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_56 (Global (None, 100)          0           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_58 (Flatten)            (None, 100)          0           global_max_pooling1d_54[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_59 (Flatten)            (None, 100)          0           global_max_pooling1d_55[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_60 (Flatten)            (None, 100)          0           global_max_pooling1d_56[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_41 (TensorFl [(None, 300)]        0           flatten_58[0][0]                 \n",
            "                                                                 flatten_59[0][0]                 \n",
            "                                                                 flatten_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_102 (Dense)               (None, 500)          150500      tf_op_layer_concat_41[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_103 (Dense)               (None, 5)            2505        dense_102[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,069,905\n",
            "Trainable params: 3,069,905\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 1.1100 - accuracy: 0.5593 - val_loss: 0.8752 - val_accuracy: 0.7117\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.6282 - accuracy: 0.7778 - val_loss: 0.7798 - val_accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.4556 - accuracy: 0.8426 - val_loss: 0.7289 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.3320 - accuracy: 0.8862 - val_loss: 0.7311 - val_accuracy: 0.7390\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.2315 - accuracy: 0.9234 - val_loss: 0.7941 - val_accuracy: 0.7171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8190bb9d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpcb35XU4x7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "198c8584-5a84-4780-e225-266b3b5b84b5"
      },
      "source": [
        "# kernel size: 3,4,5\n",
        "dropout = [0, 0.25, 0.35, 0.5]\n",
        "filters = [50, 100, 200, 300]\n",
        "# MaxOverTime pooling\n",
        "\n",
        "for i in filters:\n",
        "  for j in dropout:\n",
        "    print(\"Number of hidden units: \", i, \"dropout: \",j)\n",
        "\n",
        "    def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "      # define CNN layer\n",
        "      inputs1 = Input(shape=(input_length,))\n",
        "      emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "      conv1 = Conv1D(filters=i, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "      drop1 = Dropout(j)(conv1)\n",
        "      pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "      # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "      flat1 = Flatten()(pool1)\n",
        "\n",
        "      inputs2 = Input(shape=(input_length,))\n",
        "      emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "      conv2 = Conv1D(filters=i, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "      drop2 = Dropout(j)(conv2)\n",
        "      pool2 = GlobalMaxPooling1D()(drop2)\n",
        "      # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "      flat2 = Flatten()(pool2)\n",
        "\n",
        "      inputs3 = Input(shape=(input_length,))\n",
        "      emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "      conv3 = Conv1D(filters=i, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "      drop3 = Dropout(j)(conv3)\n",
        "      pool3 = GlobalMaxPooling1D()(drop3)\n",
        "      # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "      flat3 = Flatten()(pool3)\n",
        "\n",
        "      # merge CNN output with reference stats\n",
        "      merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "      dense1 = Dense(500, activation='relu')(merged)\n",
        "      outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "      model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # print(model.summary())\n",
        "      # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "      return model\n",
        "\n",
        "    np.random.seed(5)\n",
        "    random.seed(5)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                  inter_op_parallelism_threads=1)\n",
        "    tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "    # define model\n",
        "    model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "    model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "    # save the model\n",
        "    # model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of hidden units:  50 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 1.1334 - accuracy: 0.5552 - val_loss: 0.8149 - val_accuracy: 0.7066\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.5854 - accuracy: 0.7982 - val_loss: 0.7577 - val_accuracy: 0.7351\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.3422 - accuracy: 0.8863 - val_loss: 0.7922 - val_accuracy: 0.7285\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 41ms/step - loss: 0.1646 - accuracy: 0.9513 - val_loss: 0.8979 - val_accuracy: 0.7257\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 41ms/step - loss: 0.0613 - accuracy: 0.9844 - val_loss: 1.0848 - val_accuracy: 0.7148\n",
            "Number of hidden units:  50 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 1.1381 - accuracy: 0.5543 - val_loss: 0.8370 - val_accuracy: 0.7105\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.6105 - accuracy: 0.7856 - val_loss: 0.7442 - val_accuracy: 0.7300\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.4042 - accuracy: 0.8608 - val_loss: 0.7333 - val_accuracy: 0.7312\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 0.2524 - accuracy: 0.9148 - val_loss: 0.7853 - val_accuracy: 0.7324\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.1464 - accuracy: 0.9562 - val_loss: 0.8781 - val_accuracy: 0.7187\n",
            "Number of hidden units:  50 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 1.1432 - accuracy: 0.5502 - val_loss: 0.8649 - val_accuracy: 0.7081\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.6255 - accuracy: 0.7811 - val_loss: 0.7570 - val_accuracy: 0.7281\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 39ms/step - loss: 0.4332 - accuracy: 0.8501 - val_loss: 0.7278 - val_accuracy: 0.7328\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.2938 - accuracy: 0.8979 - val_loss: 0.7625 - val_accuracy: 0.7285\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 40ms/step - loss: 0.1919 - accuracy: 0.9372 - val_loss: 0.8215 - val_accuracy: 0.7269\n",
            "Number of hidden units:  50 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 7s 42ms/step - loss: 1.1484 - accuracy: 0.5468 - val_loss: 0.9051 - val_accuracy: 0.7074\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 7s 43ms/step - loss: 0.6489 - accuracy: 0.7732 - val_loss: 0.7801 - val_accuracy: 0.7316\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.4726 - accuracy: 0.8348 - val_loss: 0.7321 - val_accuracy: 0.7387\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 7s 41ms/step - loss: 0.3549 - accuracy: 0.8750 - val_loss: 0.7300 - val_accuracy: 0.7332\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 41ms/step - loss: 0.2580 - accuracy: 0.9124 - val_loss: 0.7630 - val_accuracy: 0.7250\n",
            "Number of hidden units:  100 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 1.1069 - accuracy: 0.5673 - val_loss: 0.7912 - val_accuracy: 0.7179\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.5767 - accuracy: 0.7980 - val_loss: 0.7528 - val_accuracy: 0.7308\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.3454 - accuracy: 0.8837 - val_loss: 0.7925 - val_accuracy: 0.7242\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.1692 - accuracy: 0.9481 - val_loss: 0.8917 - val_accuracy: 0.7242\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.0605 - accuracy: 0.9831 - val_loss: 1.0775 - val_accuracy: 0.7210\n",
            "Number of hidden units:  100 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 1.1034 - accuracy: 0.5666 - val_loss: 0.8186 - val_accuracy: 0.7152\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.5979 - accuracy: 0.7898 - val_loss: 0.7514 - val_accuracy: 0.7261\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.4003 - accuracy: 0.8632 - val_loss: 0.7378 - val_accuracy: 0.7320\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.2503 - accuracy: 0.9169 - val_loss: 0.8016 - val_accuracy: 0.7175\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.1397 - accuracy: 0.9572 - val_loss: 0.8906 - val_accuracy: 0.7207\n",
            "Number of hidden units:  100 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 9s 57ms/step - loss: 1.1036 - accuracy: 0.5652 - val_loss: 0.8350 - val_accuracy: 0.7089\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.6060 - accuracy: 0.7850 - val_loss: 0.7558 - val_accuracy: 0.7226\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.4197 - accuracy: 0.8560 - val_loss: 0.7306 - val_accuracy: 0.7332\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.2840 - accuracy: 0.9034 - val_loss: 0.7658 - val_accuracy: 0.7297\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.1744 - accuracy: 0.9442 - val_loss: 0.8317 - val_accuracy: 0.7281\n",
            "Number of hidden units:  100 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 1.1100 - accuracy: 0.5593 - val_loss: 0.8752 - val_accuracy: 0.7117\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.6282 - accuracy: 0.7778 - val_loss: 0.7798 - val_accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.4556 - accuracy: 0.8426 - val_loss: 0.7289 - val_accuracy: 0.7347\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 9s 55ms/step - loss: 0.3320 - accuracy: 0.8862 - val_loss: 0.7311 - val_accuracy: 0.7390\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 9s 56ms/step - loss: 0.2315 - accuracy: 0.9234 - val_loss: 0.7941 - val_accuracy: 0.7171\n",
            "Number of hidden units:  200 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 1.0786 - accuracy: 0.5792 - val_loss: 0.7790 - val_accuracy: 0.7195\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 12s 78ms/step - loss: 0.5749 - accuracy: 0.8002 - val_loss: 0.7567 - val_accuracy: 0.7265\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 13s 78ms/step - loss: 0.3524 - accuracy: 0.8790 - val_loss: 0.7936 - val_accuracy: 0.7293\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.1783 - accuracy: 0.9440 - val_loss: 0.9128 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.0698 - accuracy: 0.9809 - val_loss: 1.0985 - val_accuracy: 0.7164\n",
            "Number of hidden units:  200 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 13s 82ms/step - loss: 1.0769 - accuracy: 0.5723 - val_loss: 0.8037 - val_accuracy: 0.7179\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.5903 - accuracy: 0.7937 - val_loss: 0.7547 - val_accuracy: 0.7195\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.3976 - accuracy: 0.8616 - val_loss: 0.7364 - val_accuracy: 0.7410\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.2490 - accuracy: 0.9184 - val_loss: 0.8028 - val_accuracy: 0.7281\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.1352 - accuracy: 0.9582 - val_loss: 0.9031 - val_accuracy: 0.7203\n",
            "Number of hidden units:  200 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 1.0753 - accuracy: 0.5752 - val_loss: 0.8228 - val_accuracy: 0.7207\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.5976 - accuracy: 0.7897 - val_loss: 0.7548 - val_accuracy: 0.7207\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.4139 - accuracy: 0.8551 - val_loss: 0.7308 - val_accuracy: 0.7355\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.2774 - accuracy: 0.9058 - val_loss: 0.7711 - val_accuracy: 0.7289\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.1650 - accuracy: 0.9479 - val_loss: 0.8575 - val_accuracy: 0.7187\n",
            "Number of hidden units:  200 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 13s 81ms/step - loss: 1.0796 - accuracy: 0.5733 - val_loss: 0.8628 - val_accuracy: 0.7117\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.6110 - accuracy: 0.7861 - val_loss: 0.7741 - val_accuracy: 0.7238\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.4396 - accuracy: 0.8472 - val_loss: 0.7225 - val_accuracy: 0.7293\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.3122 - accuracy: 0.8919 - val_loss: 0.7475 - val_accuracy: 0.7265\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.2080 - accuracy: 0.9318 - val_loss: 0.8217 - val_accuracy: 0.7167\n",
            "Number of hidden units:  300 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 1.0647 - accuracy: 0.5803 - val_loss: 0.7747 - val_accuracy: 0.7250\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.5761 - accuracy: 0.8000 - val_loss: 0.7616 - val_accuracy: 0.7207\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.3648 - accuracy: 0.8731 - val_loss: 0.7895 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.1938 - accuracy: 0.9387 - val_loss: 0.9139 - val_accuracy: 0.7226\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.0815 - accuracy: 0.9764 - val_loss: 1.0806 - val_accuracy: 0.7195\n",
            "Number of hidden units:  300 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 1.0575 - accuracy: 0.5801 - val_loss: 0.7983 - val_accuracy: 0.7226\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.5913 - accuracy: 0.7934 - val_loss: 0.7559 - val_accuracy: 0.7195\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.4052 - accuracy: 0.8601 - val_loss: 0.7346 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 22s 134ms/step - loss: 0.2562 - accuracy: 0.9119 - val_loss: 0.8173 - val_accuracy: 0.7191\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.1387 - accuracy: 0.9570 - val_loss: 0.9201 - val_accuracy: 0.7160\n",
            "Number of hidden units:  300 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 1.0629 - accuracy: 0.5790 - val_loss: 0.8167 - val_accuracy: 0.7218\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 22s 135ms/step - loss: 0.6003 - accuracy: 0.7901 - val_loss: 0.7596 - val_accuracy: 0.7179\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.4215 - accuracy: 0.8536 - val_loss: 0.7238 - val_accuracy: 0.7344\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 22s 138ms/step - loss: 0.2801 - accuracy: 0.9040 - val_loss: 0.7867 - val_accuracy: 0.7183\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.1658 - accuracy: 0.9467 - val_loss: 0.8705 - val_accuracy: 0.7124\n",
            "Number of hidden units:  300 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 1.0690 - accuracy: 0.5772 - val_loss: 0.8609 - val_accuracy: 0.7187\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.6082 - accuracy: 0.7833 - val_loss: 0.7716 - val_accuracy: 0.7152\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.4411 - accuracy: 0.8448 - val_loss: 0.7229 - val_accuracy: 0.7340\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 22s 136ms/step - loss: 0.3141 - accuracy: 0.8905 - val_loss: 0.7541 - val_accuracy: 0.7254\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.2094 - accuracy: 0.9336 - val_loss: 0.8225 - val_accuracy: 0.7183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCmub42Y7xXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3da4cb42-45a5-4606-f7f7-ec972819c5c8"
      },
      "source": [
        "# kernel size: 3,4,5\n",
        "# removed padding = \"same\"\n",
        "dropout = [0, 0.25, 0.35, 0.5]\n",
        "filters = [50, 100, 200, 300]\n",
        "# MaxOverTime pooling\n",
        "\n",
        "for i in filters:\n",
        "  for j in dropout:\n",
        "    print(\"Number of hidden units: \", i, \"dropout: \",j)\n",
        "\n",
        "    def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "      # define CNN layer\n",
        "      inputs1 = Input(shape=(input_length,))\n",
        "      emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "      conv1 = Conv1D(filters=i, kernel_size=3, activation='relu', strides=1)(emb1) # removed padding='same', \n",
        "      drop1 = Dropout(j)(conv1)\n",
        "      pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "      # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "      flat1 = Flatten()(pool1)\n",
        "\n",
        "      inputs2 = Input(shape=(input_length,))\n",
        "      emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "      conv2 = Conv1D(filters=i, kernel_size=4, activation='relu', strides=1)(emb2) # removed padding='same', \n",
        "      drop2 = Dropout(j)(conv2)\n",
        "      pool2 = GlobalMaxPooling1D()(drop2)\n",
        "      # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "      flat2 = Flatten()(pool2)\n",
        "\n",
        "      inputs3 = Input(shape=(input_length,))\n",
        "      emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "      conv3 = Conv1D(filters=i, kernel_size=5, activation='relu', strides=1)(emb3) # removed padding='same', \n",
        "      drop3 = Dropout(j)(conv3)\n",
        "      pool3 = GlobalMaxPooling1D()(drop3)\n",
        "      # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "      flat3 = Flatten()(pool3)\n",
        "\n",
        "      # merge CNN output with reference stats\n",
        "      merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "      dense1 = Dense(500, activation='relu')(merged)\n",
        "      outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "      model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # print(model.summary())\n",
        "      # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "      return model\n",
        "\n",
        "    np.random.seed(5)\n",
        "    random.seed(5)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                  inter_op_parallelism_threads=1)\n",
        "    tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "    # define model\n",
        "    model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "    model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "    # save the model\n",
        "    # model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of hidden units:  50 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 1.1480 - accuracy: 0.5474 - val_loss: 0.8246 - val_accuracy: 0.7023\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.5914 - accuracy: 0.7934 - val_loss: 0.7591 - val_accuracy: 0.7261\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.3412 - accuracy: 0.8858 - val_loss: 0.7892 - val_accuracy: 0.7222\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.1599 - accuracy: 0.9521 - val_loss: 0.9036 - val_accuracy: 0.7257\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.0597 - accuracy: 0.9848 - val_loss: 1.0869 - val_accuracy: 0.7105\n",
            "Number of hidden units:  50 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 1.1609 - accuracy: 0.5361 - val_loss: 0.8568 - val_accuracy: 0.7031\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.6323 - accuracy: 0.7746 - val_loss: 0.7588 - val_accuracy: 0.7234\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.4217 - accuracy: 0.8552 - val_loss: 0.7385 - val_accuracy: 0.7320\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.2591 - accuracy: 0.9128 - val_loss: 0.7972 - val_accuracy: 0.7254\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.1444 - accuracy: 0.9545 - val_loss: 0.8880 - val_accuracy: 0.7222\n",
            "Number of hidden units:  50 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 1.1649 - accuracy: 0.5343 - val_loss: 0.8739 - val_accuracy: 0.7027\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.6449 - accuracy: 0.7695 - val_loss: 0.7713 - val_accuracy: 0.7175\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.4508 - accuracy: 0.8446 - val_loss: 0.7276 - val_accuracy: 0.7402\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.3035 - accuracy: 0.8967 - val_loss: 0.7670 - val_accuracy: 0.7312\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.1918 - accuracy: 0.9380 - val_loss: 0.8388 - val_accuracy: 0.7203\n",
            "Number of hidden units:  50 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 1.1816 - accuracy: 0.5217 - val_loss: 0.9159 - val_accuracy: 0.7058\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.6786 - accuracy: 0.7576 - val_loss: 0.7912 - val_accuracy: 0.7113\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 38ms/step - loss: 0.4980 - accuracy: 0.8257 - val_loss: 0.7342 - val_accuracy: 0.7363\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.3677 - accuracy: 0.8744 - val_loss: 0.7352 - val_accuracy: 0.7285\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.2639 - accuracy: 0.9128 - val_loss: 0.7716 - val_accuracy: 0.7218\n",
            "Number of hidden units:  100 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.1155 - accuracy: 0.5642 - val_loss: 0.7950 - val_accuracy: 0.7171\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.5784 - accuracy: 0.7973 - val_loss: 0.7589 - val_accuracy: 0.7234\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.3391 - accuracy: 0.8866 - val_loss: 0.8029 - val_accuracy: 0.7187\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.1577 - accuracy: 0.9523 - val_loss: 0.9003 - val_accuracy: 0.7242\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.0554 - accuracy: 0.9853 - val_loss: 1.1164 - val_accuracy: 0.7171\n",
            "Number of hidden units:  100 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 1.1177 - accuracy: 0.5591 - val_loss: 0.8217 - val_accuracy: 0.7105\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.6053 - accuracy: 0.7864 - val_loss: 0.7554 - val_accuracy: 0.7214\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.3962 - accuracy: 0.8636 - val_loss: 0.7605 - val_accuracy: 0.7226\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.2331 - accuracy: 0.9248 - val_loss: 0.8057 - val_accuracy: 0.7164\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.1210 - accuracy: 0.9628 - val_loss: 0.9346 - val_accuracy: 0.7105\n",
            "Number of hidden units:  100 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 1.1224 - accuracy: 0.5571 - val_loss: 0.8381 - val_accuracy: 0.7074\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.6203 - accuracy: 0.7811 - val_loss: 0.7571 - val_accuracy: 0.7238\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 9s 54ms/step - loss: 0.4236 - accuracy: 0.8539 - val_loss: 0.7463 - val_accuracy: 0.7226\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.2719 - accuracy: 0.9083 - val_loss: 0.7721 - val_accuracy: 0.7210\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 0.1635 - accuracy: 0.9482 - val_loss: 0.8670 - val_accuracy: 0.7191\n",
            "Number of hidden units:  100 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 51ms/step - loss: 1.1410 - accuracy: 0.5472 - val_loss: 0.8768 - val_accuracy: 0.7066\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.6429 - accuracy: 0.7693 - val_loss: 0.7721 - val_accuracy: 0.7242\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 53ms/step - loss: 0.4623 - accuracy: 0.8388 - val_loss: 0.7365 - val_accuracy: 0.7285\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.3265 - accuracy: 0.8878 - val_loss: 0.7500 - val_accuracy: 0.7297\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 52ms/step - loss: 0.2263 - accuracy: 0.9244 - val_loss: 0.8619 - val_accuracy: 0.7066\n",
            "Number of hidden units:  200 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 1.0849 - accuracy: 0.5730 - val_loss: 0.7878 - val_accuracy: 0.7160\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.5759 - accuracy: 0.7991 - val_loss: 0.7703 - val_accuracy: 0.7210\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 12s 72ms/step - loss: 0.3428 - accuracy: 0.8843 - val_loss: 0.8215 - val_accuracy: 0.7191\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 12s 72ms/step - loss: 0.1615 - accuracy: 0.9518 - val_loss: 0.9458 - val_accuracy: 0.7222\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.0598 - accuracy: 0.9828 - val_loss: 1.2090 - val_accuracy: 0.7066\n",
            "Number of hidden units:  200 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 1.0917 - accuracy: 0.5643 - val_loss: 0.8079 - val_accuracy: 0.7203\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.6012 - accuracy: 0.7869 - val_loss: 0.7555 - val_accuracy: 0.7230\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.3968 - accuracy: 0.8603 - val_loss: 0.7514 - val_accuracy: 0.7304\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.2346 - accuracy: 0.9228 - val_loss: 0.8350 - val_accuracy: 0.7175\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.1198 - accuracy: 0.9639 - val_loss: 0.9863 - val_accuracy: 0.7132\n",
            "Number of hidden units:  200 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 1.0961 - accuracy: 0.5643 - val_loss: 0.8294 - val_accuracy: 0.7160\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.6108 - accuracy: 0.7817 - val_loss: 0.7603 - val_accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.4135 - accuracy: 0.8562 - val_loss: 0.7366 - val_accuracy: 0.7254\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 12s 72ms/step - loss: 0.2547 - accuracy: 0.9146 - val_loss: 0.8222 - val_accuracy: 0.7164\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.1421 - accuracy: 0.9550 - val_loss: 0.9153 - val_accuracy: 0.7128\n",
            "Number of hidden units:  200 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 1.1111 - accuracy: 0.5536 - val_loss: 0.8674 - val_accuracy: 0.7124\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.6295 - accuracy: 0.7782 - val_loss: 0.7813 - val_accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.4457 - accuracy: 0.8452 - val_loss: 0.7317 - val_accuracy: 0.7312\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.3020 - accuracy: 0.8964 - val_loss: 0.7713 - val_accuracy: 0.7187\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.1918 - accuracy: 0.9363 - val_loss: 0.8746 - val_accuracy: 0.7117\n",
            "Number of hidden units:  300 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 1.0734 - accuracy: 0.5768 - val_loss: 0.7820 - val_accuracy: 0.7164\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 20s 127ms/step - loss: 0.5801 - accuracy: 0.7969 - val_loss: 0.7649 - val_accuracy: 0.7226\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.3555 - accuracy: 0.8792 - val_loss: 0.7973 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.1708 - accuracy: 0.9486 - val_loss: 0.9753 - val_accuracy: 0.7128\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.0657 - accuracy: 0.9809 - val_loss: 1.2147 - val_accuracy: 0.7081\n",
            "Number of hidden units:  300 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 21s 128ms/step - loss: 1.0811 - accuracy: 0.5687 - val_loss: 0.8081 - val_accuracy: 0.7199\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.6004 - accuracy: 0.7888 - val_loss: 0.7516 - val_accuracy: 0.7214\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.3979 - accuracy: 0.8661 - val_loss: 0.7412 - val_accuracy: 0.7297\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.2300 - accuracy: 0.9246 - val_loss: 0.8348 - val_accuracy: 0.7167\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 20s 127ms/step - loss: 0.1126 - accuracy: 0.9663 - val_loss: 0.9997 - val_accuracy: 0.7089\n",
            "Number of hidden units:  300 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 1.0884 - accuracy: 0.5659 - val_loss: 0.8264 - val_accuracy: 0.7140\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 21s 128ms/step - loss: 0.6087 - accuracy: 0.7832 - val_loss: 0.7580 - val_accuracy: 0.7148\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.4160 - accuracy: 0.8556 - val_loss: 0.7413 - val_accuracy: 0.7308\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.2546 - accuracy: 0.9130 - val_loss: 0.8017 - val_accuracy: 0.7152\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 21s 129ms/step - loss: 0.1350 - accuracy: 0.9574 - val_loss: 0.9651 - val_accuracy: 0.7066\n",
            "Number of hidden units:  300 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 21s 128ms/step - loss: 1.0993 - accuracy: 0.5616 - val_loss: 0.8608 - val_accuracy: 0.7160\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.6233 - accuracy: 0.7788 - val_loss: 0.7780 - val_accuracy: 0.7140\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 21s 130ms/step - loss: 0.4458 - accuracy: 0.8439 - val_loss: 0.7282 - val_accuracy: 0.7340\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.3025 - accuracy: 0.8980 - val_loss: 0.7617 - val_accuracy: 0.7285\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 20s 128ms/step - loss: 0.1847 - accuracy: 0.9416 - val_loss: 0.8423 - val_accuracy: 0.7214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0PsF4z24Vbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70d386be-1cfe-4b7b-a106-e47ea1a11744"
      },
      "source": [
        "# This one no good!\n",
        "# kernel size: 4,6,8 - CHANGED\n",
        "# Increased stride in layers with wider kernels to 1-2-3\n",
        "# MaxOverTime pooling\n",
        "\n",
        "dropout = [0, 0.25, 0.35, 0.5]\n",
        "filters = [50, 100, 200, 300]\n",
        "# MaxOverTime pooling\n",
        "\n",
        "for i in filters:\n",
        "  for j in dropout:\n",
        "    print(\"Number of hidden units: \", i, \"dropout: \",j)\n",
        "\n",
        "    def custom_CNN(input_length=20,input_length2=12,vocab_size=dict_len+1,emb_dim = 100):\n",
        "      # define CNN layer\n",
        "      inputs1 = Input(shape=(input_length,))\n",
        "      emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "      conv1 = Conv1D(filters=i, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "      drop1 = Dropout(j)(conv1)\n",
        "      pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "      # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "      flat1 = Flatten()(pool1)\n",
        "\n",
        "      inputs2 = Input(shape=(input_length,))\n",
        "      emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "      conv2 = Conv1D(filters=i, kernel_size=6, padding='same', activation='relu', strides=2)(emb2)\n",
        "      drop2 = Dropout(j)(conv2)\n",
        "      pool2 = GlobalMaxPooling1D()(drop2)\n",
        "      # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "      flat2 = Flatten()(pool2)\n",
        "\n",
        "      inputs3 = Input(shape=(input_length,))\n",
        "      emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "      conv3 = Conv1D(filters=i, kernel_size=8, padding='same', activation='relu', strides=3)(emb3)\n",
        "      drop3 = Dropout(j)(conv3)\n",
        "      pool3 = GlobalMaxPooling1D()(drop3)\n",
        "      # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "      flat3 = Flatten()(pool3)\n",
        "\n",
        "      # merge CNN output with reference stats\n",
        "      merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "      dense1 = Dense(500, activation='relu')(merged)\n",
        "      outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "      model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # print(model.summary())\n",
        "      # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "      return model\n",
        "\n",
        "    np.random.seed(5)\n",
        "    random.seed(5)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                  inter_op_parallelism_threads=1)\n",
        "    tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "    # define model\n",
        "    model = custom_CNN(input_length=20, input_length2=12, vocab_size=dict_len+1,emb_dim = 100)\n",
        "    model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=5) \n",
        "    # save the model\n",
        "    # model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of hidden units:  50 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.1470 - accuracy: 0.5436 - val_loss: 0.8073 - val_accuracy: 0.7128\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.5631 - accuracy: 0.8037 - val_loss: 0.7626 - val_accuracy: 0.7312\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.2927 - accuracy: 0.9044 - val_loss: 0.8308 - val_accuracy: 0.7167\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.1130 - accuracy: 0.9686 - val_loss: 1.0286 - val_accuracy: 0.6999\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.0328 - accuracy: 0.9922 - val_loss: 1.2906 - val_accuracy: 0.6929\n",
            "Number of hidden units:  50 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.1553 - accuracy: 0.5382 - val_loss: 0.8373 - val_accuracy: 0.7050\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.6013 - accuracy: 0.7861 - val_loss: 0.7554 - val_accuracy: 0.7273\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.3607 - accuracy: 0.8763 - val_loss: 0.7667 - val_accuracy: 0.7254\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.1954 - accuracy: 0.9358 - val_loss: 0.9246 - val_accuracy: 0.6925\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.0939 - accuracy: 0.9721 - val_loss: 1.0291 - val_accuracy: 0.6944\n",
            "Number of hidden units:  50 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.1626 - accuracy: 0.5362 - val_loss: 0.8514 - val_accuracy: 0.7070\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.6112 - accuracy: 0.7863 - val_loss: 0.7551 - val_accuracy: 0.7336\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.3847 - accuracy: 0.8680 - val_loss: 0.7438 - val_accuracy: 0.7308\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 0.2238 - accuracy: 0.9266 - val_loss: 0.8512 - val_accuracy: 0.7081\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.1186 - accuracy: 0.9634 - val_loss: 0.9595 - val_accuracy: 0.6905\n",
            "Number of hidden units:  50 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 6s 37ms/step - loss: 1.1836 - accuracy: 0.5233 - val_loss: 0.9041 - val_accuracy: 0.7027\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.6376 - accuracy: 0.7755 - val_loss: 0.7718 - val_accuracy: 0.7324\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.4251 - accuracy: 0.8506 - val_loss: 0.7431 - val_accuracy: 0.7332\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.2788 - accuracy: 0.9041 - val_loss: 0.7964 - val_accuracy: 0.7132\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 6s 36ms/step - loss: 0.1665 - accuracy: 0.9459 - val_loss: 0.8854 - val_accuracy: 0.7050\n",
            "Number of hidden units:  100 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 1.1216 - accuracy: 0.5539 - val_loss: 0.7971 - val_accuracy: 0.7109\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.5513 - accuracy: 0.8073 - val_loss: 0.7569 - val_accuracy: 0.7300\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.2808 - accuracy: 0.9112 - val_loss: 0.8338 - val_accuracy: 0.7293\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.1008 - accuracy: 0.9708 - val_loss: 1.0498 - val_accuracy: 0.7058\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 7s 46ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 1.3275 - val_accuracy: 0.6948\n",
            "Number of hidden units:  100 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 1.1257 - accuracy: 0.5530 - val_loss: 0.8203 - val_accuracy: 0.7097\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.5756 - accuracy: 0.7976 - val_loss: 0.7575 - val_accuracy: 0.7246\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.3330 - accuracy: 0.8893 - val_loss: 0.7871 - val_accuracy: 0.7207\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.1557 - accuracy: 0.9493 - val_loss: 0.9680 - val_accuracy: 0.6929\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.0645 - accuracy: 0.9795 - val_loss: 1.1845 - val_accuracy: 0.6823\n",
            "Number of hidden units:  100 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 1.1309 - accuracy: 0.5484 - val_loss: 0.8388 - val_accuracy: 0.7093\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.5850 - accuracy: 0.7921 - val_loss: 0.7647 - val_accuracy: 0.7222\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.3518 - accuracy: 0.8802 - val_loss: 0.7626 - val_accuracy: 0.7246\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 7s 47ms/step - loss: 0.1835 - accuracy: 0.9405 - val_loss: 0.9027 - val_accuracy: 0.6972\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.0850 - accuracy: 0.9732 - val_loss: 1.0923 - val_accuracy: 0.6839\n",
            "Number of hidden units:  100 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 1.1451 - accuracy: 0.5409 - val_loss: 0.8785 - val_accuracy: 0.7089\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 8s 50ms/step - loss: 0.6117 - accuracy: 0.7850 - val_loss: 0.7780 - val_accuracy: 0.7250\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 8s 49ms/step - loss: 0.3952 - accuracy: 0.8667 - val_loss: 0.7453 - val_accuracy: 0.7312\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 8s 48ms/step - loss: 0.2334 - accuracy: 0.9218 - val_loss: 0.8275 - val_accuracy: 0.6995\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 8s 47ms/step - loss: 0.1272 - accuracy: 0.9591 - val_loss: 1.0022 - val_accuracy: 0.6780\n",
            "Number of hidden units:  200 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 1.0851 - accuracy: 0.5707 - val_loss: 0.7811 - val_accuracy: 0.7195\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.5401 - accuracy: 0.8140 - val_loss: 0.7741 - val_accuracy: 0.7230\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.2707 - accuracy: 0.9118 - val_loss: 0.8808 - val_accuracy: 0.7132\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.0979 - accuracy: 0.9706 - val_loss: 1.0955 - val_accuracy: 0.6987\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.0288 - accuracy: 0.9911 - val_loss: 1.4306 - val_accuracy: 0.7007\n",
            "Number of hidden units:  200 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 1.0877 - accuracy: 0.5706 - val_loss: 0.8025 - val_accuracy: 0.7210\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.5623 - accuracy: 0.8040 - val_loss: 0.7631 - val_accuracy: 0.7214\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.3148 - accuracy: 0.8936 - val_loss: 0.8020 - val_accuracy: 0.7265\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.1407 - accuracy: 0.9548 - val_loss: 1.0046 - val_accuracy: 0.6901\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.0565 - accuracy: 0.9824 - val_loss: 1.2693 - val_accuracy: 0.6776\n",
            "Number of hidden units:  200 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 1.0943 - accuracy: 0.5647 - val_loss: 0.8239 - val_accuracy: 0.7226\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.5752 - accuracy: 0.8000 - val_loss: 0.7609 - val_accuracy: 0.7218\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 14s 86ms/step - loss: 0.3384 - accuracy: 0.8863 - val_loss: 0.7834 - val_accuracy: 0.7218\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.1631 - accuracy: 0.9483 - val_loss: 0.9894 - val_accuracy: 0.6858\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.0732 - accuracy: 0.9772 - val_loss: 1.1777 - val_accuracy: 0.6776\n",
            "Number of hidden units:  200 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 1.1027 - accuracy: 0.5584 - val_loss: 0.8549 - val_accuracy: 0.7187\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.5981 - accuracy: 0.7900 - val_loss: 0.7778 - val_accuracy: 0.7144\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.3731 - accuracy: 0.8726 - val_loss: 0.7495 - val_accuracy: 0.7242\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 14s 86ms/step - loss: 0.2048 - accuracy: 0.9318 - val_loss: 0.8866 - val_accuracy: 0.6897\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 14s 89ms/step - loss: 0.1056 - accuracy: 0.9645 - val_loss: 1.0284 - val_accuracy: 0.6866\n",
            "Number of hidden units:  300 dropout:  0\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 1.0701 - accuracy: 0.5761 - val_loss: 0.7815 - val_accuracy: 0.7261\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.5390 - accuracy: 0.8136 - val_loss: 0.7789 - val_accuracy: 0.7261\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.2682 - accuracy: 0.9137 - val_loss: 0.9152 - val_accuracy: 0.7144\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0992 - accuracy: 0.9693 - val_loss: 1.1930 - val_accuracy: 0.6890\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 1.5550 - val_accuracy: 0.6788\n",
            "Number of hidden units:  300 dropout:  0.25\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 25s 154ms/step - loss: 1.0736 - accuracy: 0.5724 - val_loss: 0.8053 - val_accuracy: 0.7210\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.5557 - accuracy: 0.8047 - val_loss: 0.7684 - val_accuracy: 0.7250\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.3096 - accuracy: 0.8961 - val_loss: 0.8445 - val_accuracy: 0.7156\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.1401 - accuracy: 0.9549 - val_loss: 1.0613 - val_accuracy: 0.6823\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0547 - accuracy: 0.9827 - val_loss: 1.3280 - val_accuracy: 0.6772\n",
            "Number of hidden units:  300 dropout:  0.35\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 25s 155ms/step - loss: 1.0791 - accuracy: 0.5674 - val_loss: 0.8215 - val_accuracy: 0.7167\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.5698 - accuracy: 0.7989 - val_loss: 0.7631 - val_accuracy: 0.7226\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 25s 153ms/step - loss: 0.3265 - accuracy: 0.8895 - val_loss: 0.8188 - val_accuracy: 0.7175\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.1522 - accuracy: 0.9509 - val_loss: 0.9853 - val_accuracy: 0.6917\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 1.2643 - val_accuracy: 0.6678\n",
            "Number of hidden units:  300 dropout:  0.5\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 1.0873 - accuracy: 0.5662 - val_loss: 0.8581 - val_accuracy: 0.7128\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.5869 - accuracy: 0.7936 - val_loss: 0.7814 - val_accuracy: 0.7136\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.3583 - accuracy: 0.8773 - val_loss: 0.7914 - val_accuracy: 0.7136\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.1872 - accuracy: 0.9373 - val_loss: 0.9175 - val_accuracy: 0.6913\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.0911 - accuracy: 0.9694 - val_loss: 1.1210 - val_accuracy: 0.6729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRFXm4NO7U5S",
        "colab_type": "text"
      },
      "source": [
        "## Using CV: results closer to Kaggle \n",
        "\n",
        "Summary:\n",
        "\n",
        "Best CNN, one layer (emb = 100, 500 hu in Conv1D, 250 in Dense) - **72.6% on Kaggle**\n",
        "\n",
        " * CV seed 10: 73.82% (+/- 1.49%)\n",
        " * CV seed 1234: 73.65% (+/- 1.13%)\n",
        "\n",
        " \n",
        " Best GRU: Emb=128, GRU=256 - **72.38% on Kaggle (with Word2Vec embeddings)**\n",
        " \n",
        " * dropout=0, CV seed 10: 73.14% (+/- 1.18%)\n",
        " * dropout=0.3, CV seed 10: 73.38% (+/- 1.26%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4vVrYG91uP",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L_v0I2SWxab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7c3ec14-86f2-480a-89fe-ba396c64c979"
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                      input_length=20))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(500,3,\n",
        "                  padding='same', \n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(250))\n",
        "  # model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(5, activation=\"softmax\"))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\t\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70657, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70657 to 0.71049, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71049 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72770\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.72770 to 0.73161, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73161\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73161\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.16%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68936, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68936 to 0.73005, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73005 to 0.73083, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73083 to 0.73474, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73474 to 0.73709, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73709\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73709\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.71%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70892, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70892 to 0.73161, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73161 to 0.73787, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73787\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73787 to 0.73944, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73944\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73944\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.94%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69484, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69484 to 0.71987, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71987 to 0.72926, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72926 to 0.73474, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73474\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73474\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73474\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.47%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70501 to 0.72613, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.72613\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72613 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73239\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.24%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71909, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.71909 to 0.75117, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.75117 to 0.76526, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.76526 to 0.77387, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.77387\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.77387\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.77387\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 77.39%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70814, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70814 to 0.71987, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71987 to 0.73944, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73944\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73944\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73944\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73944\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.94%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67684, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67684 to 0.71753, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71753 to 0.72222, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72222\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72222\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72222\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72222\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.22%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70814, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70814 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73239 to 0.75274, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75274\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75274\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75274\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75274\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 75.27%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67502, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67502 to 0.71026, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71026 to 0.71339, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.71339 to 0.71887, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71887\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71887\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71887\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 71.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YctHjyVcPIVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0395c8ff-c928-454a-a165-4d2a341cf185"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.82% (+/- 1.49%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcCJiEyKAU5EZFL/Hzqw2NbG+jFRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x81mXsNGjoeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df5ee612-b3c1-48c7-906c-a425a55a82be"
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=100,\n",
        "                      input_length=20))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(500,3,\n",
        "                  padding='same', \n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(250))\n",
        "  # model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(5, activation=\"softmax\"))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69797, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69797 to 0.72222, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72222 to 0.73318, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73318 to 0.74022, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74022\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74022\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74022\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.02%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69484, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69484 to 0.72457, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72457 to 0.73083, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73083\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73083\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73083\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73083\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.08%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69249, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69249 to 0.74726, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.74726\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.74726\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74726\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74726\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74726\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.73%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70501 to 0.72144, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72144 to 0.72926, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72926\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72926\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72926\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72926\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.93%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68936, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68936 to 0.69562, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.69562 to 0.72144, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72144\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72144\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72144\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72144\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.14%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71127, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.71127 to 0.74335, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.74335\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.74335 to 0.75587, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.75587 to 0.75822, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75822\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75822\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 75.82%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68153, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68153 to 0.71205, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71205 to 0.73552, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73552\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73552\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73552\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73552\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.55%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69875, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69875 to 0.71909, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71909 to 0.73083, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73083 to 0.73787, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73787 to 0.74257, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74257\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74257\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.26%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70266, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70266 to 0.71909, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.71909\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71909\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71909\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71909\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71909\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 71.91%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71104, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.71104 to 0.73923, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.73923\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73923 to 0.74080, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74080\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74080\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74080\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxMIPbYRnpJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "af5b062e-b489-43ee-84ed-98df717dc969"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.65% (+/- 1.13%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWaI3xpgYZ4neGGNi3P8PEcGiho31J34AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAW2lrLSG2Dy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2c6ac82-9389-4644-e5fb-f386106a247f"
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=128,\n",
        "                      input_length=20))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(50, 3,\n",
        "  #  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                  padding='same', # 0.4 points better with same\n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(Conv1D(100, 4,\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(250))\n",
        "  # model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(5, activation=\"softmax\"))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67136, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67136 to 0.68388, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.68388 to 0.73005, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73005 to 0.74648, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74648\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74648\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74648\n",
            "accuracy: 74.65%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.59937, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.59937 to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70501 to 0.72457, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72457\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72457\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72457\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72457\n",
            "accuracy: 72.46%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70266, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70266 to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70501 to 0.72926, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72926 to 0.73631, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73631 to 0.75039, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75039\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75039\n",
            "accuracy: 75.04%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.64554, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.64554 to 0.71674, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71674 to 0.73005, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73005\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73005\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73005\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73005\n",
            "accuracy: 73.00%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68153, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68153 to 0.70657, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70657 to 0.71518, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71518\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71518\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71518\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71518\n",
            "accuracy: 71.52%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.65493, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.65493 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72770 to 0.72848, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72848 to 0.75117, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75117\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75117\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75117\n",
            "accuracy: 75.12%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67293, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67293 to 0.71049, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71049 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73239\n",
            "accuracy: 73.24%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61659, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.61659 to 0.70579, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70579 to 0.71362, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71362\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71362\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71362\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71362\n",
            "accuracy: 71.36%\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.66588 to 0.71283, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71283 to 0.73787, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73787 to 0.74570, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74570\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74570\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74570\n",
            "accuracy: 74.57%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61942, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.61942 to 0.69851, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.69851\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.69851 to 0.71026, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71026\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71026\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71026\n",
            "accuracy: 71.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv7BKDjjHvl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "3e1eb2ca-53ee-4595-9234-012c7cbe7c7a"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.20% (+/- 1.50%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvuLblQbVgG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4902060b-0a99-4ae6-f807-c6db4526c223"
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=128,\n",
        "                      input_length=20))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(50, 3,\n",
        "  #  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                  padding='same', # 0.4 points better with same\n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(Conv1D(100, 4,\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(250))\n",
        "  # model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(5, activation=\"softmax\"))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=[earlystop, checkpoint],verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67136, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67136 to 0.68388, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.68388 to 0.73005, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73005 to 0.74648, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74648\n",
            "Epoch 00005: early stopping\n",
            "accuracy: 74.65%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.59937, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.59937 to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70501 to 0.72457, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72457\n",
            "Epoch 00004: early stopping\n",
            "accuracy: 72.46%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70266, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70266 to 0.70501, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70501 to 0.72926, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72926 to 0.73631, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73631 to 0.75039, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75039\n",
            "Epoch 00006: early stopping\n",
            "accuracy: 75.04%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.64554, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.64554 to 0.71674, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71674 to 0.73005, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73005\n",
            "Epoch 00004: early stopping\n",
            "accuracy: 73.00%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68153, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68153 to 0.70657, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70657 to 0.71518, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71518\n",
            "Epoch 00004: early stopping\n",
            "accuracy: 71.52%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.65493, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.65493 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72770 to 0.72848, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72848 to 0.75117, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75117\n",
            "Epoch 00005: early stopping\n",
            "accuracy: 75.12%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67293, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67293 to 0.71049, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71049 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73239\n",
            "Epoch 00004: early stopping\n",
            "accuracy: 73.24%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61659, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.61659 to 0.70579, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70579 to 0.71362, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71362\n",
            "Epoch 00004: early stopping\n",
            "accuracy: 71.36%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.66588, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.66588 to 0.71283, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71283 to 0.73787, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73787 to 0.74570, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74570\n",
            "Epoch 00005: early stopping\n",
            "accuracy: 74.57%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61942, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.61942 to 0.69851, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.69851\n",
            "Epoch 00003: early stopping\n",
            "accuracy: 69.85%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTVwFTBheA_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43786e4a-9089-4fea-b934-60373e1514f3"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.08% (+/- 1.70%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuUbhV2aq7KB",
        "colab_type": "text"
      },
      "source": [
        "### GRU\n",
        "\n",
        "Emb=128, GRU=256, drop=0 or drop=0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2JzyV2LEkQ9A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef769171-8e41-4991-9419-844f9d0c87c3"
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=128)) \n",
        "  model.add(GRU(256, dropout=0)) \n",
        "  model.add(Dense(5, activation=\"softmax\"))  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69014, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69014 to 0.71831, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71831 to 0.72144, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72144 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73239 to 0.73474, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73474\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73474\n",
            "accuracy: 73.47%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70736, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70736 to 0.72222, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.72222\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72222 to 0.73396, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73396\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73396\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73396\n",
            "accuracy: 73.40%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69797, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69797 to 0.70266, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70266 to 0.73161, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73161 to 0.74491, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.74491 to 0.74883, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74883\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74883\n",
            "accuracy: 74.88%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67762, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67762 to 0.71831, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71831 to 0.72613, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72613\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72613\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72613\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72613\n",
            "accuracy: 72.61%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68779, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68779 to 0.70423, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70423 to 0.71049, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.71049 to 0.72066, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72066\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72066\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72066\n",
            "accuracy: 72.07%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68858, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68858 to 0.73944, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73944 to 0.74883, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.74883\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74883\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74883\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74883\n",
            "accuracy: 74.88%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.70188, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.70188 to 0.72066, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72066 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72770\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72770\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72770\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72770\n",
            "accuracy: 72.77%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68701, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68701 to 0.71127, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.71127\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.71127\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71127\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71127\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71127\n",
            "accuracy: 71.13%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71909, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.71909 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73239 to 0.73318, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73318 to 0.74100, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74100\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74100\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74100\n",
            "accuracy: 74.10%\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.66719, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.66719 to 0.70164, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70164 to 0.70399, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.70399 to 0.72044, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72044\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72044\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72044\n",
            "accuracy: 72.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CWFAzyxpdrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "28407d54-3736-4224-81ed-5306cae2997d"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.14% (+/- 1.18%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXRb93Xo++8GOM8UCVISBw2WSJOy\nNVLyFEeKJStxBidumlhO0sRuUzdtcm/T1fuyOq3kvXa1K/e2vc29t71N3TRumqS2Y9dO09RJLCfx\nEMmDOGmwRksiCFISSYkASXEmsd8fBBVKhkSCBHAAcH/WwiJ5cA6wCUobB/v8fvsnqooxxpjU5XI6\nAGOMMbFlid4YY1KcJXpjjElxluiNMSbFWaI3xpgUZ4neGGNSXNpsO4hILfDUjE2rgS8DRcBvAj2h\n7X+kqs+HOf59wP8C3MA3VPWrsz1naWmprly5ctbgjTHGTGlqarqoqp5w90kk4+hFxA10ArcBjwCX\nVfWvZtn/JHAv0AEcAB5S1aM3ep6GhgZtbGycc1zGGLPYiUiTqjaEuy/S0s1O4LSqeue4/zbgbVU9\no6pjwJPAhyN8TmOMMQsQaaLfAzwx4+cviMghEfmmiBSH2b8C8M34uSO0zRhjTJzMOdGLSAZwP/B0\naNPfAzcBG4HzwF8vJBAReVREGkWksaenZ/YDjDHGzEkkZ/T3Ac2q2gWgql2qOqmqQeAfmSrTXKsT\nqJrxc2Vo2zuo6mOq2qCqDR5P2OsJxhhj5iGSRP8QM8o2IrJsxn0PAEfCHHMAWCsiq0KfCPYAP5hP\noMYYY+ZnToleRHKZGjnz7IzN/0NEDovIIeA9wO+F9l0uIs8DqOoE8AXgJ8Ax4Huq+lYU4zfGGDOL\nWcfRA6jqIFByzbZfu86+54D3z/j5eeAd4+uNMcbEh82MTVGDoxP86xvtTAZtvYFE8uqpHk52DTgd\nhllkLNGnqMdeOcMfPXeYV0/ZCKZEEQwqv/PdZr787+EuZxkTO5boU9DoxCTffWNqTttLJyzRJ4qT\n3QMMjExwoM1P39C40+GYRcQSfQr64cHzXLw8RmleJq+ctESfKJq8fgAmg8pLJ7sdjsYsJpboU4yq\n8s/721hTlsdv77iJMxcHab805HRYBmhq81Oal0FpXiZ7j3Y5HY5ZRCzRp5gmr5/DnX08fOdK3lM7\nNfHsZTt7TAhN7X62rChmV10ZL5/oYWwi6HRIZpGwRJ9iHt/fRkFWGr+yuYJVpblULcm2On0C6BkY\nxXtpKJToyxkYneDNs71Oh2UWCUv0KeRcYJgfH7nAnm3V5GSkISLsqClj/+lLjE5MOh3eojZdn9+y\nopi71pSSmebixWNWvjHxYYk+hXz7dS+qyqfvWHFl2/YaD8Pjkxw463cwMtPc7ifD7eKWikKyM9zc\nvbaUF491Ecl6EMbMlyX6FDEyPskTb7azu34plcU5V7bfcVMJGW6X1ekd1tjWy62VhWSmuQHYVVdO\nh3+YEzZ5ysSBJfoU8f2WTgJD4zx818qrtudmprF1VbHV6R00Mj7Jkc5+Glb8csmGe+rKAHjRRt+Y\nOLBEnwJUlcf3tVG3rIDbVi15x/07aso41X2ZzsCwA9GZt871MTYZZPOMRF+Wn8XGqiL2HrNPWib2\nLNGngNfOXOJE1wCP3LUSEXnH/dunh1naWb0jGtumro9srr56EbZ768s56AvQ3T/iRFhmEbFEnwIe\n39fGktwM7t+wPOz9a8vyWF6YZXV6hzR5/awsycGTn3nV9l115QD89Lj9XUxsWaJPcu2XhnjxWBef\n2FZNVro77D4iwvZaD/vevsT4pE3SiSdVpbndf1XZZlpNeR6VxdlWpzcxZ4k+yf3La224RfjU7Stu\nuN/2mjIuj05cGc9t4sN7aYiLl8doWPHOayciwq66cn7x9kWGx2yeg4kdS/RJbHB0gqcafdx36zKW\nFmbdcN+71pSQ5hJetiZncTVzolQ499aXMzoR5BdvX4xnWGaRsUSfxP6tuYOBkQkeuWZIZTj5Wels\nWWHDLOOt0esnPyuNtWV5Ye/ftmoJ+VlpVr4xMTVroheRWhFpnXHrF5Evzrj/90VERaT0OsdPzjjW\nFgaPkmBwqkvlhspCNlUVzemY7bUejp3vp8tGecRNs9fP5upiXK53joYCSHe72FFbxk+PdxG01cBM\njMya6FX1hKpuVNWNwBZgCHgOQESqgN1A+w0eYnj6eFW9PxpBG3jlVA9negZ55K5VYYdUhrOjZmqS\njpVv4qNveJyT3QPXLdtM21VXxsXLY7R2BOIUmVlsIi3d7AROq6o39PPfAF8C7FQkzh7f14YnP5P3\n37pszsfULcunLD/TEn2ctLT7UeWqGbHh7KgpI80lVr4xMRNpot8DPAEgIh8GOlX14CzHZIlIo4i8\nLiIfud5OIvJoaL/Gnh5LRDdyuucyL5/s4VO3rSAjbe5/QhFhe42HV0/2MGHDLGOu2evHJbBhltJa\nYU46W1cusW6WJmbmnCVEJAO4H3haRHKAPwK+PIdDV6hqA/AJ4GsiclO4nVT1MVVtUNUGj8cz17AW\npW/tbyPD7eITt1VHfOz2Wg/9IxMctDJBzDV6/dQtKyA3M23WfXfVl3Oy67KtBmZiIpIz+vuAZlXt\nAm4CVgEHRaQNqASaRWTptQepamfo6xngJWDTAmNe1PqGx3mmqYMPbVj+jpmWc3H3Gg8usUXDY21i\nMkirLzBr2WbarukmZ3ZWb2IgkkT/EKGyjaoeVtUyVV2pqiuBDmCzql6YeYCIFItIZuj7UuAu4GhU\nIl+knm70MTQ2OachleEU5qSzqbrY6vQxdvzCAENjk2FnxIazoiSXmvI8S/QmJuaU6EUkF7gXeHYO\n+zaIyDdCP9YBjSJyEPg58FVVtUQ/T5NB5VuvtbF1ZTG3VBTO+3F21Hg41NHHxcuj0QvOXGW2iVLh\n7Kor542zvfQNjccqLLNIzSnRq+qgqpaoat917l+pqhdD3zeq6mdD3+9X1VtVdUPo6z9FL/TF52fH\nu/H1DvPIXasW9DjT3SxfPWVn9bHS5PWztCCLiqLsOR+zq76cyaDykjWfM1FmM2OTyOP7zrK8MIvd\n9eULepxblhdSkpthdfoYavL62bKieM5zHAA2VhZRmpfBi9aj3kSZJfokcfxCP/tPX+LX7lhJmnth\nfzaXS3h3jYdXTvYwabMxo+583zCdgeGIyjYw9Xe55+YyXjrRbV1GTVRZok8S39rfRla6iz1bq6Ly\neDtqPfiHxjncGbYaZxag2Ts1dDXSRA9TdfqBkQkOnO2NdlhmEbNEnwT8g2M829zJA5sqKM7NiMpj\n3r3Wg4itOhULjd5estJd1C8viPjYd60tJTPNxV4bfWOiyBJ9EnjiQDujE0EevnNhF2FnWpKbwfrK\nIrvwFwPNXj8bKotIn0eJLScjjXetKeXFY12oWlnNRIcl+gQ3MRnk2695ufOmEmqX5kf1sbfXeDjo\nC+AfHIvq4y5mw2OTvHWuf15lm2m76svx9Q5zsutyFCMzi5kl+gT3k7e6ON83suAhleHsqPUQVHjV\nFr2ImoMdASaCSsPK+Sf6nTfbLFkTXZboE9zj+85SvSSHe0L/+aNpQ2URRTnpVqePoumJUpuq5p/o\nywqy2FBVxF7rZmmixBJ9Ajvc0Uej18+n71iB+zoLVyyE2yXcvdbDyyd7bNGLKGny+rnJk7vgi+a7\nbi6j1Rege8AWiTELZ4k+gT2+/yy5GW4+HqUhleFsr/Fw8fIoR8/3x+w5FotgUGlu94ddCDxSu0KT\n4n5+3C6Wm4WzRJ+gegZG+eHB8/zqlkoKstJj9jzvrplaAdKanC3cmYuXCQyNL+hC7LSbl+ZTUZTN\n3qOW6M3CWaJPUP/6Rjtjk0E+fefKmD5PWX4W65YXWJ0+Cq40MlvAhdhpIsK99eX84u0ehscmF/x4\nZnGzRJ+AxiaCfOcNLztqPdzkyYv58+2o9dDU7qdv2LomLkST109RTjqrS3Oj8ni76soZGQ+yz0ZF\nmQWyRJ+A/vPwOXoGRmMypDKc7TVlTAaV/ZZQFqTR62dLdWSNzG5k26ol5Gem2TBLs2CW6BOMqvL4\nvjZWe3K5e01pXJ5zc3UR+Vlp1s1yAXoHxzjTMxiVss20jDQX22s9vHis20ZFxdDoxCRHUrznkyX6\nBNPcHuBQRx+P3LkSVwyGVIaT5nbxrjWlvHyyx6bdz1PzdH2+OnqJHqbKNxcvj9oavzH0r2+086G/\n/QVnLw46HUrMWKJPMI/vO0t+Vhq/srkyrs+7o9bDhf4RTnQNxPV5U0VTu580l7Chqiiqj7uj1oPb\nJVa+iaHGNj+q8MJbF2bfOUnNmuhFpFZEWmfc+kXkizPu/30R0dCasOGO/4yInArdPhPN4FPN+b5h\nfnTkAg82VJGbmRbX5353zdSqUzb6Zn6avH7WVRSSle6O6uMW5WSwdWUxP7XFSGKm1Tf1aemFFJ6J\nPGuiV9UTqrpRVTcCW4Ah4DkAEakCdgPt4Y4VkSXAV4DbgG3AV0Qkup9tU8h3XvcSVOUzMR5SGc6y\nwmxuXppvdfp5GJsIctAXiHrZZtquunKOXxjA1zsUk8dfzLr7R+gMDFNekElzuz9lZyJHWrrZCZxW\nVW/o578BvgRcr7D7XmCvqvaqqh/YC7xvXpGmuJHxSf71jXZ21ZVTtSTHkRi213ho9PZyeXTCkedP\nVkfP9zM6EVxQI7MbuTc0S9bKN9HXEjqb/68716JKyn5yijTR7wGeABCRDwOdqnrwBvtXAL4ZP3eE\ntplr/KD1HP6hcR65a6VjMWyv9TA+acMsI9XYNrUaVDRmxIazoiSXtWV5luhjoNUXIM0lfHRzJdVL\nclK2Tj/nRC8iGcD9wNMikgP8EfDlaAUiIo+KSKOINPb0LK7ygaryzX1nuXlpPnesLnEsjoYVS8jN\ncFs7hAg1t/upLM6mvCArZs+xq76cN8702qS2KGtp91O/vICsdDe768vZ9/allPxEG8kZ/X1As6p2\nATcBq4CDItIGVALNIrL0mmM6gZkduSpD295BVR9T1QZVbfB4PBGElfzeONvL8QsDPHznyqhNtpmP\njDQXd64p5aUTNsxyrlSVJq8/Zmfz03bVlTERVHsTjqLJoHK4o4+NoZFSu9ctZWwymJIDEiJJ9A8R\nKtuo6mFVLVPVlaq6kqmSzGZVvfZzz0+A3SJSHLoIuzu0zczw+L6zFOek85FNzle1ttd46AwMc7on\ndccUR1OHf5iu/tGYJ/qNVcWU5GbwYgqPDIm3U90DDI5NXkn0W1YUsyQ3gxeOpl75Zk6JXkRygXuB\nZ+ewb4OIfANAVXuBPwMOhG5/GtpmQny9Q+w92sVD26qjPjRvPraHhlm+dCI1L0pFW3N7aKJUjBO9\n2yXcc3MZL53oZnwyGNPnWixa2qcuxG4KjZZyu4RddWX87Hg3YxOp9RrPKdGr6qCqlqhq2HnCoTP7\ni6HvG1X1szPu+6aqrgndHo9O2Knj2697ERE+dfsKp0MBoGpJDjd5cq1EMEeNbX5yM9zUlkd3Pd9w\ndtWX0z8ywYE2O1eKhtb2AEU56aws+eUot931SxkYmeCNs5ccjCz6bGasg4bGJnjyzXbed8tSlhdl\nOx3OFTtqy3jjbK+1x52DJq+fTdXFpLlj/1/p7rWlZKS5eNF61EdFqy/Ahsqiq66LvWttKdnpbl54\nK7VKZJboHfRvzZ30j0zw6w4OqQxne42HsYkgr59JrbOaaLs8OsHxC/1sjnHZZlpORhrvWlPK3mMX\n7GL5Ag2MjHOye4BN1Ve3rMhKd7O9xsPeo10p1UjOEr1DVJV/3neWWysK2RyjGZXztW3VErLSXVa+\nmUVre4Cgxr4+P9OuunJ8vcOc6r4ct+dMRYc7+lDlyoXYmXavK+dC/wiHU6ijpSV6h7x66iKnewZ5\n5C5nh1SGk5Xu5o7VJXZBdhZNXj8ivOOsMJZ21pUBsNdG3yzI9IzYcIn+npvLcLskpUbfWKJ3yOP7\nzlKal8kH1i9zOpSwdtSW0XZpiLYUbt26UI3eXmrL82O6pu+1yguyWF9ZaLNkF6ilPcDq0lyKcjLe\ncV9RTga3rVqSUnV6S/QOOHtxkJ+f6OGTt1WTmeb8kMpwpodZWvkmvMmg0toeiGvZZtquunJafQF6\nBkbj/typQFVp9QXCns1P211fzqnuy5zpSY0SmSV6B3xrfxvpbuGTt1c7Hcp1rSzNZWVJjiX66zjV\nPcDA6IRjiV4Vfn7cSmvz0RkY5uLlUTbeoOR277qpSf6pUiKzRB9n/SPjPN3o44Prl1OWH7veKNGw\nvcbD/tMXGRm3YZbXamyLz0SpcOqW5VNRlM1eK9/My5WJUlXX/9tVFGVzS0VByvSot0QfZ880djA4\nNulol8q52lFbxsh40CbohNHs9VOal0m1Ay2lRaZmcL56qsfehOeh1RcgM83FzctuPMltd/3SlOlR\nb4k+jiaDyrdea2PLimLWV8ZvpMZ83b66hIw0ly1GEkaj18+WFUWOjZjaVV/OyHiQfdZSOmKtvgC3\nVBSSPsskt93rylOmR70l+jj6+fFuvJeGeNiBFaTmIzvDzW2rllid/hrdAyO09w7RsGKJYzHctqqE\nvMw0G30TobGJIIc7+9g0h7V9a8vzU6ZHvSX6OPrn/W0sLcjifbdc2805cW2v8fB292U6/LaM3bRm\n71SNN14zYsPJSHOxvcbDi8e6U2oGZ6wdv9DP2ETwhhdip4lIyvSot0QfJye7BvjF2xf5tTtWzPqR\nMZHsqJ2aoGNn9b/U5O0lI83FLRUFjsaxq76MnoHRlJrBGWutN5goFU6q9KhPnoyT5B7f10ZmmouH\ntiXukMpwbvLkUlGUbXX6GZq8ftZXFDo+B+I9tVMzOK18M3ct7QE8+ZlUzLGJYKr0qLdEHweBoTGe\na+ngIxsrWJL7zpl4iUxE2FHrYf/bF1OuR/d8jIxPcqSz35FhldcqysmgYUVxyoz1jofpiVJzvYie\nKj3qLdHHwZMHfIyMB3k4CYZUhrO9xsPg2CSNXhtmeaSzj7HJYEIkeoB768s5fmEAX69dQ5lNYGiM\nsxcH51y2mZYKPeot0cfYxGSQf9nfxu2rl1C3zNma7nzduaaUdLdYnZ6psg04eyF2pp115QD81Mo3\ns5quz0fahC4VetTPmuhFpFZEWmfc+kXkiyLyZyJyKLTtBRFZfp3jJ2cc+4Po/wqJbe/RLs71jfDI\nXaucDmXe8jLTaFixJOkvSEVDo9fPypIcSvMynQ4FgFWludzkyeXFFBjrHWst7QFEiHgOSyr0qJ81\n0avqCVXdqKobgS3AEPAc8Jequj60/YfAl6/zEMPTx6vq/VGLPEk8vq+NyuJsdoXOvJLVjloPxy8M\ncKEv+WcJzpeq0uz1s8XB8fPh7Kov5/Uzl+gfGXc6lITW6gtQU5ZPXmZaxMcme4/6SEs3O4HTqupV\n1f4Z23OB5Hyri6EjnX282dbLZ+5YiduVWD3nI7W9drqb5eI9c/ReGuLS4FjC1Oen3VtXzkRQecVK\na9c1l46VN5LsPeojTfR7gCemfxCRPxcRH/BJrn9GnyUijSLyuoh8ZJ5xJqV/3t9Gdrqbj2+tcjqU\nBastz2dpQdairtM3hurzDSsTK9Fvqp4aAviijb65rrMXB+kbHp/3IjHJ3qN+zoleRDKA+4Gnp7ep\n6h+rahXwXeAL1zl0hao2AJ8AviYiN13n8R8NvSE09vTML5k809TB3qNdHDvf7/jH2IuXR/lB6zk+\nuqWCwuz4LUwRKyLC9hoPr566yMRk8g4zW4gmr5/8rDTWePKcDuUqbpdwz81TQwDHF+nfZjZXJkot\nYDWwZO5RH0mx6j6gWVXDvaV9F3ge+Mq1d6hqZ+jrGRF5CdgEnA6z32PAYwANDQ0Rl4GCQeUPnz3E\n+OQvD83PSqOiKJvK4mwqi3OoKMqmojj7yteS3IyYNaV64o12xiaDPHxn8l6EvdaOWg9PNfpo8QXY\nujKx6tTx0OTtZXN1Ma4ELMPtqivnmaYOGtv83HFTidPhJJxWX4DcDDdry27csfJG7l23lP/3P46y\n92gXv7U9sd7sZxNJon+Iq8s2a1X1VOjHDwPHrz1ARIqBIVUdFZFS4C7gfywg3usSgf1/sJPOwDCd\n/mE6A0N0+Ke+7/AP88aZXgau6VeRle4KJf2cK28IV74WZ1OWnzWv2vrYRJBvv+7l3TUe1pQl1z+I\nG7lzTSlul/DSie5Fl+j7hsc52XWZD60PO7jMcXevLSUjzcWLx7os0YfR0h5gfWXRgq6VzexR/1vb\nwxYmEtacEr2I5AL3Ar81Y/NXRaQWCAJe4HOhfRuAz6nqZ4E64B9EJMhUmeirqno0ivHPjBFPfiae\n/MzrXnDpGx6nwz8UeiMYvvK1wz/Mkc4+egfHrto/zSUsK8qaejMoyrnyBlAZ+kSwrDCbjLR3Vr9+\ndOQ83QOj/PeProzFr+qYwux0tlQX8/LJHv6f997sdDhx1dLu3EIjc5GbmcadN5Xw4rEu/uQDdQm3\n4LyTRsYnOXa+n9989+oFP9bu+qX8zYsn6R4YSfiFg2aaU6JX1UGg5JptH73Ovo3AZ0Pf7wduXWCM\nUVOYnU5hdiHrlheGvX9obIJzocQ//QYw/Waw7+2LdA2MoDOKSiJQFuqbUVmcc6Us9K9vtLO6NPfK\nuqupZHuth7/8yYmk+4e+UE1eP26XsGGeozbiYVddOX/y/SO83X2ZteXzL1GkmrfO9TER1HmPuJlp\n97py/ufek/z0WHdS9a2KfEBpCsvJSGNNWT5rrlPHG5sIcr4vVA666hPBEC0+P88fPs9EaELFn314\nXULWchdqe81Uon/15EU+uqXS6XDipsnrp25ZPrnzGIMdLzvryviT78PeY12W6Gf45dKBC0/0M3vU\nW6JPURlpLlaU5LKiJDfs/ZNBpXtghEuXx6hP0nYHs6lfVkBpXiYvnexZNIl+YjJIqy/AxxL8911W\nmM2tFYX89Fg3v7NjjdPhJIwWX4CKomzKChb+CXS6R/2/vObl8ujEvCZfOcF63USR2yUsK8zmlorC\nlDybB3C5podZ9jCZpNPBI3X8wgBDY5NsSYIL0Lvqymlu93Px8qjToSSM1vb5T5QKJxl71FuiNxHb\nXushMDTOwY6A06HExXQjs0S9EDvTrvoyVOFnxxfvDOaZugdG6AwMz3uiVDjJ2KPeEr2J2N1rSnEJ\nSXVGsxCNXj/LCrPmvFiFk+qXFbC8MMtmyYa0tke2otRcJGOPekv0JmLFuRlsqCripUXSDqHZ60+Y\ntsSzERF21pXz6qmLjIxPOh2O41p9AdJcwi0V4UfazVey9ai3RG/mZUdNGYc6Au+Ye5BqzvdNjaza\nUp0ciR6mulkOj0+y//RFp0NxXEt7gLplBWSlR3fZx2TrUW+J3szL9loPqvDqqdQ+q29K0EZmN3L7\n6iXkZrjZe3Rx1+kng8qhjuheiJ2WbD3qLdGbeVlfUciS3IyUr9M3ef1kp7uTanWwzDQ322s9/Ox4\nciShWHm7+zKDY5MxSfSQXD3qLdGbeXG5hLvXlvLyyZ6UTiZNXj8bqgpJdyfXf5VddeV09Y9y5Fzi\nJ6FYmW5bEc0RNzMlU4/65PrXaxLKjloPlwbHeOtc/+w7J6GhsQneOtefFMMqr/We2jJcwqIefdPq\nC1CYnc6q0vATHBcqmXrUW6I383b32qlePi+dSM1a8EFfH5NBTcpEX5ybQcPKJexdxGvJtvoCbKgq\nimmDt2TpUW+J3sxbaV4m6ysLU3bVqebQR//NSTTiZqZddWUcO99Ph3/I6VDi7vLoBCe6BqLS3+ZG\n7l23FIC9Cf7JyRK9WZDtNR6a2/30DaXewtRNXj9ryvIoyslwOpR5mV6Q/qeL8Kz+UEcA1YWtKDUX\nM3vUJzJL9GZBdtR6CCr84u3UGrMdDCpNXn9SjZ+/1mpPHqs9ubx4LLGTUCxcWTqwMvZtpXfXL6W5\n3U/3wEjMn2u+LNGbBdlQWURBVhovn0yts8YzFy/TNzzOliQaPx/OvXXlvH7mEgMOr6Ecby3tAVaV\n5lKcG/tPY7vXlaOa2J+cLNGbBUlzu7i7xsPLJ3tQTZ1hlo1tydPI7EZ21ZczPqm8cjK1PnHdiKrS\n6ovNRKlwZvaoT1SzJnoRqRWR1hm3fhH5ooj8mYgcCm17QUTCLqYpIp8RkVOh22ei/ysYp22v8dDV\nP8rxCwNOhxI1TV4/xTnprI7R0Lx42VxdTHFO+qIq35zrG6FnYDRuiX66R/2+ty9x+Zp1qRPFrIle\nVU+o6kZV3QhsAYaA54C/VNX1oe0/BL587bEisgT4CnAbsA34SmjBcJNCdtRMD7NMndE3Te1+tqwo\nTvq1V90u4Z6by/nZ8W4mJpOj0+JCTXesjNVEqXASvUd9pKWbncBpVfWq6sxZMrlAuM/t7wX2qmqv\nqvqBvcD75heqSVRlBVnULStImTp97+AYZ3oGk6Zj5Wx21ZXRNzxOY6hvT6prafeTkebi5qXxa1uR\n6D3qI030e4Anpn8QkT8XER/wScKc0QMVgG/Gzx2hbSbF7Kj10NjmT4mLfs3TjcxWJP6KUnNxd42H\nDLdr0cySbfUFuGV5ARlp8bsEmeg96uf8SohIBnA/8PT0NlX9Y1WtAr4LfGEhgYjIoyLSKCKNPT2J\n+fHHXN/2Gg8TQWX/6eToz30jjV4/6W5hfWV0e5g7JS8zjTtuKuHFY10pdcE8nPHJIIc7+9jkwLDY\nRO5RH8lb3n1As6qGOy34LvDRMNs7gaoZP1eGtr2Dqj6mqg2q2uDxeCIIyySCLSuKyctMS4k6fbPX\nz7rlhVHvYe6kXfXltF0a4nTPoNOhxNTx8wOMTgTjdiF2pkTuUR9Jon+Iq8s2a2fc92HgeJhjfgLs\nFpHi0EXY3aFtJsWku13ctaaEV5J8mOXYRJCDHYGkH1Z5rV11ZQApP/qm1TdVdnMi0Sdyj/o5JXoR\nyQXuBZ6dsfmrInJERA4xlcB/N7Rvg4h8A0BVe4E/Aw6Ebn8a2mZS0I7aMjoDw7zdndgNnm7krXN9\njE4EUy7RLyucmqqf6nX6Fl+A0rxMKoudWd83UXvUzynRq+qgqpaoat+MbR9V1VtCQyw/pKqdoe2N\nqvrZGft9U1XXhG6PR/9XMJ9XQlIAABggSURBVIlie2iYZTI3OZteUSrVEj1M9b5pavdz6fKo06HE\nTGv71EQpp4bFJmqPepsZa6JmeVE2NeV5SV2nb/L6qSzOprwgy+lQom5X3dRU/Z8dT41hsNfqGxrn\nzMXBuI6fv1ai9qi3RG+ianuNhzfP9jI0lpgzBG9EVWn0+mlIwbN5gHXLC1hakJWydfrWjtBEKQfq\n8zMlYo96S/QmqnbUljE2GeS1JBxm2eEfpmdgNCXLNjA1VX9XfRmvnLzIyPik0+FEXUu7HxG41eFh\nsYnYo94SvYmqhpXF5GS4k7JOP12fT5UZseHsqitneHyS184k3xvxbFp9AdaW5ZGfle5oHInYo94S\nvYmqzDQ3d95Uwksnkm+YZZPXT26GO65T5+PtjptKyM1wp9zom+mOlZuqEuNNOtF61FuiN1G3vcZD\ne+8QbZeSawm7Rq+fTdXFuF3J3cjsRjLT3Ly7xpNys2TbLg0RGBqP+YpSc5VoPeot0Zuo214zNTkn\nmRYNHxgZ58SF/pStz8+0q66crv5RjnT2z75zknByolQ4idaj3hK9ibrqkhxWl+YmVZ3+oK+PoKbm\n+PlrvSc01vuHh885HUrUtLYHyMlwU1Oe73QoQOL1qLdEb2Jie62H105fSprRHY3eXkRiv5h0IliS\nm8E9N5fxb02djKdIj/oWX4D1lYUJVXZLpB71luhNTGyv8TA6EeSNs8nR8aLJ66e2PJ8Ch0dsxMue\nrVVcvDyaEpOnRsYnOXa+n40JciF2WiL1qLdEb2Li9tUlZKa5kqJOPxlUWtpTr5HZjWyv8VBekMlT\nB3yz75zg3jrXz/ikJkx9floi9ai3RG9iIivdze2rS5KiTn+ya4DLoxM0rFw8iT7N7eJjW6p46UQ3\n5/uGnQ5nQVrapy7EOtn64HoSpUe9JXoTMztqPZzpGcTXm9jDLK80MqtOjRWl5urjDVUEFZ5p7HA6\nlAVp9QVYXpiVkP2JEqVHvSV6EzPT3SxfSvCz+iavn9K8TKqWONPa1inVJTnctaaEpxp9Cdc/PRKt\nvkDCXkRPlB71luhNzKwqzaV6SQ4vJ3idvinUyMyp1rZOenBrNR3+Yfadvuh0KPPSMzBKh384YWbE\nhpMIPeot0ZuYERF21Hp49dRFXj2VmGf13QMjtPcOLaoLsTPtri+nKCedJ5P0omyrb6pjZaKe0UNi\n9Ki3RG9i6vPvWcOq0lwefvwAT7zZ7nQ479A8XZ9fRBdiZ8pKd/PApgpeeOsCvYNjTocTsVafH7dL\nuGV54i7kngg96i3Rm5gqL8ji6c/dwbvWlPKHzx7mL54/llD14Cavn4w0F+uWp24js9ns2VrN+KTy\nbHPyXZRtaQ9Qtyyf7IzEXsjd6R71syZ6EakVkdYZt34R+aKI/KWIHBeRQyLynIiE/ewkIm0icjh0\nbGP0fwWT6PKz0vmnzzTw6TtW8NgrZ/jcd5oSZmGSRq+f9RWFZKYldqKIpdql+WyqLuKpA76kanQ2\nGVQOdfQl3Pj5cJzuUT9rolfVE6q6UVU3AluAIeA5YC9wi6quB04Cf3iDh3lP6DEaohG0ST5pbhd/\n+uFb+MqH6nnxWBcP/sPrdPU728J1ZHySI519i7ZsM9OerVWc6r5Mc3vA6VDm7HTPZS6PTiTcjNhw\nnO5RH2npZidwWlW9qvqCqk6flr0OVEY3NJOKHrlrFf/46QZO91zmI3+3j6PnnOugeKSzj/FJZUt1\n4ieKWPvg+uXkZrh56kDiXUe5nkSeKBWOkz3qI030e4Anwmz/deBH1zlGgRdEpElEHo3w+UwK2llX\nztOfuwOAj319Pz877sxZTuP0hdhFOuJmptzMND60YTn/cfA8AyPjToczJ62+AAVZaawqyXU6lDlx\nskf9nBO9iGQA9wNPX7P9j4EJ4LvXOfRdqroZuA/4vIi8+zqP/6iINIpIY09PYg7FM9Gzbnkh3//8\nXazy5PLZbzXy+L6zcY+hyetnVWkuJXmZcX/uRPTg1iqGxyf54aHzTocyJy3tATZUFeFKoI6VN+Jk\nj/pIzujvA5pV9crpl4g8DHwQ+KRe5yqOqnaGvnYzVdvfdp39HlPVBlVt8Hg8EYRlklV5QRbf+607\n2FlXzv/3H0f5yr8fYSJObXNVlWavn81WtrliY1URteX5PJmAw2CvNTg6wcmuATYl0d/PyR71kST6\nh5hRthGR9wFfAu5X1bDNTEQkV0Typ78HdgNH5h+uSTU5GWl8/VNb+M27V/Gt17x89l8a41I6aLs0\nxKXBsUXVyGw2IsKDW6s42NHn6LWTuTjUMbVQzKYkGHEzk1M96ueU6ENJ+l7g2Rmb/xbIB/aGhk5+\nPbTvchF5PrRPOfALETkIvAn8p6r+OGrRm5Tgdgl//IF6/uKBW3n11EU+9vXX6AzEtqNik9Xnw3pg\nUwUZbhffa0zsmbLTM2I3JFmid6pH/ZwSvaoOqmqJqvbN2LZGVaumh16q6udC28+p6vtD359R1Q2h\n2zpV/fPY/BomFXzitmr++ZGtdPqH+fDf7uOgL3ZD/Zq8vRRkpbHGkxez50hGxbkZvPeWpTzb3JHQ\nq4O1+vysLMlhSW6G06FExKke9TYz1iSUu9d6ePZ37iQr3cWDj73Gj4/E5sJgk9fP5hXFSXMhL54e\n2lpF/8gEP0mQha2vpTq1UEwyTJQKx4ke9ZboTcJZW57P9z9/F3XLCvjcd5r5+sunozpjs29onJNd\nl238/HXcvrqE6iU5PPlmYpZvzveN0D0wmrSJ3oke9ZboTUIqzcvkid+8nQ+uX8ZXf3ScP/i3w1Fb\nyLrZt7gbmc3G5Zq6KPvamUu0XRx0Opx3mK7PJ9OIm5my0t28u6Y0rj3qLdGbhJWV7uZ/79nEf7ln\nDU81+vjMN9+kb2jhI3KavVMdD5P1jDAefnVLJS4hIS/KtrRPNaKrW5a8jeh21y+Na496S/Qmoblc\nwu/vruWvP7aBA229/Mrf78N7aWFnmY1tfuqXFZCTkRalKFNPeUEW99xcxtNNHXGb2zBXrb4A65YX\nkJGWvOkr3j3qk/eVMovKR7dU8p3fuI1Lg2M88H/309jWO6/HmZgM0uoL2LDKOXhwazU9A6P8PM5j\nvm9kfDLI4c6+hF5Rai6KczPYtjJ+Peot0ZukcdvqEp77nbsozE7nE//4Bv/e2hnxYxw7P8Dw+CSb\nLdHP6j21HsryMxNqpuyJCwOMjAcTekWpudq9Ln496i3Rm6SyqjSXZ3/7TjZWF/G7T7bytRdPRjQi\np8k79UmgwRL9rNLcLn51SyU/P9HNhT5nW0pPa5m+EJsC11furS8H4tOj3hK9STrFuRl8+ze28dHN\nlXztxVP83lOtjE7MbXJPU3uAZYVZLC/KjnGUqeHjDVUEFZ5pSoyLsq3tAUrzMqgsTv6/X2VxDuuW\nx6dHvSV6k5Qy09z81cfW89921/D91nN86htvzGnN06a2XqvPR2BlaS53rC7hqUZfQiwB2eLzs7Gq\nCJHUmOgWrx71luhN0hIRvnDPWv7PQ5s42NHHA/93H6dvUO88FxjmXN+IJfoI7dlWha93mNfOxG8m\nZzh9Q+Oc6RlMqWGx8epRb4neJL0PbVjOE795O5dHJnjg7/ax//TFsPtZI7P5ee+6pRRmp/PkAWfL\nNwc7knuiVDg3L82nakl2zHvUW6I3KWHLimK+//m7KC/I4tP/9GbYiT5NXj/Z6e6knmjjhKx0Nw9s\nquAnRy7gn0N5LFZa2gOIwPrKQsdiiLapHvVLY96j3hK9SRlVS3J45rfv5I6bSvjSM4f47z8+flVd\nubndz4aqQtLd9s8+Ug9urWJsMshzLZEPaY2WVp+fNZ488rPSHYshFnbXl8e8R739izcppTA7nW8+\nvJWHtlXz9y+d5gtPNDM8NsnQ2ARvneunYcUSp0NMSnXLCthQVcRTB3xRbTA3V6pKqy+QNAuBRyIe\nPeot0ZuUk+528RcP3MKffKCOHx25wJ5/fJ0Xj3UzGVSrzy/Anq1VnOgauDKWPZ68l4bwD42zMcln\nxIaT5nax8+bY9qi3RG9Skojw2btX8/VPbeHkhQF+98kWgJQ8I4yXD21YTk6Gm6ccaF883bEylUbc\nzLR7XWx71M+a6EWkNrRU4PStX0S+KCJ/KSLHReSQiDwnImH/AiLyPhE5ISJvi8gfRP9XMOb63rtu\nKd/7rTvw5GVya0UhRTnJtSJRIsnLTOOD65fxH4fOxX1x61ZfgOx0NzXlqbki2N0x7lE/a6JX1RPT\nywUCW4Ah4DlgL3CLqq4HTgJ/eO2xIuIG/g64D6gHHhKR+ijGb8ysbq0s5Gf/bQff+vVtToeS9B7c\nWs3Q2CQ/PHgurs/b0u5nfWUhaSl6IT3WPeojfdV2AqdV1auqL6jq9Nv660BlmP23AW+H1o4dA54E\nPjz/cI2Zn7zMtKRbXzQRba4uYm1ZXlzH1I+MT3L0fH9KNDK7kd9412q+/KF6gjG42B1pot8DPBFm\n+68DPwqzvQKY+S+iI7TNGJOERIQ926pp9QU4fqE/Ls959Hw/45OaEo3MbmTbqiW8/9ZlMfnUMudH\nFJEM4H7g6Wu2/zEwAXx3IYGIyKMi0igijT09idP/2hhztQc2VZDhdvFUnM7qW9pTb0ZsvEXy1nEf\n0KyqV64WiMjDwAeBT2r4wbWdQNWMnytD295BVR9T1QZVbfB4PBGEZYyJpyW5GexeV85zLZ2MjM+t\na+hCtPqmOo6WF2TF/LlSVSSJ/iFmlG1E5H3Al4D7VXXoOsccANaKyKrQJ4I9wA/mG6wxJjHs2VpN\nYGg8Li12W0MdK838zSnRi0gucC/w7IzNfwvkA3tDwy6/Htp3uYg8DxC6WPsF4CfAMeB7qvpWFOM3\nxjjgzptKqCzOjvnqUxcvj+LrHbb5Dws0p9WRVXUQKLlm25rr7HsOeP+Mn58Hnl9AjMaYBONyCQ82\nVPHXe0/ivTTIipLcmDxPa/v0RCmrzy9Eag5KNcbE3K82VOISwnYKjZZWXwC3S7i1InU6VjrBEr0x\nZl6WFWazo7aMpxs7mJiMTY+WVl+Am5fmk53hjsnjLxaW6I0x8/bg1iq6B0Z5KQYtdoNB5aAvYBdi\no8ASvTFm3u65uYzSvMyYzJQ93XOZgdEJS/RRYIneGDNv6W4XH2uo5Ocnuunqj+4C19PtkG2i1MJZ\nojfGLMjHG6qYDCrPNHVE9XFb2gPkZ6WxujQ2I3oWE0v0xpgFWVWay+2rl/C9Rl9UOy+2hurzLpdE\n7TEXK0v0xpgF27O1Gu+lIV6P0sIZQ2MTnLjQn/KNzOLFEr0xZsHed8tSCrLSeDJKq08d6ugjqKR8\na+J4sURvjFmwrHQ3D2yq4MdHLuAfHFvw400vHbih0hJ9NFiiN8ZExYNbqxmbDPL91rANaiPS2h5g\nRUkOJXmZUYjMWKI3xkRF/fIC1lcW8uSbPsJ3LZ+7FutYGVWW6I0xUfPg1ipOdA1wsKNv3o9xvm+Y\nrv5RS/RRZIneGBM1929YTna6m6cOzL99cautKBV1luiNMVGTn5XOB9cv4wet5xgcnZjXY7T4AmS4\nXdQty49ydIuXJXpjTFTt2VbF4Ngk/3no/LyOb20PUL+8gMw061gZLZbojTFRtbm6mDVleTw5j/LN\nxGSQQ50BW1EqymZN9CJSG1oqcPrWLyJfFJGPichbIhIUkYYbHN8mIodDxzZGN3xjTKIREfZsraK5\nPcDJroGIjj1+YYCR8aBdiI2yWRO9qp5Q1Y2quhHYAgwBzwFHgF8BXpnD87wn9BjXfUMwxqSOBzZV\nkO6WiGfKTk+U2mRLB0ZVpKWbncBpVfWq6jFVPRGLoIwxya0kL5Pd9Ut5tqWD0YnJOR/X6gtQkptB\n1ZLsGEa3+ESa6PcAT0R4jAIviEiTiDwa4bHGmCT14NYqAkPjvPBW15yPaWmfmiglYh0ro2nOiV5E\nMoD7gacjfI53qepm4D7g8yLy7us8/qMi0igijT090V+WzBgTX+9aU0pFUTZPzXH1qb7hcU73DFp9\nPgYiOaO/D2hW1bm/PQOq2hn62s1UbX/bdfZ7TFUbVLXB4/FE8hTGmATkcgkfb6jiF29fxNc7NOv+\nhzpsolSsRJLoHyLCso2I5IpI/vT3wG6mLuIaYxaBjzVU4hL4XuPsZ/Ut7QFEYH1VYRwiW1zmlOhD\nSfpe4NkZ2x4QkQ7gDuA/ReQnoe3LReT50G7lwC9E5CDwJvCfqvrjaP4CxpjEtbwom+01Hp5u7GBi\nMnjDfVt9AW7y5FGQlR6n6BaPOSV6VR1U1RJV7Zux7TlVrVTVTFUtV9X3hrafU9X3h74/o6obQrd1\nqvrnsfk1jDGJ6sGt1VzoH+GVU9e/9qaqtPoCtqJUjNjMWGNMTO2sK6M0L+OGY+p9vcP0Do7ZilIx\nYoneGBNT6W4XH91SyU+Pd9PdPxJ2nxafH8BG3MSIJXpjTMw92FDFZFB5prkj7P0t7QGy093UllvH\nyliwRG+MibnVnjy2rVrCUwfCrz7V6gtwa2UhaW5LSbFgr6oxJi72bK3Ce2mI18/0XrV9dGKSo+f6\n7UJsDFmiN8bExX23LCM/K+0dq08dPdfP2KR1rIwlS/TGmLjIznDzkY0VPH/kAn1D41e2X+lYaTNi\nY8YSvTEmbvZsq2JsIsj3WzuvbGtpD7C0IIulhVkORpbaLNEbY+Jm3fJCbq0o5Ik3269clG31Baxs\nE2OW6I0xcfXg1iqOXxjgcGcfly6P0t47ZEsHxpglemNMXN2/cTlZ6S6eeNN3pT5vZ/SxleZ0AMaY\nxaUgK50P3LqcH7R2kpPhxu0Sbq20jpWxZGf0xpi427OtisGxSb79upfa8nxyMuycM5Ys0Rtj4q5h\nRTGrPbmMTQStkVkcWKI3xsSdiLBnaxVg9fl4sM9LxhhH7NlWzYW+Ud5bv9TpUFKeJXpjjCMKstL5\n8ofqnQ5jUbDSjTHGpLhZE72I1IpI64xbv4h8UUQ+JiJviUhQRBpucPz7ROSEiLwtIn8Q3fCNMcbM\nZtbSjaqeADYCiIgb6ASeA3KAXwH+4XrHhvb/O6YWFu8ADojID1T16MJDN8YYMxeR1uh3AqdV1Tu9\nQURutP824G1VPRPa90ngw4AlemOMiZNIa/R7gCci2L8CmLkicEdomzHGmDiZc6IXkQzgfuDpWAQi\nIo+KSKOINPb09MTiKYwxZlGK5Iz+PqBZVbsiOKYTqJrxc2Vo2zuo6mOq2qCqDR6PJ4KnMMYYcyOR\nJPqHiKxsA3AAWCsiq0KfCPYAP4jwMYwxxiyAhFuR/R07ieQC7cBqVe0LbXsA+D+ABwgArar6XhFZ\nDnxDVd8f2u/9wNcAN/BNVf3zOTxfD+Cdbb/rKAUuzvPYVGOvxdXs9biavR6/lAqvxQpVDVsOmVOi\nTyYi0qiq1x3Xv5jYa3E1ez2uZq/HL6X6a2EzY40xJsVZojfGmBSXion+MacDSCD2WlzNXo+r2evx\nSyn9WqRcjd4YY8zVUvGM3hhjzAwpk+itS+YviUiViPxcRI6GOoz+rtMxOU1E3CLSIiI/dDoWp4lI\nkYg8IyLHReSYiNzhdExOEpHfC/0/OSIiT4hIltMxRVtKJPoZXTLvA+qBh0RkMa9oMAH8vqrWA7cD\nn1/krwfA7wLHnA4iQfwv4MeqejOwgUX8uohIBfBfgQZVvYWp+T57nI0q+lIi0TOjS6aqjgHTXTIX\nJVU9r6rNoe8HmPqPvGibyYlIJfAB4BtOx+I0ESkE3g38E4CqjqlqwNmoHJcGZItIGlPt1885HE/U\npUqity6Z1yEiK4FNwBvORuKorwFfAoJOB5IAVgE9wOOhUtY3QjPfFyVV7QT+iqmZ/+eBPlV9wdmo\noi9VEr0JQ0TygH8Dvqiq/U7H4wQR+SDQrapNTseSINKAzcDfq+omYBBYtNe0RKSYqU//q4DlQK6I\nfMrZqKIvVRL9nLtkLhYiks5Ukv+uqj7rdDwOugu4X0TamCrp3SMi33E2JEd1AB2qOv0J7xmmEv9i\ntQs4q6o9qjoOPAvc6XBMUZcqid66ZM4gU8t+/RNwTFX/p9PxOElV/1BVK1V1JVP/Ln6mqil3xjZX\nqnoB8IlIbWjTThb3im/twO0ikhP6f7OTFLw4HelSgglJVSdE5AvAT/hll8y3HA7LSXcBvwYcFpHW\n0LY/UtXnHYzJJI7/Anw3dFJ0BnjE4Xgco6pviMgzQDNTo9VaSMFZsjYz1hhjUlyqlG6MMcZchyV6\nY4xJcZbojTEmxVmiN8aYFGeJ3hhjUpwlemOMSXGW6I0xJsVZojfGmBT3/wO8Z5L+heGmhwAAAABJ\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbT4LuW7lByP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d33211a1-632e-4142-e83c-30a612373a8e"
      },
      "source": [
        "# DROPOUT increased from 0 to 0.3\n",
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len + 1, output_dim=128)) \n",
        "  model.add(GRU(256, dropout=0.3)) \n",
        "  model.add(Dense(5, activation=\"softmax\"))  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68701 to 0.70266, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70266 to 0.72222, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72222 to 0.74257, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.74257 to 0.74883, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74883\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74883\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.88%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69405, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69405 to 0.71283, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71283 to 0.71596, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.71596 to 0.73083, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73083\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73083\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73083\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.08%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68388, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68388 to 0.71283, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71283 to 0.73552, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.73552\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.73552 to 0.73944, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.73944 to 0.74178, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74178\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.18%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68779, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68779 to 0.71831, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71831 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.72770 to 0.73239, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.73239\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.73239\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 73.24%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68466, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68466 to 0.70579, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.70579 to 0.71283, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.71283 to 0.72066, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72066\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72066\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72066\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.07%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67919, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67919 to 0.73944, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.73944 to 0.74022, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.74022 to 0.74413, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.74413 to 0.75430, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75430\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75430\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 75.43%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68779, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.68779 to 0.71753, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.71753 to 0.72379, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.72379\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.72379\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.72379\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.72379 to 0.72692, saving model to weights.best.hdf5\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.69%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.65962, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.65962 to 0.69953, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.69953 to 0.70736, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.70736 to 0.71596, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.71596\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.71596\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.71596\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 71.60%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.69797, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.69797 to 0.72770, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.72770 to 0.73787, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.73787 to 0.74570, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.74570\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.74570\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.74570\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 74.57%\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67110, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67110 to 0.68442, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.68442 to 0.69616, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.69616 to 0.71261, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.71261 to 0.71652, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.71652 to 0.72044, saving model to weights.best.hdf5\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.72044\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "accuracy: 72.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPIj5LVjpjaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "772d6c82-a545-450e-dc64-4175173f6636"
      },
      "source": [
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "plt.plot(cvscores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73.38% (+/- 1.26%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXyc5XXo8d+Z0b5rJHnRYkuWjFds\nS9gitgmEAAYTMIH0NnBzc3PTNLQ3oQ20KaXQQNKEhmwl6XLb0jRteksIITUhiVmz3oTNi+Rl5H3T\nMvIi2dKM9vXcP2bGyELbSDPzzrzzfD8ffSy9874zR7J09Oo8z3MeUVUMwzAM+3JYHYBhGIYRWSbR\nG4Zh2JxJ9IZhGDZnEr1hGIbNmURvGIZhc0lWBzCRwsJCLS8vtzoMwzCMuLFnz552VS2a6LGYTPTl\n5eXs3r3b6jAMwzDihog0TvaYKd0YhmHY3LR39CKyDHh2zKElwKNAHvBJoC1w/GFVfXGC608DXcAI\nMKyq6+cYs2EYhhGCaRO9qh4B1gGIiBPwAM8DHweeVNWvz+B1rlfV9rkEahiGYcxOqKWbG4ATqjpp\nLcgwDMOILaEm+ruBZ8Z8fJ+I7BeR74hI/iTXKPCqiOwRkXtnFaVhGIYxazNO9CKSAmwDngsc+keg\nEn9Z5wzwjUkuvUZVa4CtwKdF5NpJnv9eEdktIrvb2tomOsUwDMOYhVDu6LcCdap6DkBVz6nqiKqO\nAv8C1E50kap6Av+ex1/bn+y8p1R1vaquLyqacCqoYRiGMQuhJPp7GFO2EZGFYx67E3CPv0BEMkUk\nO/g+sGWi8wwjUfz2WDtHz3VZHYaRYGaU6ANJ+iZg+5jDXxWRAyKyH7geeCBwbrGIBKdZzgd+KyL7\ngJ3ADlV9OWzRG0YcUVXue6aOx3ccsjoUI8HMaGWsqvYABeOOfXSSc1uBWwPvnwTWzjFGw7CFlo4+\nOnuH2NPYwcio4nSI1SEZCcKsjDWMKGlo9QLQPTDMoTM+i6MxEolJ9IYRJW6PDwncxO88ddHaYIyE\nYhK9YUSJu9XLsvnZlOans+u0SfRG9MRk90rDsBtVxe3xct0V81CUXx9pQ1URMXV6I/LMHb1hRMH5\nrgHauwdZXZJDbbmLCz2DnGzvsTosI0GYO3rDiAK3xz8Qu7okF1dmCgC7Tl2ksijLyrCMBGHu6A0j\nCoIDsSsW5rCkMJPCrBQzIGtEjbmjN4wocLd6qSjMJCvV/yO3odzFTjMga0SJuaM3jCho8HhZXZx7\n6eMN5S5aOvpo7eyzMCojUZhEbxgRdrFnkFZvP6tLci4dq61wAZhplkZUmERvGBEWXBE79o5+xcIc\nslOTTJ3eiAqT6A0jwtwef7uDVWMSvdMh1CzON3f0RlSYRG8YEeZu9VLmSic3I/my47UVLo6e66aj\nZ9CiyIxEYRK9YUTY+IHYIFOnN6LFJHrDiCBf/xCnL/SyqjjnXY+tKc0lJclhEr0RcSbRG0YEHWwN\n1OdL3n1Hn5rkZF1ZnhmQNSLOJHrDiKBLrQ8mKN0A1Ja7cLf66BkYjmZYRoKZNtGLyDIR2TvmzSci\n94vI50XEM+b4rZNcf4uIHBGR4yLyUPg/BcOIXQ2tPubnpFKUnTrh4xsqXIyMKvVNnVGOzEgk0yZ6\nVT2iqutUdR1wFdALPB94+MngY6r64vhrRcQJ/AOwFVgJ3CMiK8MXvmHENvckA7FBNYvycAjsPHUh\nilEZiSbU0s0NwAlVbZzh+bXAcVU9qaqDwPeBO0J8TcOIS32DI5xo656wPh+UnZbMquJc0/fGiKhQ\nE/3dwDNjPr5PRPaLyHdEJH+C80uA5jEftwSOvYuI3Csiu0Vkd1tbW4hhGUbsOXTWx6jC6glm3Iy1\nodxFfVMng8OjUYrMSDQzTvQikgJsA54LHPpHoBJYB5wBvjGXQFT1KVVdr6rri4qK5vJUhhETGsb0\noJ9KbUU+A8OjHPCYOr0RGaHc0W8F6lT1HICqnlPVEVUdBf4Ff5lmPA9QNubj0sAxw7A9t8eHKzOF\nhblpU563ody/cGrnqY5ohGUkoFAS/T2MKduIyMIxj90JuCe4ZhewVEQqAn8R3A38eDaBGka8cbd6\nWVWcM+2+sAVZqVQWZZqFU0bEzCjRi0gmcBOwfczhr4rIARHZD1wPPBA4t1hEXgRQ1WHgPuAV4BDw\nA1VtCGP8hhGTBoZHOHqua9qyTVBthYtdpy8yMqoRjsxIRDPaYUpVe4CCccc+Osm5rcCtYz5+EXjX\n1MtwU1X2tXjJSUtiidmH07DYsXPdDI3olFMrx6qtcPHMzmaOnO1i5TSDt4YRKtusjO0bGuGep97i\nX35zyupQDOPSitiJetxMJFinN+UbIxJsk+gzUpK4ZfUCfrq/lf6hEavDMRKcu9VLdmoSi1wZMzq/\nND+D4tw0M5/eiAjbJHqAO6tL6Oof5heHz1sdipHg3B4fK4tzcDimHogdq7bCxc5TF1E1dXojvGyV\n6DdXFTIvO5XtdWYGp2Gd4ZFRDp3xzXggNmhDhYu2rgEaL/RGKDIjUdkq0TsdwgerS/jVkfNc6B6w\nOhwjQZ1s72FgePSyzcBnojY4n96Ub4wws1WiB3/5ZnhU+en+M1aHYiSo6VoTT6ZqXhauzBTTn94I\nO9sl+hULc1i+IJvt9aZ8Y1jD7fGRluwIeZqviLDebBhuRIDtEj3Ah2pK2dfcyYm2bqtDMRKQu9XL\nyoU5OEMYiA2qrXDReKGXc77+CERmJCpbJvo71hXjEHjeDMoaUTY6qhxsDX0gNuidvjfmrt4IH1sm\n+nk5aWyuKuT5eg+jZkm5EUWNF3vpHhgOuT4ftKo4h4wUpynfGGFly0QP/vKNp7PP/MAYUXVpRWyI\nM26CkpwOrlqcb+7ojbCybaLfsmo+GSlOM6feiCp3q5dkp7B0Xvasn2NDuYsj57rw9g6FMTIjkdk2\n0QdbIrx44IxpiWBETYPHx7IF2aQkzf5Hq7bChSrsbjR39UZ42DbRg7980zUwzM8OnbM6FCMBqCru\n1qk3A5+JdWV5JDvFLJwywsbWif49SwpYkJNmyjdGVHg6++jsHZpyM/CZSEt2sqY0z9TpjbCxdaJ3\nOoQ7qov59dE22k1LBCPCGlp9wPSbgc9EbYWLAy1e+gZN2THSVNX2X2dbJ3qAu6pLGRlVfrKv1epQ\nDJtr8HhxOoQVC8OQ6MtdDI8q9c1mH9lI+/G+VtZ/6TVb98eaNtGLyDIR2TvmzSci9495/E9FREWk\ncJLrR8ZcG/X9YpctyGZVcQ7Pm5YIRoS5W31UFWWRluyc83PVLM5HxCycioZfH22jZ3CE109csDqU\niJk20avqEVVdp6rrgKuAXuB5ABEpA7YATVM8RV/welXdFo6gQ3VndQn7W7wcP99lxcsbCcLt8c56\n/vx4uenJrFiQY9aBRMHepk4A3jzRbnEkkRNq6eYG4ISqNgY+fhJ4EIjp5afbAi0RzKCsESnnff2c\n7xqY84ybsWorXNQ1djI0Mhq25zQu19EzyMn2HgBeP57Ad/Tj3A08AyAidwAeVd03zTVpIrJbRN4S\nkQ9OdpKI3Bs4b3dbW1uIYU1tXnYa115RxI9MSwQjQi4NxM5xxs1YG8pd9A2NXHpuI/z2Nvvv5m9e\nNZ+mi700X7Tnpi8zTvQikgJsA54TkQzgYeDRGVy6WFXXA/8d+KaIVE50kqo+parrVXV9UVHRTMOa\nsTurS2j19vPWKfv+1jasE2x9sDIMM26CNlTkA7DTfM9GTF1TBw6BP7zOn5betGmdPpQ7+q1Anaqe\nAyqBCmCfiJwGSoE6EVkw/iJV9QT+PQn8CqieY8yzsmXlArJSk0xHSyMi3K1eKgozyUpNCttzzstO\no6Iwk52nzMybSKlv6mT5ghzWleVRmJXCGzat04eS6O8hULZR1QOqOk9Vy1W1HGgBalT17NgLRCRf\nRFID7xcCm4GDYYk8ROkpTrauXsBL7rO2nzNrRJ/b42NVGO/mgzaU57O78aIpOUbAyKiyt7mT6kV5\niAgbKwt5/cQFW27OPqNELyKZwE3A9hmcu15Evh34cAWwW0T2Ab8EnlBVSxI9wJ01JXQPDPPqwbPT\nn2wYM9TRM4insy+s9fmgDeUuOnuHOHbebKITbsfPd9M9MEzNIn+JbHNlAW1dA7bcsGhGiV5Ve1S1\nQFW9kzxerqrtgfd3q+rvB95/Q1WvVNW1gX//NXyhh+49FQUU56aZOfVGWL2zIjb8if7qigLAbBge\nCfVN/pJY9aI8ADZV+pcC2XH2je1Xxo7lcAh3VJfwm2PtnO8yW7UZ4dHQGuhBH4HSTZkrnfk5qewy\nC6fCrq6pg7yMZCoKMwFYVJBBaX66Lev0CZXoAe6qLmFkVPnxXtMSwQgPd6uPkrx08jNTwv7cIsKG\nchc7T120Ze3YSvVNnVSX+evzQZsqC3jzxAVGbDYmknCJfun8bK4syTXlGyNsGjxeVodpRexErq5w\ncdbXT0tHX8ReI9F4+/zjHsH6fNDmqkJ8/cMctNnahYRL9OCfU9/Q6uPoOdMSwZibrv4hTrb3RKQ+\nH7ShwmwYHm77Agulqscl+o1L/GMir9usfJOQiX7bumKcDjEtEYw5O3TGf7MQiRk3QVfMyyY3Pdkk\n+jCqa+pABNaWXf7/Ni8njaXzsnjDZgunEjLRF2alcl2gJYLdanFGdM11M/CZcDiEDeX5psFZGNU3\ndXLFvGyy05Lf9dimygJ2nbrI4LB9egwlZKIHf/nmrK+ft07a6ze3EV3uVi/zslOZl50W0dfZUO7i\nZHsPbV327ZkeLaOjSn1TBzWL8yZ8fGNlIX1DI5f64NhBwib6m1bOJzs1yZRvjDlp8PgiWrYJCtbp\nzV393J1s78HXP0x1Wf6Ej29cUoAIvH7cPnX6hE30aclObr1yIS+5z9A7OGx1OEYc6hsc4dj5rojM\nnx9vdXEu6clOU6cPg7rAQqnJ7uhzM5JZXZxrqwZnCZvowd8SoXdwhFcbzlkdihGHDp/1MaqwKoIz\nboJSkhxUL8ozd/RhUN/USU5aEksKsyY9Z1NVAfXNHba5CUzoRF9b7qIkL53tNpxT/+aJC2x58tcc\nOWumkEaK+1IP+sjf0YO/Tn/wjA9f/1BUXs+u6ps6WLcoH4dDJj1nU2UhQyPKrtP26Bya0Ine4RDu\nrC7ht8faOO+zT0uEgeERHn7+AEfPdfOZ79czMGy6dUbCwVYveRnJlOSlR+X1aitcqMKeRnskHyt0\nDwxz5FwX1WUTl22CNpTnk+wU27RDSOhED/7yzajCCzZqifDt35ziVHsPn7imgsNnu/ibV49aHZIt\nuT0+VhfnXraEPpKqF+WR5BDT92YO9jV3ourffH0qGSlJVJfl84ZNGpwlfKKvLMpibWmubco3ns4+\n/v4Xx7l51Xw+d9tK7qldxFO/OWmmkYbZ4PAoR852RXT+/HgZKUmsLsk1A7JzEOxYua506jt68Nfp\n3a1evL3xXypL+EQPcFdNKYfO+Dh0Jv77Wzy+4yCK8rnbVgLwlx9YwWJXBn/6g32mthtGx853MTgy\nGtHWBxOprXCxv8VL/5Apx81GfVMnlUWZ5Ga8e6HUeJsqC1GFN21wk2QSPXD72mKSHBL3jc5+c6yN\nFw+c5b7rqyjNzwAgMzWJJz+8jrO+fj7/QoPFEdpHgyf8m4HPRG25i8GR0Uu9WoyZU1Xqmzvf1chs\nMuvK8khPdvKmDer0JtEDrswU3resiBf2xm9LhMHhUR77cQPlBRl88tollz1WvSif+66vYnu9hx37\nz1gUob24W71kpSax2JUR1dddXx7cMNyUb0LVeKGXiz2D72pkNpmUJAcbKly8boP59NMmehFZJiJ7\nx7z5ROT+MY//qYhoYE/Yia7/mIgcC7x9LJzBh9NdNaWc8w3E7Sj7v/72FCfbenhs2ypSk5zvevy+\n91extiyPh58/wFmvfWYYWcXt8bKyOGfKKXqRkJeRwrL52WbHqVmYbqHURDZXFnD8fHfcz8qbNtGr\n6hFVXaeq64CrgF7geQARKQO2AE0TXSsiLuAx4GqgFnhMRGb26zTK3r98Htlp8dkSobWzj7/9+TG2\nrJzP9cvmTXhOstPBk7+7lsHhUf7sh/vMZtNzMDKqHDzji3p9Pqi2wkVdYwfDI/ZpuhUN9U2dZKUm\nsXRe9oyvCW4vGO/dLEMt3dwAnFDVxsDHTwIPApNljZuB11T1oqp2AK8Bt8wq0ghLS3Zy25qFvOw+\nS89AfK2Ge3zHIUb1nQHYySwpyuKRD6zgN8fa+e6bp6MSmx2dbOumf2g0Kq0PJrKhwkXP4MilFsnG\nzNQ1dbC2LBdnCH+FrSzOITc9OW7/0g8KNdHfDTwDICJ3AB5V3TfF+SVA85iPWwLH3kVE7hWR3SKy\nu62tLcSwwuOumlL6hkZ4peGsJa8/G7891s6OA2f49PVVlM2gXvyRqxfx/uXzeOKlwxwzG6/Mijuw\nR2y0B2KDasv9Dc7ePhXfd5nR1Ds4zOGzXZM2MpuM0yG8Z4mL149fiOutHGec6EUkBdgGPCciGcDD\nwKPhCkRVn1LV9aq6vqioKFxPG5L1i/Mpc6XHTfnGPwDrZnFBBveOG4CdjIjwxIeuJDM1ifuf3Wur\nntvR0uDxkZrkoLIo05LXX5CbxiJXhul7E4L9LV5GRjWk+nzQ5qpCPJ19NF+M360cQ7mj3wrUqeo5\noBKoAPaJyGmgFKgTkQXjrvEAZWM+Lg0ci0kiwp3rSnj9RHtcDFh+5/VTnGjr4fO3ryIt+d0DsJOZ\nl53GE3ddSUOrj2/+zKyaDZW71cuKhTkkOa2btLah3MWu0x1xfZcZTfVN/umo60K8owf/RiQQ39sL\nhvKdeg+Bso2qHlDVeaparqrl+EsyNao6vubxCrBFRPIDg7BbAsdi1p01pajCC3tj9vcRAGe8/gHY\nG1fM5/rlEw/ATmXLqgV8eH0Z//TrE+bOMASjoxroQW9NfT6otiKfiz2DnGjrtjSOeFHX1EFFYSau\nzJSQr60symJedmpcD8jOKNGLSCZwE7B9BueuF5FvA6jqReCLwK7A218FjsWsisJMqhflsb3OE9N3\nS4/vOMTIqPLY7VMPwE7lc7evpDQ/gwee3UuXWTU7I80dvXQNDFs24yaotsJ/l7nzlGlwNh1Vpb6p\nc9pGZpMRETZVFvDmifaYzglTmVGiV9UeVS1QVe8kj5eranvg/d2q+vtjHvuOqlYF3v4tPGFH1l3V\nJRw518XBGG2J8Prxdn66/wyfet/MBmAnk5WaxJMfXktrZx9f+MnBMEZoX26LVsSOV16QQWFWqvlr\nbAZaOvpo7x6geppGZlPZVFVIe/cgR8/F519QZmXsBG5bU0yyU3g+BgdlgytgF7ky+IPrZjYAO5Wr\nFrv41Puq+OGeFl52m1Wz03G3ekl2CkvnT75pRTSICLUV+WaF7AwEF0rN9o4extTp43R7QZPoJ5Cf\nmcL1y+bxwr7WmFuU8u9vnOL4+W4eu31lSAOwU/nMjUu5siSXv9h+IO5XAEaa2+PlivnZE64+jrYN\n5S48nX14OuN3Nkg01Dd1kp7sZPmCmS+UGq80P4PFBRlxW6c3iX4Sd9WU0NY1wG9j6Df4WW8/3/rZ\nMW5cMY8bVswP2/MmOx08+eF19A2N8OB/7Y/bOmSkqSoNrdatiB2vNrhhuLmrn1J9UwdrSnPnPEtq\nU2UBb5+8EHM3fzNhEv0krl8+j9z05JjqaPn4i4cYGlUevW1V2J+7al4WD9+6gl8daeM/32qc/oIE\ndMbbz8WeQctn3AQtX5BDdmoSb5tEP6n+oREaWn3TbjQyE5sqC+kaGL60hWQ8MYl+EqlJ/pYIrzSc\npTsGWiK8caKdn+xr5X9fV8migsh0TPzoexZz3RVFPP7iITNtbwJuj38uwiqLB2KDnA7hqvJ8MyA7\nBbfHy/Cozqk+H7Qxjuv0JtFP4a6aEvqHRnnpgLWDlEMjozz2QgNlrnT+9/sqI/Y6IsLXfmcN6clO\nHnh2L0Nx+CdqJLlbfTgEViyIjTt68Jdvjp/v5kL3gNWhxKTgQqmZtiaeSmFWKssXZPNmHNbpTaKf\nQs2ifBYXZFhevvn3109z7Hw3j90W2grY2ZiXk8aX77qS/S1e/vbnxyL6WvHmYKuXyqIs0lOsH4gN\nCva92XXazKefSF1TB2WudIqyU8PyfBsrC9h1+iIDw/G1w5dJ9FMQEe6sLuHNkxdotWhmwzlfP9/8\n2VHev3weN64M3wDsVG5ZvZAP1ZTyD788zp5GUxYIcnt8ls+fH+/K0lxSkhymfDMJ/0Kp8HVG31xZ\nyMDwKHWN8bXDl0n007izugRV+JFFLRH+OjAAO5cVsLPx+W0rKc5L54Fn98Vd2+ZIaOsa4Kyv37LW\nxJNJTXJSXZZnEv0EWjv7OOvrp2bR3OvzQbVLXDiEuNte0CT6aSwuyOSqxfk8b0FLhLdOXuCFva38\n4XWVLC6IbqfE7LRk/uZ319Hc0csXf2pWzTZY3Jp4KrUVLtweb0xMGogl4azPB+WkJbOmNC/uthc0\niX4G7qwu4dj5bhqiOK1qaGSUR19wU5qfzqciOAA7ldoKF394XSXf39XMawfPWRJDrAj+36+MsTt6\n8C+cGlWoazR1+rHqmjpITXKwYmF4/882VRawr7kzrn6xmkQ/A7etWUiK0xHVPvXffeM0R8918+ht\n4VsBOxsP3HgFq4pzeOi/9tPWlbgzO9weL+UFGeSkJVsdyrvULM7H6RBTvhmnvqmDK0v8YxjhtLmq\nkOFRjauFaibRz0BeRgrvXz6PH+/zRGVV3HlfP9/82THet6yIm6I0ADuZlCQH3/zwOroHhvnzBF41\n6271xsz8+fGyUpNYVZxjFk6NMTA8gtsTnoVS4121OJ+UJEdcbS9oEv0M3VlTQnv3IL85Fvn/3L9+\n8RCDw6N8/vZViMx8f8tIWTo/m4e2LucXh8/zvZ0T7gNva97eIZov9sVM64OJbCh3sbe5M+6m/UXK\nwVYfgyOjYVkoNV5aspOrFuXz+vH4qdObRD9D1y+bR15GMtsjPKf+7ZMX+NHeVv7guiWUF1qzVd1E\nPraxnGuqCvnSTw9xqr3H6nCi6p2B2NirzwfVVrgYHB7lQMuEncQTTl1gIDYSd/Tgr9MfPOOjo2cw\nIs8fbibRz1BKkoPb1xTzasNZfBHapGN4xN+CuCQvnU+9ryoirzFbDofw9f+2lpQkBw88uzcuGzvN\nVnAz8FUxfkcPsNPU6QF/fb44N435OWkRef5NVYUAvHkyPu7qTaIPwZ01JQwMj/LygfE7JobHf7zZ\nyOGzXTx6+8qYWn0ZtCA3jcfvXM3e5k7+/pfHrQ4natweH8W5abPahi5aXJkpVM3LMv3pA+qbOue0\n0ch01pTmkpnijJs6/bSJXkSWicjeMW8+EblfRL4oIvsDx14VkeJJrh8Zc+2Pw/8pRE91WR4VhZls\nr28J+3Of7+rnydeOct0VRWyxeAB2KretKebO6hL+7hfHqW9KjOl8DTE8EDvWhnIXe053MDKamAPm\nQed8/Xg6+yJSnw9KdjqorXDxRpzU6adN9Kp6RFXXqeo64CqgF3ge+Jqqrgkc/ynw6CRP0Re8XlW3\nhS1yCwRbIrx18iItHb1hfe4nXjzMwPAon98WGwOwU/nCHatYkJPGn/xgH72D8TOXeDZ6BoY52d4T\n0wOxQVdXuOgaGObw2fhroxtOwRuQSNXngzZXFXKyvYcz3tjf+CXU0s0NwAlVbVTVsd9NmUBC3Ebc\nWV0CwAt7W8P2nDtPXWR7vYdPXltBRQwNwE4mJy2Zb/zuWk5f6OFLOw5ZHU5EHTrjQzW2B2KDNgQ2\nIkn08k19UycpTkfE21UE2xbHw119qIn+buCZ4Aci8riINAMfYfI7+jQR2S0ib4nIByd7YhG5N3De\n7ra2thDDip4yVwa15S6217WEZU75cGAFbEleOp++PrYGYKfyniUF3PveJXzv7SZ+fsi+q2aDPehj\nsfXBeCV56ZTkpSf8wqm6pg5WleREfLvHFQtyyM9IjovtBWec6EUkBdgGPBc8pqqPqGoZ8DRw3ySX\nLlbV9cB/B74pIhOu51fVp1R1vaquLyoqmvEnYIU7a0o40dbDAc/cp7L937f8A7Cfu20FGSlJYYgu\nev5kyxUsX5DNn//Xftv2Q3e3+ijMSmVemNrcRlpthYudpzoSdmHb0Mgo+1u8Ye1YORmHQ9hYWcAb\nJ9pj/usdyh39VqBOVSe6fXsa+NBEF6mqJ/DvSeBXQHWIMcacW69cSErS3FsitHUN8DevHuW9Swu5\nedWCMEUXPalJTr559zp8fcM8tP1AzH+zz4bb42V1SU7Mj5sEbSh30d49kHBrHYIOnfExMDxKzeLI\nDcSOtamykDPefk5fCO+YXbiFkujv4fKyzdIxj90BHB5/gYjki0hq4P1CYDMQ960Qc9OTuXHFPH6y\nr3VOuzA98dJh+odH+EIcDMBOZvmCHB68ZRmvHTzHD3Y3Wx1OWPUPjXDsfHdcDMQG1Vb472QTtXwT\niY6VU9kUJ9sLzijRi0gmcBOwfczhJ0TELSL7gS3AZwLnrheRbwfOWQHsFpF9wC+BJ1Q17hM9wF3V\npVzoGeT/HZ3deMLu0xf5r7oWPvneJSwpygpzdNH1e5sr2FRZwBd+cpDGC/a5kzxytouRUY2Lgdig\nyqIsXJkp7DyVGFNfx6tv6mB+TirFuZFZKDVeRWEmC3PTYn57wRklelXtUdUCVfWOOfYhVV0dmGJ5\n+5gSzW5V/f3A+2+o6pWqujbw779G5tOIvuuWFeHKTJlV+WZ4ZJTPvdBAcW4a970/fgZgJxNcNZvk\nEFutmo2HFbHjiQgbyvPZeTq2E0+k1AV2lIrWX8gi79TpR2N4/YJZGTtLyU4Ht69ZyGuHzuHtC60l\nwtNvN3HojI+/vG1l3A3ATqY4L50vfnA1dU2d/OOvTlgdTli4PT5y05MpzU+3OpSQbCh30Xyxj7Pe\nfqtDiar27gGaLvZSHcYdpWZic2UhHb1DHD7bFdXXDYVJ9HNwV00pg8OjvHTgzIyvaesa4OuvHuG9\nSwvZujr+BmCncse6Em5fW8y3fn6M/S3xtafmRBpavawqjp+B2KCrK/x140Tre1Mf4UZmk9lUFZhP\nH8PtEEyin4M1pbksKcoMqbtuG8cAAByfSURBVHzzlZcP0z80EhcrYGfjS3espig7lfuf3UvfYPy2\nzB0aGeXw2a64mD8/3oqF2WSmONl5KrHKN/VNHSQ5hCuj/H+2MDedJYWZMT2f3iT6ORAR7qouYefp\nizRfnH561Z7GDn64p4VPXLOEyjgfgJ1MbkYyX/9vaznZ1sOXX4rfVbPHz3czODwac5uBz0SS00HN\n4nx2JdiAbF1TByuLcyzZkW1jZQFvn7wwp1l4kWQS/Rx9MNAS4UfT9KkfGVU+9yM3C3PT+CMbDMBO\nZXNVIZ+4poL/eLORXx05b3U4sxJPK2IncnWFiyPnuujsjY9+6XM1fGmhVHTr80GbqwrpGRxhf4zu\nB2AS/RyV5mdwdYWL7fWeKRcMPf12IwfP+PjLD6wkM9UeA7BT+bObl7FsfjZ/9sP9XIyTzRnGamj1\nkZnipKIg9nsPTSTYn37X6cS4qz9yrovewZGo1+eD3rPEX6d/M0br9CbRh8FdNSWcau9hb/PEA5AX\nugf4+itH2FxVwK1X2msAdjJpyU6e/PA6vL1DPPL8AavDCZnb42VlcQ4OR3yOo6wtyyPF6UiYhVOX\nFkpFofXBRFyZKaxcmBOz2wuaRB8GW69cSGqSg+cnKd985eXD9A7G9wrY2VhZnMP9Ny3lJfdZfj3L\nhWVWGBlVDp7xxdX8+fHSkp2sKc1NmE6WdU0dFGalUOaybirspsoC9jR10D8Ue5MQTKIPg5y0ZG5a\nOZ+f7GtlcPjywZg9jR38YHcLn3hvBVXzsi2K0DqfuKaCRa4M/nrHobjZEONUew+9gyNxW58Pqq1w\n4fZ4bb9nAMDepk7WRXGh1EQ2VxUyODzKnsbYK5eZRB8md9WU0NE7dNng48io8tiP3SzISeOP3790\niqvtKzXJyUNbl3PkXFfc9MKJh83AZ2JDhYvhUb1U1rCrjp5BTrb3RK2R2WQ2VLhIckhMzqc3iT5M\n3ru0iILMlMvKN9/b2YTb4+ORD6xIiAHYyWxdvYD1i/P5xqtH6R6I/btLt8dLapKDqjifAnvV4nxE\n7L8RSXBszKr6fFBWahJry/Jisk5vEn2YJDsdbFtXzM8PncfbO3RpAHZTZQG3rVlodXiWEhEe+cAK\n2rsH+Odfx357BLfHx/KFOSQ54/vHIyctmZULc2yf6OuaOnAIrC2zvtS2qbKA/S2d+PpDa4sSafH9\nnRxj7qouZXBklJ8eaOWrLx+hZ2CYv7ojsQZgJ1O9KJ/b1xbzL785GdN7bKoqDa1eVsfhQqmJbCh3\nUd/c8a6xIzupb+pk+YKcmOgbtamykFGFnSdj65erSfRhtLokh6p5WfyfX57g2d3N/N41iTkAO5kH\nb17GqMLXXjlidSiTaunow9c/HNczbsaqrXDRPzR6qROn3YyMKnubOy2vzwdVL8ojNckRc+0QTKIP\nIxHhrpoSPJ19zM9J5Y9vSMwB2MmUuTL4+OZyttd5Lq08jTXvrIi1zx09wC6blm+On++me2DY8vp8\nUFqykw3lrpgbkDWJPszuqi6lMCuFL2xbRVYCD8BO5tPXV+HKTOFLOw7G5NaD7lYvSQ7hivn2+Eus\nKDuVJYWZtq3T1zf5pzJatSJ2IhsrCzh8tov2GNpHedpELyLLRGTvmDefiNwvIl8Ukf2BY6+KSPEk\n139MRI4F3j4W/k8htizITWP3X97ELasTewB2MjlpyTxw41LeOnmRnx2KvT44bo+PpfOzLWmMFSkb\nyl3sbuyI6Y0xZquuqYP8jGTKCzKsDuWSzVWFADG169S0iV5Vj6jqOlVdB1wF9ALPA18L7C61Dvgp\n8Oj4a0XEBTwGXA3UAo+JSOz86jUscU/tIiqLMvnyi4diqtufqvo3A7fJQGxQbYULb98QR8/H7sYY\ns1Xf1En1ImsXSo23ujiH7NSkmKrTh1q6uQE4oaqNquobczwTmOh24WbgNVW9qKodwGvALbML1bCL\nJKeDh29dwcn2Hr73dpPV4VxyzjfAhZ7BuF8RO15thb9Ob7fyjbdviGPnuy3rWDmZJKeDq5cUxFSd\nPtREfzfwTPADEXlcRJqBjzDBHT1QAoxdDtkSOGYkuPcvn8emygK++bOjIW/FGCl2G4gNKs1PZ0FO\nmu0S/b7gQqlFsVck2FRZQOOFXlo6pt+nIhpmnOhFJAXYBjwXPKaqj6hqGfA0cN9cAhGRe0Vkt4js\nbmuLnwZYxuwEF1F19g3xf3553OpwAP9ArAisWGivRC8i1Fa42HX6YkwOgM9WXVMHEiMLpcYL1ulj\npXwTyh39VqBOVc9N8NjTwIcmOO4BysZ8XBo49i6q+pSqrlfV9UVFRSGEZcSrVcW5fKimlH97/fSM\nduiKNLfHR2VRVkwsvAm3DRUuzvn8m2fbRX1TJ1fMyyY7LdnqUN7livlZFGalxMyAbCiJ/h4uL9uM\nnSR+B3B4gmteAbaISH5gEHZL4JhhAPDZLctwOoQnXp7o2ye67LQidrzacnvV6UdHlfqmjphZKDWe\niLCxspDXj7fHxF9RM0r0IpIJ3ARsH3P4CRFxi8h+/An8M4Fz14vItwFU9SLwRWBX4O2vAscMA/BP\nR/3ktUvYsf+Mpe1dL3QPcMbbb7uB2KCl87LITU+2zUYkJ9t78PXHzkKpiWyqLOB81wAn2nqsDmVm\niV5Ve1S1QFW9Y459SFVXB6ZY3q6qnsDx3ar6+2PO+46qVgXe/i38n4IR7/7g2iUUZadauoiqodU/\niWylTe/oHQ5hQ7nLNnf0dZcWSsXmHT3A5spgnd762TdmZaxhuczUJD675QrqmzrZceCMJTEEe8HY\npcfNRGor8jl9oZfzXf1WhzJn9U2d5KQlsaQwdltJl7nSKclL540YaFtsEr0RE37nqjKWL8jmKy8f\nZmA4+luxNXh8LHJlkJseewN74fJO35vY2wEpVPVNHaxblB/Te/qKCJsqC3jz5AXLVyWbRG/EBKfD\nP92y+WIf333jdNRf393qtd38+fFWl+SSnuxk5ynr7zDnontgmCPnumJuodRENlcV4u0b4uAZ3/Qn\nR5BJ9EbMeO/SIt63rIi/+8VxLvYMRu11vX1DNF7otXXZBvyb49QszmPn6fi+o9/X3IlqbDUym8zG\nygLA+jq9SfRGTHn41hX0DAzztz8/FrXXPBgYiLXrjJuxNpS7OHzWFzOrkWcj2LFyXWns39HPz0mj\nal6W5dsLmkRvxJQr5mdzd+0i/vOtRk62dUflNRsuDcTau3QD/r43qrCnMX5n39Q1dVI1L4vcjPgY\nT9lUWcCu0xct3eXLJHoj5jxw4xWkJjn48kvRWUTl9nhZmJtGYVZqVF7PStVl+SQ7hZ1xOiCr6l8o\nFQ/1+aBNlYX0Do6wr6XTshhMojdiTlF2Kp+6vorXDp7jrZOR/5PX3eqzfX0+KD3FyeqS3LhdONV4\noZeO3qG4qM8HvWeJCxEsnWZpEr0Rkz5xTQXFuWl8acfBiE5N6x0c5kRbt+1n3IxVW+Fif0sn/UPR\nn8Y6V8GFUtWL4ueOPi8jhdXFubxu4YCsSfRGTEpLdvLgLctxe3z8aO+EffDC4tCZLlRhdYLc0YO/\n783QiFLfZF0pYbbqmzrJSk1i6bz42upxU2UB9U0d9A1a88vVJHojZm1bW8ya0ly+9sqRiP2ABAdi\nE2HGTdD6xS6SncL2uharQwlZXVMHa8tyccbwQqmJbKoqZGhELSuZmURvxCyHQ/jLD6zkjLefb//m\nZERew+3xUpCZwvwc+w/EBuVmJPN7myt4bk/LpVJIPOgdHObw2S5qYnCjkelsKPcPglvVn94keiOm\n1Va4uHnVfP7x1yci0qPF7fGxqiQ3pvYcjYY/umEp83NSefQFNyNxsmn4/hYvI6MaV/X5oIyUJKrL\n8i1bOGUSvRHzHtq6gsHhUZ587WhYn3dgeISj57ps24N+KlmpSTzygZW4PT6+tzN29u2dSnBMYV0M\ntyaeysbKAtweL97e6C9WM4neiHkVhZl8dONint3VzJGzXWF73qNnuxke1YSqz491+5qFbFxSwNdf\nOcKF7gGrw5lWXVMHFYWZuDJTrA5lVjZXFTKq8JYFvYZMojfiwmduWEpWahKPv3gobM8ZbE2cSDNu\nxhIR/uqOVfQMDPPVl49YHc6U/AulOuNqodR468rySE92WrK9oEn0RlzIy0jhj29Yyv872savj4Zn\n83i3x0t2WhJlrvSwPF88Wjo/m49vLufZ3c2XesjEopaOPtq7B6iOo4VS46UkOdhQ4eL149Gv00+b\n6EVkmYjsHfPmE5H7ReRrInJYRPaLyPMiMuGvWhE5LSIHAtfuDv+nYCSKj25czCJXBn+941BYBhDd\nrT5WFyfeQOx4n7nxisDAbEPMDsxeWigVx3f04J9Pf+x8d9Q3f5k20avqEVVdp6rrgKuAXuB54DVg\ntaquAY4CfzHF01wfeI714QjaSEypSU4e2rqcI+e6+MHu5jk919DIKIfO+BJqRexkslKTePjWFRzw\nePn+rtgcmK1v6iQ92cnyBfG1UGq84PaC0S7fhFq6uQE4oaqNqvqqqg4Hjr8FlIY3NMN4t62rF7B+\ncT7fePUo3QPD018wiRNt3QwOjybsQOx429YWc3WFi6++fCSqewHMVH1TB2tKc0lyxne1eWVxDjlp\nSVHvexPqV+1u4JkJjv8e8NIk1yjwqojsEZF7J3tiEblXRHaLyO62tvDUYA37EfHvRNXePcA///rE\nrJ+nwePvQZ8ozcym4x+YXU33wDBfeyU6XUNnqn9ohIZWX1w1MpuM0yFsrCyIet+bGSd6EUkBtgHP\njTv+CDAMPD3Jpdeoag2wFfi0iFw70Umq+pSqrlfV9UVFRTMNy0hA1YvyuX1tMf/ym5Oc8fbN6jnc\nrV4yUpxUFGaGObr4tWxBNh/fVM73dzWztzl2+uC4PV6GRzXu6/NBmyoLaenoo/lib9ReM5Q7+q1A\nnaqeCx4Qkf8F3AZ8RFUnHMVRVU/g3/P4a/u1s47WMAIevHkZowpfe2V20wIbPD5WLMyJu54pkfaZ\nG5dSmBVbK2bf6VgZ/3f0AJur/NsLRnP2TSiJ/h7GlG1E5BbgQWCbqk74q0lEMkUkO/g+sAVwzz5c\nw/Arc2Xw8c3lbK/z4PZ4Q7p2dFRpaPUm5IrY6WSnJfPIrSvY3+Ll2V1zG/AOl/qmTspc6RRl26Mf\nUWVRFvOyU6Pa92ZGiT6QpG8Cto85/PdANvBaYOrkPwXOLRaRFwPnzAd+KyL7gJ3ADlV9OWzRGwnt\n09dX4cpM4Us7DjLJH5QTOn2hh57BEVaZgdgJ3bGumNoKF1995TAdMTAwW9/UGZeNzCYjImyqLOCN\nExdC+r6dixklelXtUdUCVfWOOValqmXBqZeq+oeB462qemvg/ZOqujbwtkpVH4/Mp2Ekopy0ZO6/\ncSlvnbzIawfPTX9BgDu4GbgZiJ2QiPDFO1bT1T/M1161dsVsa2cfZ339tqnPB22qLKS9e4Bj56Oz\nL3J8z1UyEt49tYtYUpTJEy8dZmhkZpsvN3i8pDgdLJ2fFeHo4teyBdl8bGM5z+xsYr+Fe50GG5nZ\nYcbNWJsCdfo3olSnN4neiGvJTgeP3LqCk+09PP1W44yucbd6Wb4wm+Q4n5Mdafff5B+Y/dwLDRHd\nznEqdU0dpCY5WL7AXuMppfkZLHJl8HqU6vTmO92Ie+9fPo9NlQV86+fH8PZN3QJWVf096E3ZZlo5\nack8fOty9jV3znkl8mwFF0qlJNkvVW2uKuCtkxeiMrvJfl89I+EEF1F19g3xD788PuW5LR19ePuG\nTOuDGfrguhJqy1185eXDdPZGd2B2YHgEt8dnm2mV422sLKSrfzjkWWOzYRK9YQurinP5UE0p//76\n6SkXojQkeGviUIkIX7hjFb7+4VmvWZitg60+BkdGqYnDHaVmYuOSQJ0+CuUbk+gN2/jslmU4HcIT\nL0++hL+h1YfTISyL8+ZY0bRiYQ7/c+NivreziQMtkb/7DKoLDMTa9Y6+KDuVZfOzo7K9oEn0hm0s\nyE3jk9cuYcf+M+xpnLi3utvjZem8LNKSnVGOLr49cNMVFGSm8rkX3FEbmK1v6qA4N435OWlReT0r\nbKoqYNfpiwwMj0T0dUyiN2zlD65dQlF26qSLqNytPtOxchZy0pL5i63L2dvcyXN7ojMwW9/UGdcb\njczEpspC+odGL00jjRST6A1byUxN4rNbrqC+qZMdB85c9th5Xz9tXQOsMq0PZuWumhI2lOfzlZeP\nRHxg9pyvH09nn+0WSo139RIXDol8nd4kesN2fueqMpYvyOYrLx++7E/iS3vEmjv6WRERvrBtNZ29\ng3zj1aMRfa3gtoZ2Wyg1Xk5aMleW5kV84ZRJ9IbtOB3+6ZbNF/v47hunLx13e3yI+AcXjdlZWZzD\n/9xYztNvN0Z0WmB9UycpTkdC/PW1qbKAvc2d9MxhI53pmERv2NJ7lxbxvmVF/N0vjl/aMcnt8VJR\nmElWapLF0cW3B266AldmSkQHZuuaOlhVkkNqkv0HzTdXFjI8quw8fTFir2ESvWFbD9+6gp6BYf72\n58cA/9RKM39+7nLTk3lo6wrqmzr5YV1L2J9/aGSU/S1eqsvsXbYJumpxPilOR0T3kTWJ3rCtK+Zn\nc3ftIv7zrUb2NF7E09lnVsSGyV3VJVy1OJ+vvHQYb+/UbSdCdeiMj4HhUWoW23sgNig9xUnN4ryI\nbkRiEr1haw/ceAWpSQ7u+149YFbEhovDIfzVHavo6B3kG6+Fd8Vsvc0XSk1kU2UhB8/4Itb/3yR6\nw9aKslP51PVVnPH2A2Yz8HBaVZzLR9+zmP98q/FSa4lwqGvqYH5OKsW59l0oNd7mqgJU4a2TkSnf\nmERv2N4nrqmgODeNMlc6uRnJVodjK3+yZRn5GSk8GsZWxvVNnVSX5SOSOPv5rinNIzPFGbH59NMm\nehFZFtgqMPjmE5H7ReRrInJYRPaLyPMiMmFBTURuEZEjInJcRB4K/6dgGFNLS3bynY9v4Ft3V1sd\niu3kpifz51uXs6exg+31njk/X3v3AE0XexOmPh+U7HRQW+Hi9Qj1vZk20avqkeB2gcBVQC/wPPAa\nsFpV1wBHgb8Yf62IOIF/ALYCK4F7RGRlGOM3jBlZviDHVvuOxpLfqSmlZlEeX37x0LT7AUwnEevz\nQXfWlHLLqgUR6U8faunmBuCEqjaq6quqGpzh/xZQOsH5tcDxwN6xg8D3gTtmH65hGLHGPzC7mo7e\nQZ58bW4rZuubOkhyCFcm4OrlbWuLefCW5Tgd4S9ZhZro7waemeD47wEvTXC8BBjbAaklcOxdRORe\nEdktIrvb2tpCDMswDCutLsnlI1cv5j/ePM3BwObrs1HX1MHK4hzTXTTMZpzoRSQF2AY8N+74I8Aw\n8PRcAlHVp1R1vaquLyoqmstTGYZhgc9uWUZeRgqPvuCesHPodIYDC6VMiS38Qrmj3wrUqeq54AER\n+V/AbcBHdOL/WQ9QNubj0sAxwzBsJjcjmYduWc7uxg6214X+Y37kXBe9gyNU23RHKSuFkujvYUzZ\nRkRuAR4EtqnqZHu37QKWikhF4C+Cu4EfzzZYwzBi2+9cVcq6sjy+/NJhfP2hDcwGB2LNHX34zSjR\ni0gmcBOwfczhvweygdcC0y7/KXBusYi8CBAYrL0PeAU4BPxAVRvCGL9hGDHE4RC+9MHVXOgZCHlg\ntq6pg8KsFErz0yMUXeKaURs/Ve0BCsYdq5rk3Fbg1jEfvwi8OIcYDcOII/6B2UV8943T/O76shm3\nhd7b1En1osRaKBUtZmWsYRhh99kty8hNT57xwGxHzyAn23tMfT5CTKI3DCPs8jJS+PNblrPrdAc/\n2jv9wOze5sBCqQRpTRxtJtEbhhERv7u+jLVleTy+Y/qB2bqmDhwCa8sSb6FUNJhEbxhGRDgcwhfv\nWMWFngG++dqxKc+tb+pk+YIcMlLM7l+RYBK9YRgRs6Y0j3tqF/HdN09z+OzEK2ZHRpW9zZ0J18gs\nmkyiNwwjov5syzJy0pJ49IWGCQdmj5/vpntg2NTnI8gkesMwIio/M4UHb1nOzlMXeWFv67ser2vq\nAKBmsUn0kWISvWEYEffh9WWsLc3l8RcP0TVuYLa+qYP8jGTKCzIsis7+TKI3DCPigq2M27sH+NbP\nLh+YrTcLpSLOJHrDMKJibVked29YxL+9cZojZ7sA8PYNcex8N9VlZiA2kkyiNwwjah68eRnZaUmX\nVszuCyyUMvX5yDKJ3jCMqMnPTOHPbl7G26cu8uN9rdQ1dSACa0rNQqlIMoneMIyounvDItaU5vL4\njkP89lg7y+Znk52WbHVYtmYSvWEYUeUMDMy2dQ+wu7HDNDKLApPoDcOIunVleXx4vX/zuWqz0UjE\nmcYShmFY4qGty0lPcXLzygVWh2J7JtEbhmGJvIwUHrt9ldVhJIRpE72ILAOeHXNoCfAo/k2+Pw+s\nAGpVdfck158GuoARYFhV188tZMMwDCMU0yZ6VT0CrAMQESf+BP88kAHcBfzzDF7nelVtn0OchmEY\nxiyFWrq5ATihqo3BA2bZsmEYRmwLddbN3cAzIV6jwKsiskdE7p3sJBG5V0R2i8jutra2EF/CMAzD\nmMyME72IpADbgOdCfI1rVLUG2Ap8WkSunegkVX1KVder6vqioqIQX8IwDMOYTCh39FuBOlU9F8oL\nqKon8O95/LX92lCuNwzDMOYmlER/DyGWbUQkU0Syg+8DWwB3KM9hGIZhzM2MEn0gSd8EbB9z7E4R\naQE2AjtE5JXA8WIReTFw2nzgtyKyD9gJ7FDVl8P5CRiGYRhTk4n2cLSaiLQBjdOeOLFCwEzl9DNf\ni8uZr8flzNfjHXb4WixW1QkHOGMy0c+FiOw2i7L8zNficubrcTnz9XiH3b8WpqmZYRiGzZlEbxiG\nYXN2TPRPWR1ADDFfi8uZr8flzNfjHbb+WtiuRm8YhmFczo539IZhGMYYJtEbhmHYnG0SvYjcIiJH\nROS4iDxkdTxWEpEyEfmliBwUkQYR+YzVMVlNRJwiUi8iP7U6FquJSJ6I/FBEDovIIRHZaHVMVhKR\nBwI/J24ReUZE0qyOKdxskegDffL/AX8/npXAPSKy0tqoLDUM/KmqrgTeg7+ZXCJ/PQA+AxyyOogY\n8S3gZVVdDqwlgb8uIlIC/DGwXlVXA078XXptxRaJHn+jtOOqelJVB4HvA3dYHJNlVPWMqtYF3u/C\n/4NcYm1U1hGRUuADwLetjsVqIpILXAv8K4CqDqpqp7VRWS4JSBeRJPwbKrVaHE/Y2SXRlwDNYz5u\nIYET21giUg5UA29bG4mlvgk8CIxaHUgMqADagH8LlLK+HehllZAC3XW/DjQBZwCvqr5qbVThZ5dE\nb0xARLKA/wLuV1Wf1fFYQURuA86r6h6rY4kRSUAN8I+qWg30AAk7piUi+fj/+q8AioFMEfkf1kYV\nfnZJ9B6gbMzHpYFjCUtEkvEn+adVdft059vYZmBbYJP67wPvF5H/tDYkS7UALaoa/Avvh/gTf6K6\nETilqm2qOoS/Q+8mi2MKO7sk+l3AUhGpCOyEdTfwY4tjsoz4N/L9V+CQqv6N1fFYSVX/QlVLVbUc\n//fFL1TVdndsM6WqZ4FmEVkWOHQDcNDCkKzWBLxHRDICPzc3YMPB6VA3B49JqjosIvcBr+AfNf+O\nqjZYHJaVNgMfBQ6IyN7AsYdV9cUprjESxx8BTwduik4CH7c4Hsuo6tsi8kOgDv9stXps2A7BtEAw\nDMOwObuUbgzDMIxJmERvGIZhcybRG4Zh2JxJ9IZhGDZnEr1hGIbNmURvGIZhcybRG4Zh2Nz/B7y5\naFzXI0Y6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0d_RZCLwtvt",
        "colab_type": "text"
      },
      "source": [
        "## Using Word2Vec by Google\n",
        "\n",
        "Incrementally better with Word2Vec in LSTM and CNN\n",
        "\n",
        "LSTM emb=300, 128 hu, dropout = 0.2:\n",
        "\n",
        " * no pre-trained emb: 0.7410\n",
        " * with Word2Vec: 0.7423\n",
        "\n",
        "CNN emb = 300, 500 hum 250 in Dense\n",
        "\n",
        " * no pre-trained emb: 0.7426\n",
        " * with Word2Vec:0.7453\n",
        "\n",
        "GRU emb=300, 256 hu in GRU, dropout = 0.3: 0.7410/0.7406\n",
        "\n",
        "GRU emb=300, 128 hu in GRU, dropout = 0.3: 0.7422/0.7422\n",
        "\n",
        "GRU emb=300, 128 hu in GRU, dropout = 0.2: 0.7426/0.7430\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKhsvEzJSbK",
        "colab_type": "text"
      },
      "source": [
        "#### Splitting data again: making dictionary only on the train set (minus validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U-mkUw1Cm5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_train, title_test, Y_train, Y_test = train_test_split(\n",
        "    train.title, Y, test_size=0.2, random_state=1234, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzYBwaTDrGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "40741a57-9e4a-470f-f014-0dd960aedf96"
      },
      "source": [
        "title_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    interactive visual exploration of neighbor bas...\n",
              "1    relational division four algorithms and their ...\n",
              "2    simplifying xml schema effortless handling of ...\n",
              "3    funbase a function based information managemen...\n",
              "4    inverted matrix efficient discovery of frequen...\n",
              "5    computational aspects of covering in dominance...\n",
              "6    feaspar   a feature structure parser learning ...\n",
              "7    assessing the scenic route measuring the value...\n",
              "8    webanywhere enabling a screen reading interfac...\n",
              "9    non standard semantics for the method of tempo...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol4tkV4GDWa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "6061848d-7463-4f0d-9b2b-ce569d6b8b4c"
      },
      "source": [
        "# tokenize train data\n",
        "NUM_WORDS = 10000 # keep top 10000 words in a dictionary, can be changed later\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
        "tokenizer.fit_on_texts(title_train)\n",
        "X_train = tokenizer.texts_to_sequences(title_train)\n",
        "X_train = pad_sequences(X_train, maxlen=20)\n",
        "X_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  157,\n",
              "         255,  406,    2,  847,    9,  123,    5,    8,  191],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,   46, 2062, 1460,   64,    4,  421,   56],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        2434,   62,  225, 4073,  574,    2, 1796, 1629,  624],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0, 4074,    3,  279,    9,   17,   50,   16],\n",
              "       [   0,    0,    0,    0,  916,  515,   30,  210,    2,  407, 1119,\n",
              "           5,   44,  516,    5,    6,  116,    2,  157,   34],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,  265,  575,    2, 2063,    5, 1461,  182],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 4075,\n",
              "           3,  126,   66,  376,   14,    7, 1337,  394,   27],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,  802,    6, 4076, 1462,\n",
              "         517,    6,  214,    2,   24, 2985,    5,   15,  917],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 4077, 1630,    3,\n",
              "        4078, 1219,  296,    1,    6,   15,   11, 2064,   49],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "         124,  918,  103,    1,    6,   74,    2,   71, 1631]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54GmwhQEPCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "1d20080e-a579-476d-8f6e-e94b08d081a9"
      },
      "source": [
        "title_test[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10223    modeling chinese documents with topical word c...\n",
              "10224    software engineering for distributed applicati...\n",
              "10225    bin completion algorithms for multicontainer p...\n",
              "10226    using relational operators to structure long t...\n",
              "10227    partially synchronized dec mdps in dynamic mec...\n",
              "10228     disambiguating and interpreting verb definitions\n",
              "10229    user software engineering and the design of in...\n",
              "10230    a generation model to unify topic relevance an...\n",
              "10231                 contracting in the days of ebusiness\n",
              "10232                comparing hybrid peer to peer systems\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8QaU-yeEApT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "87cece60-e3e9-4932-cc88-c2b085db3061"
      },
      "source": [
        "# tokenize test data\n",
        "X_test = tokenizer.texts_to_sequences(title_test)\n",
        "X_test = pad_sequences(X_test, maxlen=20)\n",
        "X_test[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,   78,  268,  146,   10, 1346,  100, 1854,   32],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,   28,   65,    1,   40,   70,    6,   38,  261],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0, 2918, 1862,   64,    1, 2654,    4, 2063,  176],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,   12,   46, 1003,    7,   66,  948,  290,  127],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  818, 1604, 7853,  996,    5,   54,  478,   38],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0, 5814,    4, 2431, 1118, 1194],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          79,   28,   65,    4,    6,   38,    2,  157,   19],\n",
              "       [   0,    0,    0,    0,    0,    0,    3,   72,   21,    7, 5721,\n",
              "         186,  151,    4,  520,    9,  789,    1,  929,   22],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    5,    6, 4334,    2],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,  757,  297,  197,    7,  197,   19]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiemsF3AFH_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "bc317bc4-cc53-4c07-d439-285ddf21ab41"
      },
      "source": [
        "test.title[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    autodomainmine a graphical data mining system ...\n",
              "1    anipqo almost non intrusive parametric query o...\n",
              "2    selection and ranking of text from highly impe...\n",
              "3    conditional random fields for multi agent rein...\n",
              "4                 multi dimensional description logics\n",
              "5    fast on line index construction by geometric p...\n",
              "6                                reasoning about rings\n",
              "7    transductive regression piloted by inter manif...\n",
              "8    a fast and usually linear algorithm for global...\n",
              "9    conditional constraint satisfaction logical fo...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agQ8C0pC2v6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "43a92ae0-31a2-4f59-caf2-7be310419a72"
      },
      "source": [
        "# tokenize new data for Kaggle upload\n",
        "X_new = tokenizer.texts_to_sequences(test.title)\n",
        "X_new = pad_sequences(X_new, maxlen=20)\n",
        "X_new[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    3,  456,    8,   34,   16,    1,  136,   63],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        1501,  124,  715,   20,   63,    1, 1304,  257,  298],\n",
              "       [   0,    0,    0,    0,    0,    0,   99,    4,  121,    2,   39,\n",
              "          23,  886, 1958, 5867,    1,   22,    2,  524,  175],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  342,  234,  837,    1,   47,  247,  392,   14],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   47,  195,  295,  559],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  111,   11,  358,  285,  275,   33,  983,  539],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,   67,  291, 8228],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0, 2157,  318, 4643,   33, 1233, 2043,  189],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,\n",
              "         111,    4, 4086,  140,   57,    1,  305,  245,   25],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,  342,  149,  683,  360, 1259,    4,  193]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPdJONGm16mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e5efd5d8-92cb-41a5-8d1d-3d49389f9bab"
      },
      "source": [
        "# find a number of unique tokens\n",
        "word2index = tokenizer.word_index # create a dictionary\n",
        "print('Found %s unique tokens.' % len(word2index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8357 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJI6rOP63oLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index.items() # explore our dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY4q9YOqEKL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_len2 = len(word2index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzJRiadPJemu",
        "colab_type": "text"
      },
      "source": [
        "#### Getting pre-trained Word2Vec by Google\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jis-OXaAC1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d1b6d1b5-5667-4063-c2df-67e0aafb6eff"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "\n",
        "vec_king = wv['king']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl7XVSCfDjGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a877543-cb92-4dd6-f2b6-b86eb5b5bc41"
      },
      "source": [
        "wv['king']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
              "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
              "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
              "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
              "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
              "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
              "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
              "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
              "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
              "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
              "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
              "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
              "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
              "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
              "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
              "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
              "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
              "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
              "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
              "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
              "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
              "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
              "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
              "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
              "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
              "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
              "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
              "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
              "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
              "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
              "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
              "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
              "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
              "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
              "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
              "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
              "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
              "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
              "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
              "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
              "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
              "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
              "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
              "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
              "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
              "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
              "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
              "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
              "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
              "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
              "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
              "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
              "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
              "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
              "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
              "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
              "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
              "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
              "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
              "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
              "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
              "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
              "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
              "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
              "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
              "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
              "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
              "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
              "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
              "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
              "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
              "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
              "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
              "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
              "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGbZ2JBNMvBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATING EMBEDDINGS MATRIX FOR OUR WORDS\n",
        "# code adapted from https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "# does not work...\n",
        "# word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word2index)+1,NUM_WORDS)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word2index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = wv[word]\n",
        "        # embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "# del(word_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJApcv5L2qE",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM (basic best) with and without Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1o0rqaGGzOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "d8280715-64f7-44a3-c004-831e17b1c8b2"
      },
      "source": [
        "# baseline - best simple LSTM w. emb=300 - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=300, name=\"Embedding\"))  \n",
        "model.add(LSTM(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 16s 98ms/step - loss: 1.0967 - accuracy: 0.5830 - val_loss: 0.8057 - val_accuracy: 0.7015\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 95ms/step - loss: 0.6697 - accuracy: 0.7676 - val_loss: 0.7342 - val_accuracy: 0.7340\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 15s 95ms/step - loss: 0.5369 - accuracy: 0.8117 - val_loss: 0.7527 - val_accuracy: 0.7371\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.4542 - accuracy: 0.8417 - val_loss: 0.7460 - val_accuracy: 0.7410\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.3921 - accuracy: 0.8617 - val_loss: 0.8003 - val_accuracy: 0.7277\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 15s 95ms/step - loss: 0.3411 - accuracy: 0.8796 - val_loss: 0.8289 - val_accuracy: 0.7297\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.2951 - accuracy: 0.8977 - val_loss: 0.8808 - val_accuracy: 0.7187\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 15s 95ms/step - loss: 0.2574 - accuracy: 0.9104 - val_loss: 0.9616 - val_accuracy: 0.7081\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 15s 97ms/step - loss: 0.2260 - accuracy: 0.9234 - val_loss: 0.9813 - val_accuracy: 0.7113\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.1987 - accuracy: 0.9336 - val_loss: 1.0784 - val_accuracy: 0.6968\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 1.0784 - accuracy: 0.6968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0784363746643066, 0.6967918872833252]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEMMvBzFQ5tr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "43d8962c-4d89-4046-e613-3d1fc9814bd5"
      },
      "source": [
        "# best simple LSTM, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(LSTM(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 1.0486 - accuracy: 0.6013 - val_loss: 0.8237 - val_accuracy: 0.6960\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 14s 88ms/step - loss: 0.7376 - accuracy: 0.7293 - val_loss: 0.7490 - val_accuracy: 0.7293\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 14s 86ms/step - loss: 0.6151 - accuracy: 0.7780 - val_loss: 0.7438 - val_accuracy: 0.7351\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.5218 - accuracy: 0.8126 - val_loss: 0.7313 - val_accuracy: 0.7375\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.4575 - accuracy: 0.8353 - val_loss: 0.7482 - val_accuracy: 0.7426\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.3917 - accuracy: 0.8614 - val_loss: 0.7847 - val_accuracy: 0.7347\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.3387 - accuracy: 0.8807 - val_loss: 0.8329 - val_accuracy: 0.7304\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.2935 - accuracy: 0.8939 - val_loss: 0.8787 - val_accuracy: 0.7191\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 14s 86ms/step - loss: 0.2518 - accuracy: 0.9110 - val_loss: 0.9194 - val_accuracy: 0.7132\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 14s 87ms/step - loss: 0.2116 - accuracy: 0.9271 - val_loss: 0.9966 - val_accuracy: 0.7109\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.9965 - accuracy: 0.7109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9965320825576782, 0.7108763456344604]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyqIe5t7HT8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f20d6771-177a-4c38-bc49-0eb137d76cd5"
      },
      "source": [
        "# CHANGED ONLY ONE PART VS THE PREVIOUS CHUNK - significant difference\n",
        "\n",
        "# best simple LSTM, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    weights=[embedding_matrix],  # CHANGED THIS PART                     \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(LSTM(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=7) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 1.0465 - accuracy: 0.5950 - val_loss: 0.8219 - val_accuracy: 0.6956\n",
            "Epoch 2/7\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.7396 - accuracy: 0.7288 - val_loss: 0.7504 - val_accuracy: 0.7234\n",
            "Epoch 3/7\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.6190 - accuracy: 0.7781 - val_loss: 0.7368 - val_accuracy: 0.7289\n",
            "Epoch 4/7\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.5259 - accuracy: 0.8114 - val_loss: 0.7313 - val_accuracy: 0.7363\n",
            "Epoch 5/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4612 - accuracy: 0.8368 - val_loss: 0.7470 - val_accuracy: 0.7347\n",
            "Epoch 6/7\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.3945 - accuracy: 0.8581 - val_loss: 0.7873 - val_accuracy: 0.7332\n",
            "Epoch 7/7\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.3437 - accuracy: 0.8788 - val_loss: 0.8209 - val_accuracy: 0.7320\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.8208 - accuracy: 0.7320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8207780718803406, 0.7320031523704529]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBZ1WjvVMPvu",
        "colab_type": "text"
      },
      "source": [
        "#### GRU basic best\n",
        "\n",
        "I change here emb_dim from 128 to 300, GRU hu from 256 to 128 due to the following:\n",
        "\n",
        " * emb_dim = 300, GRU hu = 256, drop = 0.3: 0.7410 \n",
        " * emb_dim = 300, GRU hu = 128, drop = 0.3: 0.7422 \n",
        " * emb_dim = 300, GRU hu = 128, drop = 0.2: 0.7426 \n",
        "\n",
        "All this is lower than 0.7430 with emb=100 (no pretraining)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr_5FQb6SdUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "96962bd5-2f91-4110-a9d8-038393f6df0e"
      },
      "source": [
        "# Run best baseline GRU w. emb=100 on newly tokenized data: drop of accuracy from 0.7449 to 0.7430\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=100, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 16s 99ms/step - loss: 1.1863 - accuracy: 0.5316 - val_loss: 0.8695 - val_accuracy: 0.6905\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 0.7586 - accuracy: 0.7294 - val_loss: 0.7654 - val_accuracy: 0.7199\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 15s 97ms/step - loss: 0.6168 - accuracy: 0.7840 - val_loss: 0.7456 - val_accuracy: 0.7316\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 16s 98ms/step - loss: 0.5373 - accuracy: 0.8148 - val_loss: 0.7366 - val_accuracy: 0.7430\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 16s 97ms/step - loss: 0.4784 - accuracy: 0.8316 - val_loss: 0.7856 - val_accuracy: 0.7250\n",
            "80/80 [==============================] - 1s 15ms/step - loss: 0.7854 - accuracy: 0.7250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7854050993919373, 0.7249608635902405]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR2ewat0TloM",
        "colab_type": "text"
      },
      "source": [
        "**GRU=256, drop=0.3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOCPTEX4NAva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "d7a1c7f8-d197-40ff-81be-b5e47d635f8f"
      },
      "source": [
        "# baseline - best simple GRU w. emb=300 - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=300, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 1.0683 - accuracy: 0.5916 - val_loss: 0.8032 - val_accuracy: 0.7034\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.6779 - accuracy: 0.7590 - val_loss: 0.7384 - val_accuracy: 0.7340\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.5512 - accuracy: 0.8056 - val_loss: 0.7482 - val_accuracy: 0.7351\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.4667 - accuracy: 0.8352 - val_loss: 0.7587 - val_accuracy: 0.7410\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.4074 - accuracy: 0.8577 - val_loss: 0.8055 - val_accuracy: 0.7273\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 21s 134ms/step - loss: 0.3488 - accuracy: 0.8789 - val_loss: 0.8520 - val_accuracy: 0.7250\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.2990 - accuracy: 0.8951 - val_loss: 0.8922 - val_accuracy: 0.7214\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.2565 - accuracy: 0.9119 - val_loss: 0.9837 - val_accuracy: 0.7097\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.2171 - accuracy: 0.9237 - val_loss: 1.0355 - val_accuracy: 0.7027\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.1821 - accuracy: 0.9376 - val_loss: 1.1173 - val_accuracy: 0.6937\n",
            "80/80 [==============================] - 2s 20ms/step - loss: 1.1175 - accuracy: 0.6937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.117499589920044, 0.6936619877815247]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezKC6kuTQKJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "9015e291-027e-4cc9-8330-4dcfcae2cfeb"
      },
      "source": [
        "# simple GRU, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(GRU(256, dropout=0.3, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 1.0648 - accuracy: 0.5819 - val_loss: 0.8338 - val_accuracy: 0.6874\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.7765 - accuracy: 0.7139 - val_loss: 0.7661 - val_accuracy: 0.7218\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.6589 - accuracy: 0.7579 - val_loss: 0.7350 - val_accuracy: 0.7308\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.5658 - accuracy: 0.7929 - val_loss: 0.7199 - val_accuracy: 0.7406\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.4999 - accuracy: 0.8211 - val_loss: 0.7407 - val_accuracy: 0.7383\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.4431 - accuracy: 0.8383 - val_loss: 0.7600 - val_accuracy: 0.7390\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.3868 - accuracy: 0.8606 - val_loss: 0.8061 - val_accuracy: 0.7304\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 21s 131ms/step - loss: 0.3364 - accuracy: 0.8788 - val_loss: 0.8846 - val_accuracy: 0.7218\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 21s 132ms/step - loss: 0.2895 - accuracy: 0.8952 - val_loss: 0.8596 - val_accuracy: 0.7332\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 21s 133ms/step - loss: 0.2509 - accuracy: 0.9073 - val_loss: 0.9276 - val_accuracy: 0.7183\n",
            "80/80 [==============================] - 2s 21ms/step - loss: 0.9273 - accuracy: 0.7183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9272509813308716, 0.7183098793029785]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tgcW_mpQzUm",
        "colab_type": "text"
      },
      "source": [
        "**GRU=128, drop=0.3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsrIpUt8MO09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "d447bf22-6139-449a-8482-9790c69636a3"
      },
      "source": [
        "# baseline - best simple GRU w. emb=300 - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=300, name=\"Embedding\"))  \n",
        "model.add(GRU(128, dropout=0.3, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 12s 78ms/step - loss: 1.0762 - accuracy: 0.5867 - val_loss: 0.7934 - val_accuracy: 0.7015\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 12s 75ms/step - loss: 0.6784 - accuracy: 0.7609 - val_loss: 0.7396 - val_accuracy: 0.7324\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.5529 - accuracy: 0.8071 - val_loss: 0.7474 - val_accuracy: 0.7422\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.4713 - accuracy: 0.8329 - val_loss: 0.7424 - val_accuracy: 0.7375\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.4128 - accuracy: 0.8544 - val_loss: 0.7915 - val_accuracy: 0.7308\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 12s 77ms/step - loss: 0.3586 - accuracy: 0.8746 - val_loss: 0.8302 - val_accuracy: 0.7238\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.3145 - accuracy: 0.8915 - val_loss: 0.8705 - val_accuracy: 0.7207\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.2796 - accuracy: 0.9045 - val_loss: 0.9192 - val_accuracy: 0.7105\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.2421 - accuracy: 0.9183 - val_loss: 0.9555 - val_accuracy: 0.7148\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.2121 - accuracy: 0.9289 - val_loss: 1.0462 - val_accuracy: 0.6956\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 1.0461 - accuracy: 0.6956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0460939407348633, 0.6956181526184082]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChFftNumPGtg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "276e8cc0-ccb6-427a-ff66-d670d94905ee"
      },
      "source": [
        "# simple GRU, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(GRU(128, dropout=0.3, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 12s 77ms/step - loss: 1.0894 - accuracy: 0.5699 - val_loss: 0.8298 - val_accuracy: 0.6897\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.7708 - accuracy: 0.7157 - val_loss: 0.7583 - val_accuracy: 0.7210\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.6564 - accuracy: 0.7587 - val_loss: 0.7344 - val_accuracy: 0.7328\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.5698 - accuracy: 0.7940 - val_loss: 0.7201 - val_accuracy: 0.7418\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.5084 - accuracy: 0.8172 - val_loss: 0.7259 - val_accuracy: 0.7422\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.4586 - accuracy: 0.8347 - val_loss: 0.7506 - val_accuracy: 0.7344\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.4098 - accuracy: 0.8501 - val_loss: 0.7717 - val_accuracy: 0.7398\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.3647 - accuracy: 0.8686 - val_loss: 0.8252 - val_accuracy: 0.7324\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.3261 - accuracy: 0.8842 - val_loss: 0.8068 - val_accuracy: 0.7379\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.2931 - accuracy: 0.8969 - val_loss: 0.8715 - val_accuracy: 0.7183\n",
            "80/80 [==============================] - 1s 8ms/step - loss: 0.8713 - accuracy: 0.7183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8712825775146484, 0.7183098793029785]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3WEB0MAQkQs",
        "colab_type": "text"
      },
      "source": [
        "**GRU=128, drop=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIzsEnvsNwlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2188de79-0a12-4e60-943a-5f3b2e5de89d"
      },
      "source": [
        "# baseline - best simple GRU w. emb=300 - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len + 1, output_dim=300, name=\"Embedding\"))  \n",
        "model.add(GRU(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 12s 77ms/step - loss: 1.0598 - accuracy: 0.5961 - val_loss: 0.7870 - val_accuracy: 0.7062\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.6616 - accuracy: 0.7650 - val_loss: 0.7369 - val_accuracy: 0.7316\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 12s 75ms/step - loss: 0.5332 - accuracy: 0.8139 - val_loss: 0.7538 - val_accuracy: 0.7426\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.4488 - accuracy: 0.8412 - val_loss: 0.7520 - val_accuracy: 0.7359\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 12s 75ms/step - loss: 0.3872 - accuracy: 0.8638 - val_loss: 0.8086 - val_accuracy: 0.7261\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.3306 - accuracy: 0.8846 - val_loss: 0.8538 - val_accuracy: 0.7207\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.2867 - accuracy: 0.9021 - val_loss: 0.9065 - val_accuracy: 0.7128\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 12s 75ms/step - loss: 0.2510 - accuracy: 0.9133 - val_loss: 0.9650 - val_accuracy: 0.7101\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 12s 75ms/step - loss: 0.2157 - accuracy: 0.9276 - val_loss: 1.0112 - val_accuracy: 0.7007\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.1847 - accuracy: 0.9382 - val_loss: 1.1219 - val_accuracy: 0.6937\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 1.1219 - accuracy: 0.6937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.121912956237793, 0.6936619877815247]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phdKV8_UM8ZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "fa12f0ce-1a79-4395-b374-19996521a2d9"
      },
      "source": [
        "# simple GRU, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(GRU(128, dropout=0.2, name=\"LSTM\")) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 1.0654 - accuracy: 0.5834 - val_loss: 0.8182 - val_accuracy: 0.6937\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.7418 - accuracy: 0.7277 - val_loss: 0.7535 - val_accuracy: 0.7273\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.6226 - accuracy: 0.7751 - val_loss: 0.7376 - val_accuracy: 0.7300\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.5324 - accuracy: 0.8085 - val_loss: 0.7248 - val_accuracy: 0.7430\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.4651 - accuracy: 0.8336 - val_loss: 0.7401 - val_accuracy: 0.7375\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.4140 - accuracy: 0.8492 - val_loss: 0.7689 - val_accuracy: 0.7304\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.3619 - accuracy: 0.8690 - val_loss: 0.8089 - val_accuracy: 0.7308\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 12s 74ms/step - loss: 0.3122 - accuracy: 0.8896 - val_loss: 0.8888 - val_accuracy: 0.7210\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 12s 76ms/step - loss: 0.2681 - accuracy: 0.9045 - val_loss: 0.8836 - val_accuracy: 0.7222\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 12s 73ms/step - loss: 0.2315 - accuracy: 0.9203 - val_loss: 0.9598 - val_accuracy: 0.7097\n",
            "80/80 [==============================] - 1s 7ms/step - loss: 0.9596 - accuracy: 0.7097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9595659375190735, 0.7097026705741882]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDgqaPBpp19F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "4a18d265-46e6-4d78-d984-b325ed138247"
      },
      "source": [
        "# simple GRU, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(GRU(128, dropout=0.2)) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 13s 81ms/step - loss: 1.0661 - accuracy: 0.5812 - val_loss: 0.8174 - val_accuracy: 0.6952\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 13s 78ms/step - loss: 0.7398 - accuracy: 0.7268 - val_loss: 0.7568 - val_accuracy: 0.7199\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 13s 79ms/step - loss: 0.6198 - accuracy: 0.7732 - val_loss: 0.7331 - val_accuracy: 0.7304\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.5291 - accuracy: 0.8095 - val_loss: 0.7238 - val_accuracy: 0.7406\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 13s 80ms/step - loss: 0.4648 - accuracy: 0.8333 - val_loss: 0.7408 - val_accuracy: 0.7359\n",
            "80/80 [==============================] - 1s 9ms/step - loss: 0.7406 - accuracy: 0.7359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7406185865402222, 0.73591548204422]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyTXA9WoAUFF",
        "colab_type": "text"
      },
      "source": [
        "##### Predict on new dataset for kaggle (4th upload)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mObFCUXOsDa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "4f0dcfa4-4454-4ee3-fcc8-b0db06175154"
      },
      "source": [
        "# simple GRU, using pre-trained Word2Vec embeddings - RMSprop\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20, name=\"Embedding\"))  \n",
        "model.add(Bidirectional(GRU(128, dropout=0.2))) \n",
        "# model.add(GRU(128, dropout=0.2)) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=4) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "160/160 [==============================] - 22s 139ms/step - loss: 1.0723 - accuracy: 0.5817 - val_loss: 0.8123 - val_accuracy: 0.6952\n",
            "Epoch 2/4\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.7436 - accuracy: 0.7284 - val_loss: 0.7482 - val_accuracy: 0.7191\n",
            "Epoch 3/4\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.6197 - accuracy: 0.7732 - val_loss: 0.7540 - val_accuracy: 0.7265\n",
            "Epoch 4/4\n",
            "160/160 [==============================] - 22s 137ms/step - loss: 0.5381 - accuracy: 0.8050 - val_loss: 0.7160 - val_accuracy: 0.7441\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.7159 - accuracy: 0.7441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7159335613250732, 0.7441314458847046]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRZnOZXTXA0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e6e00bf1-0c3e-4b36-f656-39e5796fc3b3"
      },
      "source": [
        "Y_new4 = model.predict_classes(X_new)\n",
        "# show the inputs and predicted outputs\n",
        "print(X_new[:5])\n",
        "for i in range(5):\n",
        "\tprint(\"X=%s, Predicted=%s\" % (X_new[i], Y_new4[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    3  456\n",
            "     8   34   16    1  136   63]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 1501  124  715\n",
            "    20   63    1 1304  257  298]\n",
            " [   0    0    0    0    0    0   99    4  121    2   39   23  886 1958\n",
            "  5867    1   22    2  524  175]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0  342  234\n",
            "   837    1   47  247  392   14]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0   47  195  295  559]]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   3 456   8  34  16   1\n",
            " 136  63], Predicted=1\n",
            "X=[   0    0    0    0    0    0    0    0    0    0    0 1501  124  715\n",
            "   20   63    1 1304  257  298], Predicted=1\n",
            "X=[   0    0    0    0    0    0   99    4  121    2   39   23  886 1958\n",
            " 5867    1   22    2  524  175], Predicted=0\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0 342 234 837   1  47 247\n",
            " 392  14], Predicted=2\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  47 195\n",
            " 295 559], Predicted=2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2onf9q0FXAi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test4 = pd.DataFrame(list(zip(test['id'], Y_new4)), \n",
        "               columns = ['id', 'label'])\n",
        "test4\n",
        "test4.to_csv('test4.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBsiTwWsMF-Y",
        "colab_type": "text"
      },
      "source": [
        "#### CNN basic best (emb changed to 300)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blFrvt2VMNLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "5c2696a1-03b5-4694-ffd2-80cecd50c2be"
      },
      "source": [
        "# model from keras site, adapted, 500 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True, \n",
        "                    input_length=20))  \n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(250))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dense(5, activation=\"softmax\"))  \n",
        "model3.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model3.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model3.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 1.0458 - accuracy: 0.5910 - val_loss: 0.7552 - val_accuracy: 0.7304\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.6162 - accuracy: 0.7839 - val_loss: 0.7242 - val_accuracy: 0.7426\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.4379 - accuracy: 0.8521 - val_loss: 0.7508 - val_accuracy: 0.7414\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.3027 - accuracy: 0.8966 - val_loss: 0.8293 - val_accuracy: 0.7375\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.1913 - accuracy: 0.9366 - val_loss: 0.9311 - val_accuracy: 0.7308\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.1083 - accuracy: 0.9667 - val_loss: 1.1119 - val_accuracy: 0.7124\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0628 - accuracy: 0.9805 - val_loss: 1.3304 - val_accuracy: 0.7081\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 1.3886 - val_accuracy: 0.7140\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.0204 - accuracy: 0.9946 - val_loss: 1.9703 - val_accuracy: 0.6768\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.0153 - accuracy: 0.9955 - val_loss: 1.7416 - val_accuracy: 0.7160\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 1.7417 - accuracy: 0.7160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7417304515838623, 0.7159624695777893]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwQFoNi24Sil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "a1f24fb1-2169-4fa0-b0b7-7eeb06c6f4ee"
      },
      "source": [
        "# model from keras site, adapted, 500 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20))  \n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(250))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dense(5, activation=\"softmax\"))  \n",
        "model3.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model3.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=10) \n",
        "model3.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 1.1759 - accuracy: 0.5317 - val_loss: 0.8371 - val_accuracy: 0.6890\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.7418 - accuracy: 0.7261 - val_loss: 0.7711 - val_accuracy: 0.7148\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 25s 153ms/step - loss: 0.5665 - accuracy: 0.7898 - val_loss: 0.7286 - val_accuracy: 0.7359\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.4238 - accuracy: 0.8460 - val_loss: 0.7371 - val_accuracy: 0.7453\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.3141 - accuracy: 0.8876 - val_loss: 0.8540 - val_accuracy: 0.7148\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.2303 - accuracy: 0.9181 - val_loss: 0.8795 - val_accuracy: 0.7269\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.1584 - accuracy: 0.9441 - val_loss: 1.1493 - val_accuracy: 0.7031\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.1147 - accuracy: 0.9584 - val_loss: 1.3391 - val_accuracy: 0.6964\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.0872 - accuracy: 0.9709 - val_loss: 1.3041 - val_accuracy: 0.7105\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.0680 - accuracy: 0.9760 - val_loss: 1.3330 - val_accuracy: 0.7195\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 1.3328 - accuracy: 0.7195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3327656984329224, 0.7194835543632507]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBGkAvnBtv4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "eb78baf7-11a9-49f1-cb4a-56ba50965e1c"
      },
      "source": [
        "# model from keras site, adapted, 500 hu, emb=100\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20))  \n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(250))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dense(5, activation=\"softmax\"))  \n",
        "model3.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model3.fit(X_train, Y_train, validation_data = (X_test, Y_test), batch_size=64, epochs=5) \n",
        "model3.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 24s 147ms/step - loss: 1.1648 - accuracy: 0.5368 - val_loss: 0.8386 - val_accuracy: 0.6835\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 23s 147ms/step - loss: 0.7420 - accuracy: 0.7268 - val_loss: 0.7685 - val_accuracy: 0.7160\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 24s 147ms/step - loss: 0.5606 - accuracy: 0.7908 - val_loss: 0.7398 - val_accuracy: 0.7300\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 24s 148ms/step - loss: 0.4256 - accuracy: 0.8492 - val_loss: 0.7534 - val_accuracy: 0.7363\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 26s 165ms/step - loss: 0.3150 - accuracy: 0.8872 - val_loss: 0.8194 - val_accuracy: 0.7269\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.8194 - accuracy: 0.7269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8193970918655396, 0.726917028427124]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_okCZqgojIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = train[['label']]\n",
        "\n",
        "# adapted from https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 10\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define 10-fold cross validation split\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train_, test_ in kfold.split(X, y):\n",
        "  Y = to_categorical(y)\n",
        "  # create model\n",
        "  np.random.seed(42)\n",
        "  random.seed(12345)\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix),                       \n",
        "                    input_length=20)) \n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv1D(500,3,\n",
        "                  padding='same', \n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(250))\n",
        "  # model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(5, activation=\"softmax\"))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # save the best model\n",
        "  # filepath=\"accuracy-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint] \n",
        "  # earlystop = EarlyStopping(monitor='val_accuracy', verbose=1, patience=0)  # , patience=2 - how many epochs w/o improvement we allow\n",
        "  # Fit the model\n",
        "  model.fit(X[train_], Y[train_], validation_data = (X[test_], Y[test_]), epochs=7, batch_size=64,callbacks=callbacks_list,verbose=0)\n",
        "  # load the best model before evaluation, otherwise a model from the last epoch will be used.\n",
        "  model = load_model(\"weights.best.hdf5\")\n",
        "  scores = model.evaluate(X[test_], Y[test_], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8dEr81AJtZ3",
        "colab_type": "text"
      },
      "source": [
        "#### CNN from paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnFdDSELI1dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "434328cf-1b3b-4c8a-a422-bc8e7418a792"
      },
      "source": [
        "# NOTHING BETTER\n",
        "# kernel size: 3,4,5\n",
        "dropout = [0, 0.25, 0.35, 0.5]\n",
        "filters = [50, 100, 200, 300]\n",
        "# MaxOverTime pooling\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "for i in filters:\n",
        "  for j in dropout:\n",
        "    print(\"Number of hidden units: \", i, \"dropout: \",j)\n",
        "\n",
        "    def custom_CNN(input_length=20,vocab_size=dict_len2+1,emb_dim = 300):\n",
        "      # define CNN layer\n",
        "      inputs1 = Input(shape=(input_length,))\n",
        "      emb1 = Embedding(vocab_size, emb_dim, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix))(inputs1)\n",
        "      # Embedding(dict_len2 + 1, output_dim=EMBEDDING_DIM, trainable=True,                       \n",
        "      #               embeddings_initializer=Constant(embedding_matrix),                       \n",
        "      #               input_length=20)) \n",
        "      conv1 = Conv1D(filters=i, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "      drop1 = Dropout(j)(conv1)\n",
        "      pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "      # pool1 = GlobalMaxPooling1D()(conv1)\n",
        "      # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "      flat1 = Flatten()(pool1)\n",
        "\n",
        "      inputs2 = Input(shape=(input_length,))\n",
        "      emb2 = Embedding(vocab_size, emb_dim, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix))(inputs2)\n",
        "      conv2 = Conv1D(filters=i, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "      drop2 = Dropout(j)(conv2)\n",
        "      pool2 = GlobalMaxPooling1D()(drop2)\n",
        "      # pool2 = GlobalMaxPooling1D()(conv2)\n",
        "      # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "      flat2 = Flatten()(pool2)\n",
        "\n",
        "      inputs3 = Input(shape=(input_length,))\n",
        "      emb3 = Embedding(vocab_size, emb_dim, trainable=True,                       \n",
        "                    embeddings_initializer=Constant(embedding_matrix))(inputs3)\n",
        "      conv3 = Conv1D(filters=i, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "      drop3 = Dropout(j)(conv3)\n",
        "      pool3 = GlobalMaxPooling1D()(drop3)\n",
        "      # pool3 = GlobalMaxPooling1D()(conv3)\n",
        "      # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "      flat3 = Flatten()(pool3)\n",
        "\n",
        "      # merge CNN output with reference stats\n",
        "      merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "      dense1 = Dense(500, activation='relu',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01))(merged) #\n",
        "\n",
        "      outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "      model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "      # print(model.summary())\n",
        "      # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "      return model\n",
        "\n",
        "    np.random.seed(5)\n",
        "    random.seed(5)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                  inter_op_parallelism_threads=1)\n",
        "    tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "    # define model\n",
        "    model = custom_CNN(input_length=20, vocab_size=dict_len2+1,emb_dim = 300)\n",
        "    model.fit(x=[X_train,X_train,X_train], y=Y_train, validation_data = ([X_test,X_test,X_test], Y_test), batch_size=64, epochs=10) \n",
        "    # save the model\n",
        "    # model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of hidden units:  50 dropout:  0\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 1.8652 - accuracy: 0.5079 - val_loss: 1.1116 - val_accuracy: 0.6984\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 95ms/step - loss: 0.9333 - accuracy: 0.7470 - val_loss: 0.9634 - val_accuracy: 0.7187\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.6746 - accuracy: 0.8366 - val_loss: 0.9325 - val_accuracy: 0.7261\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4871 - accuracy: 0.9021 - val_loss: 0.9335 - val_accuracy: 0.7340\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.3494 - accuracy: 0.9465 - val_loss: 0.9447 - val_accuracy: 0.7222\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.2560 - accuracy: 0.9720 - val_loss: 0.9537 - val_accuracy: 0.7273\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.1945 - accuracy: 0.9851 - val_loss: 0.9956 - val_accuracy: 0.7066\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.1505 - accuracy: 0.9919 - val_loss: 1.0160 - val_accuracy: 0.7128\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.1265 - accuracy: 0.9933 - val_loss: 1.0172 - val_accuracy: 0.7191\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.1061 - accuracy: 0.9956 - val_loss: 1.0226 - val_accuracy: 0.7207\n",
            "Number of hidden units:  50 dropout:  0.25\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 15s 96ms/step - loss: 1.9235 - accuracy: 0.5023 - val_loss: 1.1660 - val_accuracy: 0.6870\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 94ms/step - loss: 0.9850 - accuracy: 0.7241 - val_loss: 0.9847 - val_accuracy: 0.7038\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.7544 - accuracy: 0.8095 - val_loss: 0.9177 - val_accuracy: 0.7191\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.5967 - accuracy: 0.8576 - val_loss: 0.8775 - val_accuracy: 0.7347\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.4715 - accuracy: 0.8984 - val_loss: 0.9037 - val_accuracy: 0.7183\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.3681 - accuracy: 0.9336 - val_loss: 0.8894 - val_accuracy: 0.7332\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.2954 - accuracy: 0.9536 - val_loss: 0.9079 - val_accuracy: 0.7230\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.2370 - accuracy: 0.9678 - val_loss: 0.9144 - val_accuracy: 0.7332\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 16s 99ms/step - loss: 0.2042 - accuracy: 0.9761 - val_loss: 0.9604 - val_accuracy: 0.7199\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.1722 - accuracy: 0.9800 - val_loss: 1.0605 - val_accuracy: 0.6964\n",
            "Number of hidden units:  50 dropout:  0.35\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 1.9478 - accuracy: 0.4893 - val_loss: 1.1913 - val_accuracy: 0.6839\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 1.0035 - accuracy: 0.7173 - val_loss: 1.0092 - val_accuracy: 0.7011\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.7860 - accuracy: 0.7917 - val_loss: 0.9333 - val_accuracy: 0.7136\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.6418 - accuracy: 0.8388 - val_loss: 0.8689 - val_accuracy: 0.7402\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.5196 - accuracy: 0.8760 - val_loss: 0.8700 - val_accuracy: 0.7281\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.4232 - accuracy: 0.9120 - val_loss: 0.8598 - val_accuracy: 0.7363\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.3530 - accuracy: 0.9333 - val_loss: 0.8777 - val_accuracy: 0.7257\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.2903 - accuracy: 0.9502 - val_loss: 0.9093 - val_accuracy: 0.7210\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.2433 - accuracy: 0.9631 - val_loss: 0.9119 - val_accuracy: 0.7187\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.2121 - accuracy: 0.9713 - val_loss: 1.0052 - val_accuracy: 0.6909\n",
            "Number of hidden units:  50 dropout:  0.5\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 2.0008 - accuracy: 0.4882 - val_loss: 1.2573 - val_accuracy: 0.6741\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 1.0361 - accuracy: 0.7009 - val_loss: 1.0481 - val_accuracy: 0.7117\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.8294 - accuracy: 0.7755 - val_loss: 0.9694 - val_accuracy: 0.7234\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.6996 - accuracy: 0.8146 - val_loss: 0.8835 - val_accuracy: 0.7324\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 15s 93ms/step - loss: 0.5879 - accuracy: 0.8515 - val_loss: 0.8885 - val_accuracy: 0.7246\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.5066 - accuracy: 0.8778 - val_loss: 0.8518 - val_accuracy: 0.7297\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 15s 92ms/step - loss: 0.4363 - accuracy: 0.9009 - val_loss: 0.8542 - val_accuracy: 0.7320\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 15s 91ms/step - loss: 0.3782 - accuracy: 0.9205 - val_loss: 0.8606 - val_accuracy: 0.7320\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 14s 90ms/step - loss: 0.3216 - accuracy: 0.9385 - val_loss: 0.8612 - val_accuracy: 0.7285\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 14s 91ms/step - loss: 0.2875 - accuracy: 0.9474 - val_loss: 0.8952 - val_accuracy: 0.7105\n",
            "Number of hidden units:  100 dropout:  0\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 25s 153ms/step - loss: 2.1082 - accuracy: 0.5072 - val_loss: 1.1207 - val_accuracy: 0.6948\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.9430 - accuracy: 0.7459 - val_loss: 0.9624 - val_accuracy: 0.7128\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.6878 - accuracy: 0.8357 - val_loss: 0.9323 - val_accuracy: 0.7195\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.5034 - accuracy: 0.8995 - val_loss: 0.9063 - val_accuracy: 0.7332\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.3660 - accuracy: 0.9420 - val_loss: 0.9493 - val_accuracy: 0.7203\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.2704 - accuracy: 0.9680 - val_loss: 0.9547 - val_accuracy: 0.7238\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.2041 - accuracy: 0.9830 - val_loss: 0.9887 - val_accuracy: 0.7046\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.1611 - accuracy: 0.9899 - val_loss: 0.9751 - val_accuracy: 0.7207\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 0.1358 - accuracy: 0.9913 - val_loss: 0.9736 - val_accuracy: 0.7269\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.1093 - accuracy: 0.9971 - val_loss: 0.9924 - val_accuracy: 0.7191\n",
            "Number of hidden units:  100 dropout:  0.25\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 25s 153ms/step - loss: 2.1816 - accuracy: 0.5024 - val_loss: 1.1663 - val_accuracy: 0.6890\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.9839 - accuracy: 0.7257 - val_loss: 0.9924 - val_accuracy: 0.7066\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.7543 - accuracy: 0.8063 - val_loss: 0.9516 - val_accuracy: 0.7074\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.5979 - accuracy: 0.8599 - val_loss: 0.8783 - val_accuracy: 0.7355\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 153ms/step - loss: 0.4656 - accuracy: 0.9034 - val_loss: 0.8841 - val_accuracy: 0.7320\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.3601 - accuracy: 0.9383 - val_loss: 0.9015 - val_accuracy: 0.7332\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.2917 - accuracy: 0.9544 - val_loss: 0.9525 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.2305 - accuracy: 0.9729 - val_loss: 0.9346 - val_accuracy: 0.7257\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.1927 - accuracy: 0.9798 - val_loss: 0.9391 - val_accuracy: 0.7207\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.1625 - accuracy: 0.9870 - val_loss: 1.0801 - val_accuracy: 0.6874\n",
            "Number of hidden units:  100 dropout:  0.35\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 2.2407 - accuracy: 0.4970 - val_loss: 1.2056 - val_accuracy: 0.6882\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 1.0073 - accuracy: 0.7185 - val_loss: 0.9974 - val_accuracy: 0.7136\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.7839 - accuracy: 0.7960 - val_loss: 0.9569 - val_accuracy: 0.7019\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.6357 - accuracy: 0.8423 - val_loss: 0.8698 - val_accuracy: 0.7433\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.5126 - accuracy: 0.8817 - val_loss: 0.8684 - val_accuracy: 0.7254\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.4103 - accuracy: 0.9173 - val_loss: 0.8730 - val_accuracy: 0.7316\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.3408 - accuracy: 0.9393 - val_loss: 0.8870 - val_accuracy: 0.7199\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 151ms/step - loss: 0.2751 - accuracy: 0.9575 - val_loss: 0.9017 - val_accuracy: 0.7312\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.2280 - accuracy: 0.9704 - val_loss: 0.9308 - val_accuracy: 0.7156\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 148ms/step - loss: 0.1966 - accuracy: 0.9769 - val_loss: 1.1006 - val_accuracy: 0.6639\n",
            "Number of hidden units:  100 dropout:  0.5\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 24s 152ms/step - loss: 2.3304 - accuracy: 0.4783 - val_loss: 1.2670 - val_accuracy: 0.6745\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 1.0413 - accuracy: 0.7043 - val_loss: 1.0500 - val_accuracy: 0.7081\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 24s 150ms/step - loss: 0.8248 - accuracy: 0.7765 - val_loss: 0.9669 - val_accuracy: 0.7207\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.6906 - accuracy: 0.8191 - val_loss: 0.8966 - val_accuracy: 0.7359\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.5774 - accuracy: 0.8562 - val_loss: 0.8670 - val_accuracy: 0.7328\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.4917 - accuracy: 0.8846 - val_loss: 0.8437 - val_accuracy: 0.7344\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 24s 148ms/step - loss: 0.4164 - accuracy: 0.9108 - val_loss: 0.8637 - val_accuracy: 0.7340\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.3543 - accuracy: 0.9297 - val_loss: 0.8460 - val_accuracy: 0.7402\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 24s 149ms/step - loss: 0.3062 - accuracy: 0.9450 - val_loss: 0.8834 - val_accuracy: 0.7175\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 24s 148ms/step - loss: 0.2596 - accuracy: 0.9577 - val_loss: 0.9370 - val_accuracy: 0.7003\n",
            "Number of hidden units:  200 dropout:  0\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 39s 247ms/step - loss: 2.3260 - accuracy: 0.4896 - val_loss: 1.1143 - val_accuracy: 0.6944\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.9445 - accuracy: 0.7451 - val_loss: 0.9595 - val_accuracy: 0.7152\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.6948 - accuracy: 0.8333 - val_loss: 0.9029 - val_accuracy: 0.7191\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 40s 252ms/step - loss: 0.5178 - accuracy: 0.8925 - val_loss: 0.9034 - val_accuracy: 0.7344\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 38s 239ms/step - loss: 0.3786 - accuracy: 0.9378 - val_loss: 0.9778 - val_accuracy: 0.7148\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.2843 - accuracy: 0.9655 - val_loss: 0.9329 - val_accuracy: 0.7277\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.2171 - accuracy: 0.9810 - val_loss: 0.9568 - val_accuracy: 0.7285\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.1706 - accuracy: 0.9892 - val_loss: 1.0325 - val_accuracy: 0.7261\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.1450 - accuracy: 0.9905 - val_loss: 0.9687 - val_accuracy: 0.7285\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.1195 - accuracy: 0.9948 - val_loss: 1.0402 - val_accuracy: 0.7074\n",
            "Number of hidden units:  200 dropout:  0.25\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 2.4176 - accuracy: 0.4798 - val_loss: 1.1631 - val_accuracy: 0.6894\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.9847 - accuracy: 0.7285 - val_loss: 0.9811 - val_accuracy: 0.7077\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.7537 - accuracy: 0.8080 - val_loss: 0.9044 - val_accuracy: 0.7254\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.5959 - accuracy: 0.8596 - val_loss: 0.8706 - val_accuracy: 0.7410\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.4684 - accuracy: 0.9019 - val_loss: 0.8881 - val_accuracy: 0.7316\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.3668 - accuracy: 0.9365 - val_loss: 0.8843 - val_accuracy: 0.7371\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.2916 - accuracy: 0.9583 - val_loss: 0.9250 - val_accuracy: 0.7187\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.2325 - accuracy: 0.9731 - val_loss: 0.9344 - val_accuracy: 0.7285\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.1929 - accuracy: 0.9811 - val_loss: 1.0039 - val_accuracy: 0.7117\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.1632 - accuracy: 0.9865 - val_loss: 1.0410 - val_accuracy: 0.6980\n",
            "Number of hidden units:  200 dropout:  0.35\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 2.4846 - accuracy: 0.4738 - val_loss: 1.2086 - val_accuracy: 0.6866\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 1.0055 - accuracy: 0.7189 - val_loss: 0.9961 - val_accuracy: 0.7183\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.7790 - accuracy: 0.7997 - val_loss: 0.9129 - val_accuracy: 0.7254\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.6297 - accuracy: 0.8490 - val_loss: 0.8756 - val_accuracy: 0.7414\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.5038 - accuracy: 0.8861 - val_loss: 0.8653 - val_accuracy: 0.7293\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.4070 - accuracy: 0.9208 - val_loss: 0.8672 - val_accuracy: 0.7308\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.3256 - accuracy: 0.9473 - val_loss: 0.9589 - val_accuracy: 0.6976\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.2622 - accuracy: 0.9662 - val_loss: 0.9101 - val_accuracy: 0.7304\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.2195 - accuracy: 0.9747 - val_loss: 0.9446 - val_accuracy: 0.7167\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 39s 246ms/step - loss: 0.1814 - accuracy: 0.9832 - val_loss: 1.1919 - val_accuracy: 0.6577\n",
            "Number of hidden units:  200 dropout:  0.5\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 40s 247ms/step - loss: 2.5843 - accuracy: 0.4741 - val_loss: 1.2782 - val_accuracy: 0.6678\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 40s 247ms/step - loss: 1.0431 - accuracy: 0.7071 - val_loss: 1.0365 - val_accuracy: 0.7187\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 39s 246ms/step - loss: 0.8202 - accuracy: 0.7818 - val_loss: 0.9565 - val_accuracy: 0.7171\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 39s 246ms/step - loss: 0.6812 - accuracy: 0.8263 - val_loss: 0.9008 - val_accuracy: 0.7355\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.5649 - accuracy: 0.8660 - val_loss: 0.8698 - val_accuracy: 0.7383\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 39s 245ms/step - loss: 0.4712 - accuracy: 0.8955 - val_loss: 0.8682 - val_accuracy: 0.7312\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 39s 244ms/step - loss: 0.3947 - accuracy: 0.9215 - val_loss: 0.8900 - val_accuracy: 0.7230\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.3360 - accuracy: 0.9368 - val_loss: 0.9433 - val_accuracy: 0.7136\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 39s 243ms/step - loss: 0.2795 - accuracy: 0.9557 - val_loss: 0.9289 - val_accuracy: 0.7171\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 39s 246ms/step - loss: 0.2379 - accuracy: 0.9668 - val_loss: 0.9765 - val_accuracy: 0.6991\n",
            "Number of hidden units:  300 dropout:  0\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 56s 352ms/step - loss: 2.4571 - accuracy: 0.5060 - val_loss: 1.1174 - val_accuracy: 0.6909\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 54s 334ms/step - loss: 0.9443 - accuracy: 0.7448 - val_loss: 0.9553 - val_accuracy: 0.7183\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 53s 334ms/step - loss: 0.6995 - accuracy: 0.8307 - val_loss: 0.9261 - val_accuracy: 0.7148\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 53s 334ms/step - loss: 0.5242 - accuracy: 0.8916 - val_loss: 0.9061 - val_accuracy: 0.7449\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 53s 334ms/step - loss: 0.3866 - accuracy: 0.9356 - val_loss: 0.9403 - val_accuracy: 0.7297\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 0.2917 - accuracy: 0.9628 - val_loss: 0.9786 - val_accuracy: 0.7269\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 53s 333ms/step - loss: 0.2208 - accuracy: 0.9816 - val_loss: 1.0266 - val_accuracy: 0.7023\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 53s 334ms/step - loss: 0.1770 - accuracy: 0.9859 - val_loss: 1.0033 - val_accuracy: 0.7304\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 0.1446 - accuracy: 0.9928 - val_loss: 1.0048 - val_accuracy: 0.7261\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 53s 334ms/step - loss: 0.1275 - accuracy: 0.9929 - val_loss: 1.0367 - val_accuracy: 0.7250\n",
            "Number of hidden units:  300 dropout:  0.25\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 2.5864 - accuracy: 0.4915 - val_loss: 1.1570 - val_accuracy: 0.6862\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 58s 362ms/step - loss: 0.9807 - accuracy: 0.7300 - val_loss: 0.9732 - val_accuracy: 0.7148\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 0.7503 - accuracy: 0.8091 - val_loss: 0.9225 - val_accuracy: 0.7175\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.5936 - accuracy: 0.8638 - val_loss: 0.8745 - val_accuracy: 0.7430\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 0.4633 - accuracy: 0.9044 - val_loss: 0.8904 - val_accuracy: 0.7273\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 0.3667 - accuracy: 0.9352 - val_loss: 0.8926 - val_accuracy: 0.7379\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 55s 343ms/step - loss: 0.2906 - accuracy: 0.9586 - val_loss: 0.9232 - val_accuracy: 0.7222\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 0.2320 - accuracy: 0.9731 - val_loss: 0.9468 - val_accuracy: 0.7293\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.1923 - accuracy: 0.9812 - val_loss: 0.9514 - val_accuracy: 0.7269\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 0.1595 - accuracy: 0.9881 - val_loss: 1.0048 - val_accuracy: 0.7156\n",
            "Number of hidden units:  300 dropout:  0.35\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 2.6531 - accuracy: 0.4816 - val_loss: 1.1842 - val_accuracy: 0.6917\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 1.0069 - accuracy: 0.7196 - val_loss: 0.9804 - val_accuracy: 0.7171\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 0.7834 - accuracy: 0.7962 - val_loss: 0.9378 - val_accuracy: 0.7179\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 56s 349ms/step - loss: 0.6347 - accuracy: 0.8454 - val_loss: 0.8808 - val_accuracy: 0.7383\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 57s 356ms/step - loss: 0.5106 - accuracy: 0.8868 - val_loss: 0.8843 - val_accuracy: 0.7230\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 55s 346ms/step - loss: 0.4098 - accuracy: 0.9196 - val_loss: 0.8835 - val_accuracy: 0.7316\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 56s 351ms/step - loss: 0.3324 - accuracy: 0.9439 - val_loss: 0.9323 - val_accuracy: 0.7132\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 0.2736 - accuracy: 0.9603 - val_loss: 0.9431 - val_accuracy: 0.7179\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 0.2222 - accuracy: 0.9744 - val_loss: 0.9453 - val_accuracy: 0.7238\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 0.1897 - accuracy: 0.9786 - val_loss: 0.9936 - val_accuracy: 0.7066\n",
            "Number of hidden units:  300 dropout:  0.5\n",
            "Epoch 1/10\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 2.7785 - accuracy: 0.4803 - val_loss: 1.2765 - val_accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 1.0352 - accuracy: 0.7108 - val_loss: 1.0335 - val_accuracy: 0.7081\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 0.8159 - accuracy: 0.7863 - val_loss: 0.9685 - val_accuracy: 0.7156\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 0.6806 - accuracy: 0.8260 - val_loss: 0.9033 - val_accuracy: 0.7347\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 0.5636 - accuracy: 0.8676 - val_loss: 0.8830 - val_accuracy: 0.7285\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.4685 - accuracy: 0.8968 - val_loss: 0.9163 - val_accuracy: 0.7105\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.3944 - accuracy: 0.9231 - val_loss: 0.8938 - val_accuracy: 0.7187\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.3261 - accuracy: 0.9441 - val_loss: 0.9031 - val_accuracy: 0.7312\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 0.2761 - accuracy: 0.9569 - val_loss: 0.9392 - val_accuracy: 0.7070\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 0.2374 - accuracy: 0.9664 - val_loss: 0.9884 - val_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UniYXldR6o9S",
        "colab_type": "text"
      },
      "source": [
        "# 4. Using own titles + incoming/outgoing references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTH-Q4o76_S",
        "colab_type": "text"
      },
      "source": [
        "### Upload and pre-process a new data file created externally\n",
        "\n",
        "*  ref_to concatenates titles of all references pointing towards a given article\n",
        "*  ref_from concatenates titles of all references to which a given article refers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PJTOY1A7_2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "60c5f9ad-95d6-4505-a079-82020125cce8"
      },
      "source": [
        "cross_ref.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>ref_to</th>\n",
              "      <th>ref_from</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>a framework for clustering evolving data strea...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>implementation techniques for main memory data...</td>\n",
              "      <td>providing better support for a class of decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>statix making xml count answering xml queries ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>temporal databases   status and research direc...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>dynamic itemset counting and implication rules...</td>\n",
              "      <td>pattern lattice traversal by selective jumps</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                           ref_from\n",
              "0           1  ...                                                NaN\n",
              "1           2  ...  providing better support for a class of decisi...\n",
              "2           3  ...                                                NaN\n",
              "3           4  ...                                                NaN\n",
              "4           5  ...       pattern lattice traversal by selective jumps\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "032W2aiSKS03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_ref2 = cross_ref.fillna(\"noref\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivQiNumS93aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "c8884fc9-883a-4de9-cfc0-a952b1ceea22"
      },
      "source": [
        "train3 = train.join(cross_ref2.set_index('id'), how = 'left', on=\"id\", rsuffix=\"cross\")\n",
        "train3.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>labelcross</th>\n",
              "      <th>ref_to</th>\n",
              "      <th>ref_from</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a framework for clustering evolving data strea...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>implementation techniques for main memory data...</td>\n",
              "      <td>providing better support for a class of decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>statix making xml count answering xml queries ...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>temporal databases   status and research direc...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>dynamic itemset counting and implication rules...</td>\n",
              "      <td>pattern lattice traversal by selective jumps</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>computational aspects of covering in dominance...</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>feaspar   a feature structure parser learning ...</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>gemini a natural language system for spoken la...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>assessing the scenic route measuring the value...</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>webanywhere enabling a screen reading interfac...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>knowing the users every move user activity tra...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>non standard semantics for the method of tempo...</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>towards an implementation of database manageme...</td>\n",
              "      <td>generalized events in temporal databases</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                           ref_from\n",
              "0   0  ...                                              noref\n",
              "1   3  ...  providing better support for a class of decisi...\n",
              "2   6  ...                                              noref\n",
              "3   8  ...                                              noref\n",
              "4   9  ...       pattern lattice traversal by selective jumps\n",
              "5  11  ...                                              noref\n",
              "6  13  ...                                              noref\n",
              "7  18  ...                                              noref\n",
              "8  20  ...                                              noref\n",
              "9  24  ...           generalized events in temporal databases\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0WdemAVZSYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train3[\"merged\"] = train3[\"title\"] +\" \"+ train3[\"ref_to\"] +\" \"+ train3[\"ref_from\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tow9Blfg-OC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "1d4ef547-96a3-4cb1-f292-8197dafdacbe"
      },
      "source": [
        "train3.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>labelcross</th>\n",
              "      <th>ref_to</th>\n",
              "      <th>ref_from</th>\n",
              "      <th>merged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a framework for clustering evolving data strea...</td>\n",
              "      <td>noref</td>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>implementation techniques for main memory data...</td>\n",
              "      <td>providing better support for a class of decisi...</td>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>statix making xml count answering xml queries ...</td>\n",
              "      <td>noref</td>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>temporal databases   status and research direc...</td>\n",
              "      <td>noref</td>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>dynamic itemset counting and implication rules...</td>\n",
              "      <td>pattern lattice traversal by selective jumps</td>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>computational aspects of covering in dominance...</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>computational aspects of covering in dominance...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>feaspar   a feature structure parser learning ...</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>gemini a natural language system for spoken la...</td>\n",
              "      <td>noref</td>\n",
              "      <td>feaspar   a feature structure parser learning ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>assessing the scenic route measuring the value...</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>assessing the scenic route measuring the value...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>webanywhere enabling a screen reading interfac...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>knowing the users every move user activity tra...</td>\n",
              "      <td>noref</td>\n",
              "      <td>webanywhere enabling a screen reading interfac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>non standard semantics for the method of tempo...</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>towards an implementation of database manageme...</td>\n",
              "      <td>generalized events in temporal databases</td>\n",
              "      <td>non standard semantics for the method of tempo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                             merged\n",
              "0   0  ...  interactive visual exploration of neighbor bas...\n",
              "1   3  ...  relational division four algorithms and their ...\n",
              "2   6  ...  simplifying xml schema effortless handling of ...\n",
              "3   8  ...  funbase a function based information managemen...\n",
              "4   9  ...  inverted matrix efficient discovery of frequen...\n",
              "5  11  ...  computational aspects of covering in dominance...\n",
              "6  13  ...  feaspar   a feature structure parser learning ...\n",
              "7  18  ...  assessing the scenic route measuring the value...\n",
              "8  20  ...  webanywhere enabling a screen reading interfac...\n",
              "9  24  ...  non standard semantics for the method of tempo...\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-oiWvV__k9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test3 = test.join(cross_ref2.set_index('id'), how = 'left', on=\"id\", rsuffix=\"cross\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CZa-1uWVp08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "6ba65114-ab08-46d6-fac9-5546de5e9ffc"
      },
      "source": [
        "test3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>ref_to</th>\n",
              "      <th>ref_from</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>autodomainmine a graphical data mining system ...</td>\n",
              "      <td>12780</td>\n",
              "      <td>noref</td>\n",
              "      <td>what is the nearest neighbor in high dimension...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>anipqo almost non intrusive parametric query o...</td>\n",
              "      <td>12781</td>\n",
              "      <td>noref</td>\n",
              "      <td>parametric query optimization optimization of ...</td>\n",
              "      <td>on the production of anorexic plan diagrams</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>selection and ranking of text from highly impe...</td>\n",
              "      <td>12782</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>conditional random fields for multi agent rein...</td>\n",
              "      <td>12783</td>\n",
              "      <td>noref</td>\n",
              "      <td>sequential optimality and coordination in mult...</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>multi dimensional description logics</td>\n",
              "      <td>12784</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12777</th>\n",
              "      <td>25553</td>\n",
              "      <td>currency based updates to distributed material...</td>\n",
              "      <td>25557</td>\n",
              "      <td>noref</td>\n",
              "      <td>database snapshots maintenance of views implem...</td>\n",
              "      <td>relaxed currency and consistency how to say go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12778</th>\n",
              "      <td>25556</td>\n",
              "      <td>dynamic typing in a statically typed language</td>\n",
              "      <td>25558</td>\n",
              "      <td>noref</td>\n",
              "      <td>quasi static typing an ideal model for recursi...</td>\n",
              "      <td>quasi static typing semantics for communicatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12779</th>\n",
              "      <td>25558</td>\n",
              "      <td>learning sparse metrics via linear programming</td>\n",
              "      <td>25559</td>\n",
              "      <td>noref</td>\n",
              "      <td>fastmap a fast algorithm for indexing data min...</td>\n",
              "      <td>privacy preserving cox regression for survival...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12780</th>\n",
              "      <td>25559</td>\n",
              "      <td>computer assisted reasoning with mizar</td>\n",
              "      <td>25560</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12781</th>\n",
              "      <td>25560</td>\n",
              "      <td>characterization of a large web site populatio...</td>\n",
              "      <td>25561</td>\n",
              "      <td>noref</td>\n",
              "      <td>gigascope a stream database for network applic...</td>\n",
              "      <td>analysis of multimedia workloads with implicat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12782 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                           ref_from\n",
              "0          1  ...                                              noref\n",
              "1          2  ...        on the production of anorexic plan diagrams\n",
              "2          4  ...                                              noref\n",
              "3          5  ...                                              noref\n",
              "4          7  ...                                              noref\n",
              "...      ...  ...                                                ...\n",
              "12777  25553  ...  relaxed currency and consistency how to say go...\n",
              "12778  25556  ...  quasi static typing semantics for communicatio...\n",
              "12779  25558  ...  privacy preserving cox regression for survival...\n",
              "12780  25559  ...                                              noref\n",
              "12781  25560  ...  analysis of multimedia workloads with implicat...\n",
              "\n",
              "[12782 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhRD7UBhVdFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test3[\"merged\"] = test3[\"title\"] +\" \"+ test3[\"ref_to\"] +\" \"+ test3[\"ref_from\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLfMDW2iFoxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "a484c8dd-1a5e-45e5-fd51-2f04bea4d62f"
      },
      "source": [
        "test3.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>ref_to</th>\n",
              "      <th>ref_from</th>\n",
              "      <th>merged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>autodomainmine a graphical data mining system ...</td>\n",
              "      <td>12780</td>\n",
              "      <td>noref</td>\n",
              "      <td>what is the nearest neighbor in high dimension...</td>\n",
              "      <td>noref</td>\n",
              "      <td>autodomainmine a graphical data mining system ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>anipqo almost non intrusive parametric query o...</td>\n",
              "      <td>12781</td>\n",
              "      <td>noref</td>\n",
              "      <td>parametric query optimization optimization of ...</td>\n",
              "      <td>on the production of anorexic plan diagrams</td>\n",
              "      <td>anipqo almost non intrusive parametric query o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>selection and ranking of text from highly impe...</td>\n",
              "      <td>12782</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>selection and ranking of text from highly impe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>conditional random fields for multi agent rein...</td>\n",
              "      <td>12783</td>\n",
              "      <td>noref</td>\n",
              "      <td>sequential optimality and coordination in mult...</td>\n",
              "      <td>noref</td>\n",
              "      <td>conditional random fields for multi agent rein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>multi dimensional description logics</td>\n",
              "      <td>12784</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>multi dimensional description logics noref noref</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10</td>\n",
              "      <td>fast on line index construction by geometric p...</td>\n",
              "      <td>12785</td>\n",
              "      <td>noref</td>\n",
              "      <td>the performance of three database storage stru...</td>\n",
              "      <td>low cost management of inverted files for onli...</td>\n",
              "      <td>fast on line index construction by geometric p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12</td>\n",
              "      <td>reasoning about rings</td>\n",
              "      <td>12786</td>\n",
              "      <td>noref</td>\n",
              "      <td>abstract interpretation a unified lattice mode...</td>\n",
              "      <td>automatic verification of parameterized linear...</td>\n",
              "      <td>reasoning about rings abstract interpretation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14</td>\n",
              "      <td>transductive regression piloted by inter manif...</td>\n",
              "      <td>12787</td>\n",
              "      <td>noref</td>\n",
              "      <td>efficient co regularised least squares regress...</td>\n",
              "      <td>noref</td>\n",
              "      <td>transductive regression piloted by inter manif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>15</td>\n",
              "      <td>a fast and usually linear algorithm for global...</td>\n",
              "      <td>12788</td>\n",
              "      <td>noref</td>\n",
              "      <td>a unified approach to global program optimizat...</td>\n",
              "      <td>the program structure tree computing control r...</td>\n",
              "      <td>a fast and usually linear algorithm for global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>16</td>\n",
              "      <td>conditional constraint satisfaction logical fo...</td>\n",
              "      <td>12789</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>noref</td>\n",
              "      <td>conditional constraint satisfaction logical fo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                             merged\n",
              "0   1  ...  autodomainmine a graphical data mining system ...\n",
              "1   2  ...  anipqo almost non intrusive parametric query o...\n",
              "2   4  ...  selection and ranking of text from highly impe...\n",
              "3   5  ...  conditional random fields for multi agent rein...\n",
              "4   7  ...   multi dimensional description logics noref noref\n",
              "5  10  ...  fast on line index construction by geometric p...\n",
              "6  12  ...  reasoning about rings abstract interpretation ...\n",
              "7  14  ...  transductive regression piloted by inter manif...\n",
              "8  15  ...  a fast and usually linear algorithm for global...\n",
              "9  16  ...  conditional constraint satisfaction logical fo...\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_x8MMvk8Hlmt",
        "colab": {}
      },
      "source": [
        "# tokenize train data on the whole TEXT (train+test) to see the max sent_len\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(text.title)\n",
        "t = tokenizer.texts_to_sequences(text.title)\n",
        "print(text.title[:10])\n",
        "print(t[:10])\n",
        "print(len(max(t, key=len)))\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "dict_len3 = len(word_index)\n",
        "sent_len = []\n",
        "for i in t:\n",
        "  sent_len.append(len(i))\n",
        "plt.hist(sent_len,bins=28)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5pV6n1VIyX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref_to = tokenizer.texts_to_sequences(cross_ref2.ref_to)\n",
        "ref_from = tokenizer.texts_to_sequences(cross_ref2.ref_from)\n",
        "merged = tokenizer.texts_to_sequences(train3.merged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSe-7MEtcgXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "29adf30b-3d12-4db2-b956-42b720753921"
      },
      "source": [
        "print(ref_to[:10])\n",
        "print(ref_from[:10])\n",
        "print(merged[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 59, 1, 55, 827, 8, 176, 35, 3695, 484, 1003, 1321, 5165, 161, 5, 8, 176], [142, 118, 1, 836, 119, 18, 19, 4360, 3, 90, 57, 1329, 18, 63, 617, 102, 4, 49, 596, 559, 988, 634, 9, 180, 58], [7090, 422, 56, 2207, 193, 56, 40, 10, 302, 8, 359, 210, 9, 326, 2, 261, 1166, 4, 750, 1075, 1, 40, 10, 143, 8, 176, 152, 37, 2250, 5, 3, 195, 2, 980, 476, 12, 3058, 4, 937, 10117, 475, 56, 146, 5, 49, 29, 14, 818, 980, 476, 1, 6, 130, 2, 704, 24, 56, 8, 6, 56, 15, 3, 154, 72, 888, 56, 210, 1160, 24, 56, 8, 12690, 3, 16, 1, 888, 4, 1548, 56, 704, 130, 2, 2340, 4674, 24, 56, 8], [74, 29, 1474, 4, 228, 1381], [53, 2233, 937, 4, 3721, 108, 1, 1092, 3324, 8, 230, 70, 8, 35, 1, 272, 108, 35, 484, 133, 416, 2229, 69, 35, 484, 1456, 319, 33, 3370, 975, 11, 160, 634, 9, 54, 1, 35, 272, 108, 35, 272, 108, 181, 319, 2, 1626, 5, 45, 29, 110, 58, 1, 35, 272, 108, 5, 45, 29], [], [6450, 3, 107, 26, 16, 1, 433, 26, 215], [], [3520, 6, 452, 3088, 3135, 77, 761, 447, 1, 1938, 1354, 48, 4, 770, 325], [86, 11, 142, 2, 18, 46, 19, 12, 74, 83]]\n",
            "[[], [1081, 651, 83, 1, 3, 195, 2, 165, 83, 40, 53, 23, 48, 517, 3438, 2, 663, 5, 6, 6095, 23, 43, 16], [], [], [185, 1200, 1866, 33, 1027, 6103], [], [], [], [], [226, 782, 5, 74, 29]]\n",
            "[[123, 239, 421, 2, 766, 9, 133, 5, 8, 176, 3, 59, 1, 55, 827, 8, 176, 35, 3695, 484, 1003, 1321, 5165, 161, 5, 8, 176], [49, 2612, 1299, 58, 4, 428, 57, 142, 118, 1, 836, 119, 18, 19, 4360, 3, 90, 57, 1329, 18, 63, 617, 102, 4, 49, 596, 559, 988, 634, 9, 180, 58, 1081, 651, 83, 1, 3, 195, 2, 165, 83, 40, 53, 23, 48, 517, 3438, 2, 663, 5, 6, 6095, 23, 43, 16], [2432, 56, 210, 6588, 530, 2, 2142, 980, 476, 7090, 422, 56, 2207, 193, 56, 40, 10, 302, 8, 359, 210, 9, 326, 2, 261, 1166, 4, 750, 1075, 1, 40, 10, 143, 8, 176, 152, 37, 2250, 5, 3, 195, 2, 980, 476, 12, 3058, 4, 937, 10117, 475, 56, 146, 5, 49, 29, 14, 818, 980, 476, 1, 6, 130, 2, 704, 24, 56, 8, 6, 56, 15, 3, 154, 72, 888, 56, 210, 1160, 24, 56, 8, 12690, 3, 16, 1, 888, 4, 1548, 56, 704, 130, 2, 2340, 4674, 24, 56, 8], [6589, 3, 346, 9, 17, 46, 16, 74, 29, 1474, 4, 228, 1381], [924, 568, 30, 156, 2, 484, 1626, 5, 45, 662, 5, 6, 106, 2, 123, 35, 53, 2233, 937, 4, 3721, 108, 1, 1092, 3324, 8, 230, 70, 8, 35, 1, 272, 108, 35, 484, 133, 416, 2229, 69, 35, 484, 1456, 319, 33, 3370, 975, 11, 160, 634, 9, 54, 1, 35, 272, 108, 35, 272, 108, 181, 319, 2, 1626, 5, 45, 29, 110, 58, 1, 35, 272, 108, 5, 45, 29, 185, 1200, 1866, 33, 1027, 6103], [266, 825, 2, 1897, 5, 1898, 192], [6590, 3, 127, 65, 375, 14, 7, 1300, 433, 26, 6450, 3, 107, 26, 16, 1, 433, 26, 215], [908, 6, 6592, 2143, 579, 6, 282, 2, 21, 3143, 5, 15, 767], [6593, 1030, 3, 6594, 1127, 283, 1, 6, 15, 10, 2615, 50, 3520, 6, 452, 3088, 3135, 77, 761, 447, 1, 1938, 1354, 48, 4, 770, 325], [114, 1207, 93, 1, 6, 67, 2, 74, 1627, 86, 11, 142, 2, 18, 46, 19, 12, 74, 83, 226, 782, 5, 74, 29]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRFb_raUdjZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "6405a921-547a-44cc-cfe7-ec1807410402"
      },
      "source": [
        "cross_ref2.ref_to[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    a framework for clustering evolving data strea...\n",
              "1    implementation techniques for main memory data...\n",
              "2    statix making xml count answering xml queries ...\n",
              "3    temporal databases   status and research direc...\n",
              "4    dynamic itemset counting and implication rules...\n",
              "5                                                noref\n",
              "6    gemini a natural language system for spoken la...\n",
              "7                                                noref\n",
              "8    knowing the users every move user activity tra...\n",
              "9    towards an implementation of database manageme...\n",
              "Name: ref_to, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdxF8D9Uf9fT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "6622ce1a-67d7-4a3e-99a7-59b88d7bb9f5"
      },
      "source": [
        "cross_ref2.ref_from[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                noref\n",
              "1    providing better support for a class of decisi...\n",
              "2                                                noref\n",
              "3                                                noref\n",
              "4         pattern lattice traversal by selective jumps\n",
              "5                                                noref\n",
              "6                                                noref\n",
              "7                                                noref\n",
              "8                                                noref\n",
              "9             generalized events in temporal databases\n",
              "Name: ref_from, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFjJsvcPJPhu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "623174d4-2278-4156-9b8f-39e0e389736d"
      },
      "source": [
        "print(len(max(ref_to, key=len)))\n",
        "sent_len_to = []\n",
        "for i in ref_to:\n",
        "  sent_len_to.append(len(i))\n",
        "\n",
        "plt.hist(sent_len_to,bins=28)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bgTQab1M1fA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fea844af-a487-4e32-f767-b829e7e5f156"
      },
      "source": [
        "print(len(max(ref_from, key=len)))\n",
        "sent_len_from = []\n",
        "for i in ref_from:\n",
        "  sent_len_from.append(len(i))\n",
        "\n",
        "plt.hist(sent_len_from,bins=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhoDQZIEGAiSpMZAkCQBBoIkqfl/ZfAo2P+KvcMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGMkknRejsFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bb18801e-fc05-40f3-afd1-d80311de0b75"
      },
      "source": [
        "print(len(max(merged, key=len)))\n",
        "sent_len_merged = []\n",
        "for i in merged:\n",
        "  sent_len_merged.append(len(i))\n",
        "\n",
        "plt.hist(sent_len_merged,bins=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRzpvZH6_Wg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize train data (on the whole set of labeled data)\n",
        "X = tokenizer.texts_to_sequences(train.title)\n",
        "X = pad_sequences(X, maxlen=20)\n",
        "\n",
        "X_to = tokenizer.texts_to_sequences(train3.ref_to)\n",
        "X_to = pad_sequences(X_to, maxlen=100)\n",
        "\n",
        "X_from = tokenizer.texts_to_sequences(train3.ref_from)\n",
        "X_from = pad_sequences(X_from, maxlen=100)\n",
        "\n",
        "X_merged = tokenizer.texts_to_sequences(train3.merged)\n",
        "X_merged = pad_sequences(X_merged, maxlen=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AQaDYz-WUGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TOKENIZE TEST SET\n",
        "\n",
        "X_new = tokenizer.texts_to_sequences(test3.title)\n",
        "X_new = pad_sequences(X_new, maxlen=20)\n",
        "\n",
        "X_new_to = tokenizer.texts_to_sequences(test3.ref_to)\n",
        "X_new_to = pad_sequences(X_new_to, maxlen=100)\n",
        "\n",
        "X_new_from = tokenizer.texts_to_sequences(test3.ref_from)\n",
        "X_new_from = pad_sequences(X_new_from, maxlen=100)\n",
        "\n",
        "X_new_merged = tokenizer.texts_to_sequences(test3.merged)\n",
        "X_new_merged = pad_sequences(X_new_merged, maxlen=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpN4TczxPaz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "35c58445-98b0-4f16-bcdb-12d9732f5988"
      },
      "source": [
        "X[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  123,\n",
              "         239,  421,    2,  766,    9,  133,    5,    8,  176],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,   49, 2612, 1299,   58,    4,  428,   57],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        2432,   56,  210, 6588,  530,    2, 2142,  980,  476],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0, 6589,    3,  346,    9,   17,   46,   16],\n",
              "       [   0,    0,    0,    0,  924,  568,   30,  156,    2,  484, 1626,\n",
              "           5,   45,  662,    5,    6,  106,    2,  123,   35],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,  266,  825,    2, 1897,    5, 1898,  192],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 6590,\n",
              "           3,  127,   65,  375,   14,    7, 1300,  433,   26],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,  908,    6, 6592, 2143,\n",
              "         579,    6,  282,    2,   21, 3143,    5,   15,  767],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0, 6593, 1030,    3,\n",
              "        6594, 1127,  283,    1,    6,   15,   10, 2615,   50],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "         114, 1207,   93,    1,    6,   67,    2,   74, 1627]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OWcwp6HhUa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "449f4200-d0cf-4ef6-ca85-ed423adf4988"
      },
      "source": [
        "X_to[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,  142,  118,    1,  836,  119,   18,   19, 4360,\n",
              "          3,   90,   57, 1329,   18,   63,  617,  102,    4,   49,  596,\n",
              "        559,  988,  634,    9,  180,   58], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn_aFPDIhcxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f76c92af-7db0-4d76-9dfd-a730c550b0e8"
      },
      "source": [
        "X_from[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0, 1081,  651,   83,    1,    3,  195,\n",
              "          2,  165,   83,   40,   53,   23,   48,  517, 3438,    2,  663,\n",
              "          5,    6, 6095,   23,   43,   16], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ws3paUl4Op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "5eaaee7d-6c18-4d16-8faa-82a25daf4118"
      },
      "source": [
        "X_merged[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,   49, 2612, 1299,   58,    4,  428,   57,  142,  118,\n",
              "          1,  836,  119,   18,   19, 4360,    3,   90,   57, 1329,   18,\n",
              "         63,  617,  102,    4,   49,  596,  559,  988,  634,    9,  180,\n",
              "         58, 1081,  651,   83,    1,    3,  195,    2,  165,   83,   40,\n",
              "         53,   23,   48,  517, 3438,    2,  663,    5,    6, 6095,   23,\n",
              "         43,   16], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GIS1GRPNPqcv",
        "colab": {}
      },
      "source": [
        "# MAKE A SECOND DATA SPLIT\n",
        "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X, Y, test_size=0.2, random_state=245, shuffle=True)\n",
        "X_to_train_, X_to_test_, X_from_train_, X_from_test_ = train_test_split(X_to,X_from, test_size=0.2, random_state=245, shuffle=True)\n",
        "X_merged_train_, X_merged_test_ = train_test_split(X_merged, test_size=0.2, random_state=245, shuffle=True)\n",
        "X_train2_, X_test2_ = train_test_split(np.array(train2), test_size=0.2, random_state=245, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN7PoNecGqvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "58baacd0-1a5c-48af-86e5-a7dbf208d3e2"
      },
      "source": [
        "X_to_train_[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,   11,  172,   27,   20,    9,   10, 1798,   60,   24,\n",
              "         143,  146,    7,  516,   23, 1962],\n",
              "       [ 699,   35,  484,  161,    5,    3, 3142,   34, 1686,   51,   12,\n",
              "        1686, 2576,    4, 2181, 2576,  237,    3,  716, 2032,    2,   34,\n",
              "         378,  102,   52,   71,    2,   34, 1686,  329,  919, 2513,  586,\n",
              "         237,    1,   34,   51,  863,    6,  307, 2446,    5,   34,  378,\n",
              "         311,  152, 1279,    5,  152,   37],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C40fgCMoQLaU",
        "colab_type": "text"
      },
      "source": [
        "### Training models + summary\n",
        "\n",
        "\"Merged\" means titles were merged in the following order: \"title\" + \"ref_to\" + \"ref_from\".\n",
        "\n",
        "3 separate embeddings means that \"title\" + \"ref_to\" + \"ref_from\" were inputed as three diff. embeddings (in diferent layers).\n",
        "\n",
        "3 concatenated embeddings means that \"title\" + \"ref_to\" + \"ref_from\" were concatenated (all with their paddings) before feeding to a model.\n",
        "\n",
        " in/out ref max size means per type of reference (eg. 50 per ref_to, 50 per ref_from). \n",
        "\n",
        "\n",
        "| Titles and in/out   refs  | Model                                    | Max input1 length                            | Max input2 length               | Loss   | Accuracy | Epoch |\n",
        "|---------------------------|------------------------------------------|----------------------------------------------|---------------------------------|--------|----------|-------|\n",
        "|                           |                                          | title size or merged title+ref size + references | in/out ref max size (*2) |        |          |       |\n",
        "| Merged                    | CNN(emb=100,hu=500,k=3,dense=250)        | 20                                           |  --                             | 0.6269 | 0.7883   | 3     |\n",
        "| Merged                    | CNN(emb=100,hu=500,k=3,dense=250)        | 50                                           |  --                             | 0.5589 | 0.8009   | 3     |\n",
        "| Merged                    | CNN(emb=100,hu=500,k=3,dense=250)        | 50                                           |  --                             | 0.56   | 0.8009   | 2     |\n",
        "| Merged                    | CNN(emb=100,hu=500,k=3,dense=250)        | 100                                          |  --                             | 0.54   | 0.802    | 2     |\n",
        "| Merged                    | CNN(emb=100,hu=500,k=3,dense=250)        | 200                                          |  --                             | **0.5304** | 0.8016   | 2     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| Merged                    | LSTM(emb=128,hu=128)                     | 50                                           |  --                             | 0.5494 | 0.8044   | 4     |\n",
        "| Merged                    | LSTM(emb=128,hu=128)                     | 100                                          |  --                             | 0.5704 | 0.7989   | 3     |\n",
        "| Merged                    | LSTM(emb=128,hu=128)                     | 200                                          |  --                             | **0.5435** | **0.8114**   | 4     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| Merged                    | CNN(emb=100, ks=3,4,5, hu=100,dense=500) | 50                                           |  --                             | 0.5426 | 0.8059   | 2     |\n",
        "| Merged                    | CNN(emb=100, ks=3,4,5, hu=100,dense=500) | 100                                          |  --                             |        |          |       |\n",
        "| Merged                    | CNN(emb=100, ks=3,4,5, hu=100,dense=500) | 200                                          |  --                             | 0.5441 | 0.8009   | 2     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| 3 separate embeddings     | CNN(emb=100,k=3,hu=100,dense=500)        | 20                                           | 50                              | 0.5524 | 0.7985   | 3     |\n",
        "| 3 separate embeddings     | CNN(emb=100,k=3,hu=100,dense=500)        | 20                                           | 100                             | 0.5519 | 0.8024   | 3     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| 3 concatenated embeddings | CNN(emb=100,k=3,hu=500,dense=250)        | 20                                           | 100                             | 0.5387 | 0.802    | 2     |\n",
        "| 3 concatenated embeddings | CNN(emb=100,k=3,hu=500,dense=250)        | 20                                           | 100                             | 0.5487 | 0.8056   | 3     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| 3 concatenated embeddings | CNN(emb=100, ks=3,4,5, hu=100,dense=500) | 20                                           | 50                              | 0.5815 | 0.8087   | 2     |\n",
        "| 3 concatenated embeddings | CNN(emb=100, ks=3,4,5, hu=100,dense=500) | 20                                           | 100                             | 0.5494 | 0.8083   | 3     |\n",
        "| 3 concatenated embeddings | CNN(emb=100, ks=4,6,8, hu=100,dense=500) | 20                                           | 50                              | **0.5402** | **0.8071**   | 4     |\n",
        "| 3 concatenated embeddings | CNN(emb=100, ks=4,6,8, hu=100,dense=500) | 20                                           | 100                             | **0.5351** | **0.8075**   | 3     |\n",
        "|                           |                                          |                                              |                                 |        |          |       |\n",
        "| 3 concatenated embeddings | LSTM(emb=128,hu=128), RMSprop lr=0.01    | 20                                           | 50                              | 0.5632 | 0.8052   | 3     |\n",
        "| 3 concatenated embeddings | LSTM(emb=128,hu=128), RMSprop lr=0.01    | 20                                           | 100                             | 0.596  | 0.7903   | 4     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLJCnp7GpnHQ",
        "colab_type": "text"
      },
      "source": [
        "#### With only MERGED titles/refs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDMzaebuFP0",
        "colab_type": "text"
      },
      "source": [
        "##### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zUyrhVImT63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "1f01f851-ac5f-4f0e-b0a6-e9ae1bd1645f"
      },
      "source": [
        "# with input_length=20\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len3 + 1, output_dim=100,\n",
        "                    input_length=20))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) \n",
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 20, 100)           1327400   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 20, 500)           150500    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_5 (Glob (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 1255      \n",
            "=================================================================\n",
            "Total params: 1,604,405\n",
            "Trainable params: 1,604,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 11s 1ms/sample - loss: 0.9428 - accuracy: 0.6483 - val_loss: 0.6760 - val_accuracy: 0.7586\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 11s 1ms/sample - loss: 0.5465 - accuracy: 0.8035 - val_loss: 0.6269 - val_accuracy: 0.7703\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 10s 1ms/sample - loss: 0.4329 - accuracy: 0.8455 - val_loss: 0.6113 - val_accuracy: 0.7883\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 11s 1ms/sample - loss: 0.3506 - accuracy: 0.8753 - val_loss: 0.6790 - val_accuracy: 0.7633\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 11s 1ms/sample - loss: 0.2905 - accuracy: 0.8998 - val_loss: 0.6581 - val_accuracy: 0.7848\n",
            "2556/2556 [==============================] - 0s 178us/sample - loss: 3.1249 - accuracy: 0.2520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.1248556922112645, 0.2519562]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTV2LHMkpkm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "77dbb66b-ea86-4b9a-9a52-7c217be4ae9d"
      },
      "source": [
        "# with input_length=50\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(1234) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len3 + 1, output_dim=100,\n",
        "                    input_length=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 50, 100)           1327400   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 50, 500)           150500    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_10 (Glo (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 1255      \n",
            "=================================================================\n",
            "Total params: 1,604,405\n",
            "Trainable params: 1,604,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.9104 - accuracy: 0.6542 - val_loss: 0.6262 - val_accuracy: 0.7656\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.5071 - accuracy: 0.8156 - val_loss: 0.5921 - val_accuracy: 0.7907\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.4003 - accuracy: 0.8575 - val_loss: 0.5589 - val_accuracy: 0.8009\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.3260 - accuracy: 0.8887 - val_loss: 0.6348 - val_accuracy: 0.7790\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.2596 - accuracy: 0.9093 - val_loss: 0.6166 - val_accuracy: 0.8009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5IPXZor6ecZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "7fc0831a-d0a4-4973-9ae8-552a9643d579"
      },
      "source": [
        "# with input_length=50, compatible seed with other models below\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len3 + 1, output_dim=100,input_length=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.9057 - accuracy: 0.6540 - val_loss: 0.6246 - val_accuracy: 0.7825\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 18s 2ms/sample - loss: 0.5097 - accuracy: 0.8180 - val_loss: 0.5600 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 18s 2ms/sample - loss: 0.4058 - accuracy: 0.8532 - val_loss: 0.5852 - val_accuracy: 0.7872\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 18s 2ms/sample - loss: 0.3322 - accuracy: 0.8834 - val_loss: 0.5718 - val_accuracy: 0.7938\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 18s 2ms/sample - loss: 0.2677 - accuracy: 0.9079 - val_loss: 0.6291 - val_accuracy: 0.7844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4qyaVOy-eZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "8fb4e6fe-568b-45b8-c3eb-33640ec781bd"
      },
      "source": [
        "# with input_length=100, compatible seed with other models below\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len3 + 1, output_dim=100,input_length=100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 30s 3ms/sample - loss: 0.8891 - accuracy: 0.6665 - val_loss: 0.6107 - val_accuracy: 0.7840\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 29s 3ms/sample - loss: 0.4963 - accuracy: 0.8226 - val_loss: 0.5400 - val_accuracy: 0.8020\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 29s 3ms/sample - loss: 0.3934 - accuracy: 0.8587 - val_loss: 0.5555 - val_accuracy: 0.7969\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 29s 3ms/sample - loss: 0.3268 - accuracy: 0.8856 - val_loss: 0.5519 - val_accuracy: 0.7958\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 29s 3ms/sample - loss: 0.2644 - accuracy: 0.9104 - val_loss: 0.5987 - val_accuracy: 0.7887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUqUZNUumuEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "63f1ecef-0fc5-4ed6-b14a-53552157e8e0"
      },
      "source": [
        "# with input_length=200, compatible seed with other models below\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(dict_len3 + 1, output_dim=100,input_length=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(500,\n",
        "                 kernel_size,\n",
        "#  \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "                 padding='same', # changed padding here; 0.78p better with \"same\"\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "# model.add(Dropout(0.2)) # REMOVED DROPOUT HERE\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 53s 5ms/sample - loss: 0.8956 - accuracy: 0.6660 - val_loss: 0.6186 - val_accuracy: 0.7813\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 52s 5ms/sample - loss: 0.4962 - accuracy: 0.8231 - val_loss: 0.5304 - val_accuracy: 0.8016\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 53s 5ms/sample - loss: 0.3924 - accuracy: 0.8587 - val_loss: 0.5562 - val_accuracy: 0.7981\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 54s 5ms/sample - loss: 0.3222 - accuracy: 0.8866 - val_loss: 0.5579 - val_accuracy: 0.7989\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 54s 5ms/sample - loss: 0.2626 - accuracy: 0.9078 - val_loss: 0.5904 - val_accuracy: 0.7962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTcupetaoqXo",
        "colab_type": "text"
      },
      "source": [
        "##### CNN from paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUxuz7Mttr52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "db9aabe3-fc08-47bf-d299-07167b133576"
      },
      "source": [
        "# input length = 50\n",
        "\n",
        "def custom_CNN(input_length=50,vocab_size=dict_len3+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_CNN(input_length=50, vocab_size=dict_len3+1,emb_dim = 100)\n",
        "history = model.fit(x=[X_merged_train_,X_merged_train_,X_merged_train_], y=Y_train_, \n",
        "                    validation_data = ([X_merged_test_,X_merged_test_,X_merged_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 26s 3ms/sample - loss: 0.8278 - accuracy: 0.6861 - val_loss: 0.5671 - val_accuracy: 0.8059\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 25s 2ms/sample - loss: 0.4121 - accuracy: 0.8539 - val_loss: 0.5426 - val_accuracy: 0.8059\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 24s 2ms/sample - loss: 0.2517 - accuracy: 0.9103 - val_loss: 0.5958 - val_accuracy: 0.7950\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 23s 2ms/sample - loss: 0.1169 - accuracy: 0.9610 - val_loss: 0.7285 - val_accuracy: 0.7762\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 23s 2ms/sample - loss: 0.0476 - accuracy: 0.9855 - val_loss: 0.8951 - val_accuracy: 0.7833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgtwDQ0uusW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "270aff00-0d49-47ea-c7d8-81dfc8fc342a"
      },
      "source": [
        "# input length = 100\n",
        "\n",
        "def custom_CNN(input_length=100,vocab_size=dict_len3+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_CNN(input_length=100, vocab_size=dict_len3+1,emb_dim = 100)\n",
        "history = model.fit(x=[X_merged_train_,X_merged_train_,X_merged_train_], y=Y_train_, \n",
        "                    validation_data = ([X_merged_test_,X_merged_test_,X_merged_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 44s 4ms/sample - loss: 0.8429 - accuracy: 0.6908 - val_loss: 0.5740 - val_accuracy: 0.8036\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 43s 4ms/sample - loss: 0.4068 - accuracy: 0.8554 - val_loss: 0.5339 - val_accuracy: 0.8056\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 43s 4ms/sample - loss: 0.2452 - accuracy: 0.9130 - val_loss: 0.6049 - val_accuracy: 0.7915\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 43s 4ms/sample - loss: 0.1163 - accuracy: 0.9597 - val_loss: 0.7270 - val_accuracy: 0.7852\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 42s 4ms/sample - loss: 0.0487 - accuracy: 0.9855 - val_loss: 0.9034 - val_accuracy: 0.7817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VP-BXEaWoouP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "f99a977d-c9e8-4302-d1cf-32ac11bb9f2a"
      },
      "source": [
        "# input length = 200\n",
        "\n",
        "def custom_CNN(input_length=200,vocab_size=dict_len3+1,emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_CNN(input_length=200, vocab_size=dict_len3+1,emb_dim = 100)\n",
        "history = model.fit(x=[X_merged_train_,X_merged_train_,X_merged_train_], y=Y_train_, \n",
        "                    validation_data = ([X_merged_test_,X_merged_test_,X_merged_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 91s 9ms/sample - loss: 0.8777 - accuracy: 0.6860 - val_loss: 0.5883 - val_accuracy: 0.7966\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.4118 - accuracy: 0.8543 - val_loss: 0.5441 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.2574 - accuracy: 0.9094 - val_loss: 0.6205 - val_accuracy: 0.7903\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 90s 9ms/sample - loss: 0.1272 - accuracy: 0.9544 - val_loss: 0.7325 - val_accuracy: 0.7860\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.0535 - accuracy: 0.9848 - val_loss: 0.9590 - val_accuracy: 0.7766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUL_sdTo9vkm",
        "colab_type": "text"
      },
      "source": [
        "##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD99DNUX9yF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "e1a6bbae-e522-489c-f768-75ad2de16fd0"
      },
      "source": [
        "# with input_length=50\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len3 + 1, 128, input_length=50)) # adding trainable=True does not change thre result, trainable by default?\n",
        "model.add(LSTM(128, dropout=0.2)) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 25s 2ms/sample - loss: 0.9646 - accuracy: 0.6368 - val_loss: 0.6451 - val_accuracy: 0.7739\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 23s 2ms/sample - loss: 0.5355 - accuracy: 0.8144 - val_loss: 0.5962 - val_accuracy: 0.7793\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 24s 2ms/sample - loss: 0.4409 - accuracy: 0.8485 - val_loss: 0.5673 - val_accuracy: 0.7977\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 23s 2ms/sample - loss: 0.3812 - accuracy: 0.8692 - val_loss: 0.5493 - val_accuracy: 0.8044\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 23s 2ms/sample - loss: 0.3376 - accuracy: 0.8833 - val_loss: 0.5908 - val_accuracy: 0.7770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x-HbRRj-wou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "fda26404-9f0a-42f7-eee5-83b24e8b05a8"
      },
      "source": [
        "# with input_length=100\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len3 + 1, 128, input_length=100)) # adding trainable=True does not change thre result, trainable by default?\n",
        "model.add(LSTM(128, dropout=0.2)) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 46s 4ms/sample - loss: 0.9743 - accuracy: 0.6359 - val_loss: 0.6593 - val_accuracy: 0.7625\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 44s 4ms/sample - loss: 0.5334 - accuracy: 0.8164 - val_loss: 0.5756 - val_accuracy: 0.7891\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 45s 4ms/sample - loss: 0.4395 - accuracy: 0.8508 - val_loss: 0.5704 - val_accuracy: 0.7989\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 45s 4ms/sample - loss: 0.3811 - accuracy: 0.8706 - val_loss: 0.5720 - val_accuracy: 0.7934\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 45s 4ms/sample - loss: 0.3379 - accuracy: 0.8837 - val_loss: 0.5809 - val_accuracy: 0.7860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrUq6S54nVHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "99f98980-d7a0-4ce6-8915-79f5d2c2cd82"
      },
      "source": [
        "# with input_length=200\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5) \n",
        "model = Sequential() \n",
        "model.add(Embedding(dict_len3 + 1, 128, input_length=200)) # adding trainable=True does not change thre result, trainable by default?\n",
        "model.add(LSTM(128, dropout=0.2)) \n",
        "model.add(Dense(5, activation=\"softmax\"))  \n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "history = model.fit(X_merged_train_, Y_train_, validation_data = (X_merged_test_, Y_test_), batch_size=64, epochs=4) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/4\n",
            "10223/10223 [==============================] - 91s 9ms/sample - loss: 0.9704 - accuracy: 0.6357 - val_loss: 0.6615 - val_accuracy: 0.7578\n",
            "Epoch 2/4\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.5343 - accuracy: 0.8153 - val_loss: 0.5484 - val_accuracy: 0.7938\n",
            "Epoch 3/4\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.4407 - accuracy: 0.8498 - val_loss: 0.5654 - val_accuracy: 0.7993\n",
            "Epoch 4/4\n",
            "10223/10223 [==============================] - 89s 9ms/sample - loss: 0.3814 - accuracy: 0.8671 - val_loss: 0.5435 - val_accuracy: 0.8114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IccjKeJDUiSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_new = tokenizer.texts_to_sequences(train.title)\n",
        "# X_new = pad_sequences(X_new, maxlen=20)\n",
        "\n",
        "# X_new_to = tokenizer.texts_to_sequences(train3.ref_to)\n",
        "# X_new_to = pad_sequences(X_new_to, maxlen=100)\n",
        "\n",
        "# X_new_from = tokenizer.texts_to_sequences(train3.ref_from)\n",
        "# X_new_from = pad_sequences(X_new_from, maxlen=100)\n",
        "\n",
        "# X_new_merged = tokenizer.texts_to_sequences(train3.merged)\n",
        "# X_new_merged = pad_sequences(X_new_merged, maxlen=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPjLmQ60WkwC",
        "colab_type": "text"
      },
      "source": [
        "##### Predict on test set for kaggle (10th upload)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KtHjnROMUKQN",
        "colab": {}
      },
      "source": [
        "Y_new10 = model.predict(x=X_new_merged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-GT9_eSUKQb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "1b9f033c-1459-4b83-a0d0-306bfed03ad4"
      },
      "source": [
        "Y_new10[:3] # results before softmax are shown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.8813621e-01, 3.4839161e-02, 1.7492132e-02, 3.0435205e-03,\n",
              "        5.6489006e-02],\n",
              "       [8.5078202e-02, 1.2647440e-02, 8.9278609e-01, 6.2744338e-03,\n",
              "        3.2138061e-03],\n",
              "       [8.9990628e-01, 8.4698750e-03, 3.7087988e-02, 4.6848823e-04,\n",
              "        5.4067414e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "slNlTpzGUKQg",
        "colab": {}
      },
      "source": [
        "# pick the label with max score.\n",
        "flat_predictions = np.argmax(Y_new10, axis=1).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLwDX4NLUKQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "63b29c49-8429-4706-aed5-d077eb815de7"
      },
      "source": [
        "flat_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, ..., 0, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9sz-1-wsUKQr",
        "colab": {}
      },
      "source": [
        "test10 = pd.DataFrame(list(zip(test['id'], flat_predictions)), \n",
        "               columns = ['id', 'label'])\n",
        "test10\n",
        "test10.to_csv('test10.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMggRYvlq-5X",
        "colab_type": "text"
      },
      "source": [
        "#### With separate embeddings for titles and in/out references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR-2H_5Wvfel",
        "colab_type": "text"
      },
      "source": [
        "##### CNN 3 diff embeddings, kernel = 3, filters = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdt8GOjITzbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "d3d3020d-f5fe-41b4-d850-9fdf45a50061"
      },
      "source": [
        "# with input_length2=50\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 13s 1ms/sample - loss: 0.9312 - accuracy: 0.6447 - val_loss: 0.6852 - val_accuracy: 0.7723\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 12s 1ms/sample - loss: 0.4599 - accuracy: 0.8395 - val_loss: 0.5899 - val_accuracy: 0.7856\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 12s 1ms/sample - loss: 0.3224 - accuracy: 0.8849 - val_loss: 0.5524 - val_accuracy: 0.7985\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 12s 1ms/sample - loss: 0.2278 - accuracy: 0.9227 - val_loss: 0.5534 - val_accuracy: 0.7930\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 12s 1ms/sample - loss: 0.1532 - accuracy: 0.9498 - val_loss: 0.5866 - val_accuracy: 0.7879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe477e2f128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdMz038Ux-Wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "02feeb0e-ba13-4911-86d0-f5efc50737bc"
      },
      "source": [
        "# with input_length2=100\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(inputs1)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(inputs2)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(inputs3)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 20s 2ms/sample - loss: 0.9292 - accuracy: 0.6424 - val_loss: 0.6792 - val_accuracy: 0.7778\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.4591 - accuracy: 0.8376 - val_loss: 0.5857 - val_accuracy: 0.7907\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.3231 - accuracy: 0.8853 - val_loss: 0.5519 - val_accuracy: 0.8024\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.2321 - accuracy: 0.9202 - val_loss: 0.5557 - val_accuracy: 0.7919\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 19s 2ms/sample - loss: 0.1586 - accuracy: 0.9467 - val_loss: 0.5718 - val_accuracy: 0.7911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe474dcd940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R93_6iZvv7mS",
        "colab_type": "text"
      },
      "source": [
        "#### With concatenated embeddings for titles and in/out references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74qLDzQiuOUR",
        "colab_type": "text"
      },
      "source": [
        "##### Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8jh7gHz5UIh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "27811e8c-5f41-4b59-8d2d-aeaf8fd233af"
      },
      "source": [
        "# with input_length2=100\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  drop1 = Dropout(0.2)(emb1)\n",
        "  conv1 = Conv1D(filters=500, kernel_size=3, padding='same', activation='relu', strides=1)(drop1)\n",
        "  pool1 = GlobalMaxPooling1D()(conv1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  dense1 = Dense(250, activation='relu')(flat1)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  # rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "# define model\n",
        "model = custom_model(input_length=20, input_length2=100,vocab_size=13273+1,emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 58s 6ms/sample - loss: 0.8964 - accuracy: 0.6669 - val_loss: 0.6232 - val_accuracy: 0.7801\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 57s 6ms/sample - loss: 0.4992 - accuracy: 0.8208 - val_loss: 0.5387 - val_accuracy: 0.8020\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 57s 6ms/sample - loss: 0.4001 - accuracy: 0.8571 - val_loss: 0.5487 - val_accuracy: 0.8056\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 57s 6ms/sample - loss: 0.3319 - accuracy: 0.8837 - val_loss: 0.5653 - val_accuracy: 0.7942\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 57s 6ms/sample - loss: 0.2720 - accuracy: 0.9064 - val_loss: 0.6137 - val_accuracy: 0.7829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b6ede8fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncUbj43I3s5D",
        "colab_type": "text"
      },
      "source": [
        "##### CNN with kernels=3,4,5 and 4,6,8, one concatenated embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I28LBJa30s6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "0928b9ed-d899-4c7e-fb44-43ef8d9d6d63"
      },
      "source": [
        "# with input_length2=50\n",
        "# kernels = 3,4,5\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 36s 3ms/sample - loss: 0.8285 - accuracy: 0.6928 - val_loss: 0.6628 - val_accuracy: 0.7962\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 34s 3ms/sample - loss: 0.4385 - accuracy: 0.8418 - val_loss: 0.5815 - val_accuracy: 0.8087\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 34s 3ms/sample - loss: 0.3254 - accuracy: 0.8831 - val_loss: 0.5563 - val_accuracy: 0.8052\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 34s 3ms/sample - loss: 0.2395 - accuracy: 0.9192 - val_loss: 0.5487 - val_accuracy: 0.8016\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 33s 3ms/sample - loss: 0.1728 - accuracy: 0.9401 - val_loss: 0.5854 - val_accuracy: 0.7856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe4754bc208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Sil0aG15U7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "78a32233-2172-47ed-9c98-b5da8a369b00"
      },
      "source": [
        "# with input_length2=50\n",
        "# kernels = 4,6,8\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=6, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=8, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 49s 5ms/sample - loss: 0.8278 - accuracy: 0.6905 - val_loss: 0.6520 - val_accuracy: 0.8020\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 48s 5ms/sample - loss: 0.4276 - accuracy: 0.8481 - val_loss: 0.5850 - val_accuracy: 0.8052\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 48s 5ms/sample - loss: 0.3084 - accuracy: 0.8895 - val_loss: 0.5558 - val_accuracy: 0.8048\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 48s 5ms/sample - loss: 0.2155 - accuracy: 0.9251 - val_loss: 0.5402 - val_accuracy: 0.8071\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 49s 5ms/sample - loss: 0.1491 - accuracy: 0.9523 - val_loss: 0.5986 - val_accuracy: 0.7833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe4751f2ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XdhkSm8R7Mnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "979e91cf-7aa7-4907-da4a-d7cab777ae76"
      },
      "source": [
        "# with input_length2=100\n",
        "# kernels = 3,4,5\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=3, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 60s 6ms/sample - loss: 0.8250 - accuracy: 0.6939 - val_loss: 0.6545 - val_accuracy: 0.7938\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 59s 6ms/sample - loss: 0.4406 - accuracy: 0.8443 - val_loss: 0.5868 - val_accuracy: 0.7997\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 59s 6ms/sample - loss: 0.3247 - accuracy: 0.8862 - val_loss: 0.5493 - val_accuracy: 0.8083\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 59s 6ms/sample - loss: 0.2395 - accuracy: 0.9175 - val_loss: 0.5501 - val_accuracy: 0.7934\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 60s 6ms/sample - loss: 0.1683 - accuracy: 0.9396 - val_loss: 0.5480 - val_accuracy: 0.8005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe473271d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3-5aHQEy7Mnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "d77c86df-1ad4-4b85-de95-c2360842bbfa"
      },
      "source": [
        "# with input_length2=100\n",
        "# kernels = 4,6,8\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=6, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=8, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=3) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/3\n",
            "10223/10223 [==============================] - 83s 8ms/sample - loss: 0.8238 - accuracy: 0.6932 - val_loss: 0.6508 - val_accuracy: 0.8024\n",
            "Epoch 2/3\n",
            "10223/10223 [==============================] - 82s 8ms/sample - loss: 0.4227 - accuracy: 0.8481 - val_loss: 0.5830 - val_accuracy: 0.8013\n",
            "Epoch 3/3\n",
            "10223/10223 [==============================] - 82s 8ms/sample - loss: 0.3017 - accuracy: 0.8917 - val_loss: 0.5351 - val_accuracy: 0.8075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcc3c0d0438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z_7SPscJVwqi"
      },
      "source": [
        "###### Predict on test set for kaggle (11th upload) - 81.77%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TSf0M7vcVwqm",
        "colab": {}
      },
      "source": [
        "Y_new11 = model.predict(x=[X_new, X_new_to, X_new_from])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCBmDI7nVwqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "abd124bb-e1e3-4d25-fcda-95bcf65f90f7"
      },
      "source": [
        "Y_new11[:3] # results before softmax are shown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7271129 , 0.17593327, 0.03700046, 0.03728134, 0.02267204],\n",
              "       [0.20437443, 0.06697048, 0.7124184 , 0.00811259, 0.00812415],\n",
              "       [0.82428205, 0.07986361, 0.07299255, 0.00736944, 0.01549236]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "no8d86D1Vwq1",
        "colab": {}
      },
      "source": [
        "# pick the label with max score.\n",
        "flat_predictions = np.argmax(Y_new11, axis=1).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2X7Ix3cTVwq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8aa0565e-2a18-4194-a32a-eb16d1120e13"
      },
      "source": [
        "flat_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, ..., 0, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1MFaX-F3Vwq-",
        "colab": {}
      },
      "source": [
        "test11 = pd.DataFrame(list(zip(test['id'], flat_predictions)), \n",
        "               columns = ['id', 'label'])\n",
        "test11\n",
        "test11.to_csv('test11.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zccWrch_gw",
        "colab_type": "text"
      },
      "source": [
        "##### Run model on the whole training data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxdtFe-9iHv0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "c767b86c-ff9a-41de-8808-7ede2e93c0e3"
      },
      "source": [
        "# with input_length2=100\n",
        "# kernels = 4,6,8\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=6, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=8, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X,X_to,X_from], y=Y, batch_size=64, epochs=3) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12779 samples\n",
            "Epoch 1/3\n",
            "12779/12779 [==============================] - 98s 8ms/sample - loss: 0.7745 - accuracy: 0.7097\n",
            "Epoch 2/3\n",
            "12779/12779 [==============================] - 97s 8ms/sample - loss: 0.4171 - accuracy: 0.8510\n",
            "Epoch 3/3\n",
            "12779/12779 [==============================] - 97s 8ms/sample - loss: 0.3121 - accuracy: 0.8891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb33cd0b048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sbGXe_ZYihcn"
      },
      "source": [
        "###### Predict on test set for kaggle (13th upload) - 82.1% on Kaggle.\n",
        "\n",
        "Using the whole training set for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LaOg-hsUihcr",
        "colab": {}
      },
      "source": [
        "Y_new13 = model.predict(x=[X_new, X_new_to, X_new_from])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "usxy_GGAihcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "33931e56-3210-40df-b01b-e9b079c9aa18"
      },
      "source": [
        "Y_new13[:3] # results before softmax are shown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7490175 , 0.20132141, 0.02974566, 0.01197721, 0.00793809],\n",
              "       [0.28935567, 0.2452781 , 0.44919783, 0.00643598, 0.00973232],\n",
              "       [0.8710673 , 0.06758085, 0.04683995, 0.00284915, 0.01166281]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4tLMgaquihc4",
        "colab": {}
      },
      "source": [
        "# pick the label with max score.\n",
        "flat_predictions = np.argmax(Y_new13, axis=1).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FZiVZalAihc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "184e195e-b521-4b93-c34a-b149a939ac30"
      },
      "source": [
        "flat_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, ..., 0, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rnsWxYmihdB",
        "colab": {}
      },
      "source": [
        "test13 = pd.DataFrame(list(zip(test['id'], flat_predictions)), \n",
        "               columns = ['id', 'label'])\n",
        "test13\n",
        "test13.to_csv('test13.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Vra8fiJqQhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "f31fb527-e80a-4b17-9e73-d461fd3b325b"
      },
      "source": [
        "# with input_length2=100\n",
        "# kernels = 4,6,8\n",
        "# dropout = 0.2\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100):\n",
        "  # define CNN layer\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv1 = Conv1D(filters=100, kernel_size=4, padding='same', activation='relu', strides=1)(emb1)\n",
        "  drop1 = Dropout(0.2)(conv1)\n",
        "  pool1 = GlobalMaxPooling1D()(drop1) # MaxOverTime\n",
        "  # pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  emb2 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv2 = Conv1D(filters=100, kernel_size=6, padding='same', activation='relu', strides=1)(emb2)\n",
        "  drop2 = Dropout(0.2)(conv2)\n",
        "  pool2 = GlobalMaxPooling1D()(drop2)\n",
        "  # pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  emb3 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  conv3 = Conv1D(filters=100, kernel_size=8, padding='same', activation='relu', strides=1)(emb3)\n",
        "  drop3 = Dropout(0.2)(conv3)\n",
        "  pool3 = GlobalMaxPooling1D()(drop3)\n",
        "  # pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # inputs4 = Input(shape=(input_length2,))\n",
        "\n",
        "  # merge CNN output with reference stats\n",
        "  # merged = concatenate([flat1, flat2, flat3, inputs4])\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(500, activation='relu')(merged)\n",
        "  outputs = Dense(5, activation=\"softmax\")(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # , inputs4\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 100)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 83s 8ms/sample - loss: 0.8323 - accuracy: 0.6883 - val_loss: 0.5742 - val_accuracy: 0.8001\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 82s 8ms/sample - loss: 0.4077 - accuracy: 0.8543 - val_loss: 0.5367 - val_accuracy: 0.8009\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 82s 8ms/sample - loss: 0.2604 - accuracy: 0.9067 - val_loss: 0.5391 - val_accuracy: 0.8067\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 83s 8ms/sample - loss: 0.1516 - accuracy: 0.9488 - val_loss: 0.5800 - val_accuracy: 0.8024\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 82s 8ms/sample - loss: 0.0754 - accuracy: 0.9769 - val_loss: 0.7006 - val_accuracy: 0.7997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb332e1f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sEtEutOuhXE",
        "colab_type": "text"
      },
      "source": [
        "##### LSTM\n",
        "\n",
        "Set RMSprop lr=0.01 otherwise convergence too slow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTbO2tRCTyRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "a348e009-2985-47ff-9867-6b6a700e12be"
      },
      "source": [
        "# input_length2=50\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 128):\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  lstm = LSTM(128, dropout=0.2)(emb1)\n",
        "  outputs = Dense(5, activation=\"softmax\")(lstm)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20,input_length2=50,vocab_size=13273+1,emb_dim = 128)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_15 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_4 (TensorFlo [(None, 120)]        0           input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "                                                                 input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 120, 128)     1699072     tf_op_layer_concat_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          131584      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 5)            645         lstm[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 1,831,301\n",
            "Trainable params: 1,831,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 55s 5ms/sample - loss: 1.2891 - accuracy: 0.4868 - val_loss: 1.1271 - val_accuracy: 0.5481\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 52s 5ms/sample - loss: 0.9754 - accuracy: 0.6138 - val_loss: 0.6409 - val_accuracy: 0.7703\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 53s 5ms/sample - loss: 0.5055 - accuracy: 0.8239 - val_loss: 0.5632 - val_accuracy: 0.8052\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 53s 5ms/sample - loss: 0.3666 - accuracy: 0.8741 - val_loss: 0.5675 - val_accuracy: 0.8013\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 54s 5ms/sample - loss: 0.2713 - accuracy: 0.9081 - val_loss: 0.6217 - val_accuracy: 0.7911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe474999780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwtmNoeDxoza",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "0717102b-3cc9-4049-f63e-e30e611cd13a"
      },
      "source": [
        "# input_length2=100\n",
        "\n",
        "def custom_model(input_length=20, input_length2=100, vocab_size=13274, emb_dim = 128):\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  lstm = LSTM(128, dropout=0.2)(emb1)\n",
        "  outputs = Dense(5, activation=\"softmax\")(lstm)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20,input_length2=100,vocab_size=13273+1,emb_dim = 128)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_62 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_63 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_64 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_25 (TensorFl [(None, 220)]        0           input_62[0][0]                   \n",
            "                                                                 input_63[0][0]                   \n",
            "                                                                 input_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_53 (Embedding)        (None, 220, 128)     1699072     tf_op_layer_concat_25[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 128)          131584      embedding_53[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (None, 5)            645         lstm_9[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,831,301\n",
            "Trainable params: 1,831,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 95s 9ms/sample - loss: 1.4771 - accuracy: 0.3718 - val_loss: 1.2271 - val_accuracy: 0.5110\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 93s 9ms/sample - loss: 1.0790 - accuracy: 0.5671 - val_loss: 1.1313 - val_accuracy: 0.5450\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 93s 9ms/sample - loss: 0.9407 - accuracy: 0.6201 - val_loss: 0.9443 - val_accuracy: 0.5775\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 94s 9ms/sample - loss: 0.5393 - accuracy: 0.7987 - val_loss: 0.5960 - val_accuracy: 0.7903\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 93s 9ms/sample - loss: 0.3596 - accuracy: 0.8751 - val_loss: 0.6279 - val_accuracy: 0.7903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe473a487b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTLYYPX90rDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "a12b8cfc-dd4f-4c5d-900c-fdf89a291930"
      },
      "source": [
        "# input_length2=50, 2 hidden layers\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 128):\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  lstm = LSTM(128, dropout=0.2,return_sequences=True)(emb1)\n",
        "  lstm2 = LSTM(128, dropout=0.2)(lstm)\n",
        "  outputs = Dense(5, activation=\"softmax\")(lstm2)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20,input_length2=50,vocab_size=13273+1,emb_dim = 128)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_33 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_10 (TensorFl [(None, 120)]        0           input_31[0][0]                   \n",
            "                                                                 input_32[0][0]                   \n",
            "                                                                 input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_24 (Embedding)        (None, 120, 128)     1699072     tf_op_layer_concat_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 120, 128)     131584      embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 128)          131584      lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 5)            645         lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,962,885\n",
            "Trainable params: 1,962,885\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 106s 10ms/sample - loss: 1.5889 - accuracy: 0.2693 - val_loss: 1.3288 - val_accuracy: 0.4296\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 104s 10ms/sample - loss: 1.1425 - accuracy: 0.5311 - val_loss: 0.9034 - val_accuracy: 0.6510\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 103s 10ms/sample - loss: 0.6703 - accuracy: 0.7564 - val_loss: 0.6468 - val_accuracy: 0.7688\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 103s 10ms/sample - loss: 0.5136 - accuracy: 0.8257 - val_loss: 0.7035 - val_accuracy: 0.7629\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 103s 10ms/sample - loss: 0.4252 - accuracy: 0.8562 - val_loss: 0.6036 - val_accuracy: 0.7864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe476792748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNQTgCGYxij4",
        "colab_type": "text"
      },
      "source": [
        "##### Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz0D_S6YwLx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "26d41c4b-fdf8-41cd-d561-6cfb58dc11d1"
      },
      "source": [
        "# input_length2=50\n",
        "\n",
        "def custom_model(input_length=20, input_length2=50, vocab_size=13274, emb_dim = 128):\n",
        "  inputs1 = Input(shape=(input_length,))\n",
        "  inputs2 = Input(shape=(input_length2,))\n",
        "  inputs3 = Input(shape=(input_length2,))\n",
        "  merged = concatenate([inputs1, inputs2, inputs3])\n",
        "\n",
        "  emb1 = Embedding(vocab_size, emb_dim)(merged)\n",
        "  lstm = Bidirectional(LSTM(128, dropout=0.2))(emb1)\n",
        "  outputs = Dense(5, activation=\"softmax\")(lstm)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "np.random.seed(5)\n",
        "random.seed(5)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(5)\n",
        "\n",
        "model = custom_model(input_length=20,input_length2=50,vocab_size=13273+1,emb_dim = 128)\n",
        "model.fit(x=[X_train_,X_to_train_,X_from_train_], y=Y_train_, validation_data = ([X_test_,X_to_test_,X_from_test_], Y_test_), batch_size=64, epochs=5) \n",
        "# save the model\n",
        "# model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_8 (TensorFlo [(None, 120)]        0           input_25[0][0]                   \n",
            "                                                                 input_26[0][0]                   \n",
            "                                                                 input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, 120, 128)     1699072     tf_op_layer_concat_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 256)          263168      embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 5)            1285        bidirectional[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,963,525\n",
            "Trainable params: 1,963,525\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 10223 samples, validate on 2556 samples\n",
            "Epoch 1/5\n",
            "10223/10223 [==============================] - 105s 10ms/sample - loss: 1.3186 - accuracy: 0.4697 - val_loss: 1.1504 - val_accuracy: 0.5469\n",
            "Epoch 2/5\n",
            "10223/10223 [==============================] - 102s 10ms/sample - loss: 0.6742 - accuracy: 0.7521 - val_loss: 0.5626 - val_accuracy: 0.7969\n",
            "Epoch 3/5\n",
            "10223/10223 [==============================] - 102s 10ms/sample - loss: 0.4230 - accuracy: 0.8569 - val_loss: 0.5942 - val_accuracy: 0.7969\n",
            "Epoch 4/5\n",
            "10223/10223 [==============================] - 101s 10ms/sample - loss: 0.3110 - accuracy: 0.8946 - val_loss: 0.6282 - val_accuracy: 0.7926\n",
            "Epoch 5/5\n",
            "10223/10223 [==============================] - 100s 10ms/sample - loss: 0.2282 - accuracy: 0.9253 - val_loss: 0.6200 - val_accuracy: 0.7958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe478492f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    }
  ]
}